
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{dog\_app}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Artificial Intelligence
Nanodegree}\label{artificial-intelligence-nanodegree}

\subsection{Convolutional Neural
Networks}\label{convolutional-neural-networks}

\subsection{Project: Write an Algorithm for a Dog Identification
App}\label{project-write-an-algorithm-for-a-dog-identification-app}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this notebook, some template code has already been provided for you,
and you will need to implement additional functionality to successfully
complete this project. You will not need to modify the included code
beyond what is requested. Sections that begin with
\textbf{'(IMPLEMENTATION)'} in the header indicate that the following
block of code will require additional functionality which you must
provide. Instructions will be provided for each section, and the
specifics of the implementation are marked in the code block with a
'TODO' statement. Please be sure to read the instructions carefully!

\begin{quote}
\textbf{Note}: Once you have completed all of the code implementations,
you need to finalize your work by exporting the iPython Notebook as an
HTML document. Before exporting the notebook to html, all of the code
cells need to have been run so that reviewers can see the final
implementation and output. You can then export the notebook by using the
menu above and navigating to \n", "\textbf{File -\textgreater{} Download
as -\textgreater{} HTML (.html)}. Include the finished document along
with this notebook as your submission.
\end{quote}

In addition to implementing code, there will be questions that you must
answer which relate to the project and your implementation. Each section
where you will answer a question is preceded by a \textbf{'Question X'}
header. Carefully read each question and provide thorough answers in the
following text boxes that begin with \textbf{'Answer:'}. Your project
submission will be evaluated based on your answers to each of the
questions and the implementation you provide.

\begin{quote}
\textbf{Note:} Code and Markdown cells can be executed using the
\textbf{Shift + Enter} keyboard shortcut. Markdown cells can be edited
by double-clicking the cell to enter edit mode.
\end{quote}

The rubric contains \emph{optional} "Stand Out Suggestions" for
enhancing the project beyond the minimum requirements. If you decide to
pursue the "Stand Out Suggestions", you should include the code in this
IPython notebook.

 \#\# Step 0: Import Datasets

\subsubsection{Import Dog Dataset}\label{import-dog-dataset}

In the code cell below, we import a dataset of dog images. We populate a
few variables through the use of the \texttt{load\_files} function from
the scikit-learn library: - \texttt{train\_files},
\texttt{valid\_files}, \texttt{test\_files} - numpy arrays containing
file paths to images - \texttt{train\_targets}, \texttt{valid\_targets},
\texttt{test\_targets} - numpy arrays containing onehot-encoded
classification labels - \texttt{dog\_names} - list of string-valued dog
breed names for translating labels

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k}{import} \PY{n}{load\PYZus{}files}       
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{np\PYZus{}utils}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{glob} \PY{k}{import} \PY{n}{glob}
        
        \PY{c+c1}{\PYZsh{} define function to load train, test, and validation datasets}
        \PY{k}{def} \PY{n+nf}{load\PYZus{}dataset}\PY{p}{(}\PY{n}{path}\PY{p}{)}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{load\PYZus{}files}\PY{p}{(}\PY{n}{path}\PY{p}{)}
            \PY{n}{dog\PYZus{}files} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{filenames}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
            \PY{n}{dog\PYZus{}targets} \PY{o}{=} \PY{n}{np\PYZus{}utils}\PY{o}{.}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{133}\PY{p}{)}
            \PY{k}{return} \PY{n}{dog\PYZus{}files}\PY{p}{,} \PY{n}{dog\PYZus{}targets}
        
        \PY{c+c1}{\PYZsh{} load train, test, and validation datasets}
        \PY{n}{train\PYZus{}files}\PY{p}{,} \PY{n}{train\PYZus{}targets} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogImages/train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{valid\PYZus{}files}\PY{p}{,} \PY{n}{valid\PYZus{}targets} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogImages/valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{test\PYZus{}files}\PY{p}{,} \PY{n}{test\PYZus{}targets} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogImages/test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} load list of dog names}
        \PY{n}{dog\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{n}{item}\PY{p}{[}\PY{l+m+mi}{20}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{item} \PY{o+ow}{in} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{glob}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dogImages/train/*/}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} print statistics about the dataset}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ total dog categories.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{dog\PYZus{}names}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ total dog images.}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{train\PYZus{}files}\PY{p}{,} \PY{n}{valid\PYZus{}files}\PY{p}{,} \PY{n}{test\PYZus{}files}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ training dog images.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ validation dog images.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{valid\PYZus{}files}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ test dog images.}\PY{l+s+s1}{\PYZsq{}}\PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Using TensorFlow backend.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
There are 133 total dog categories.
There are 8351 total dog images.

There are 6680 training dog images.
There are 835 validation dog images.
There are 836 test dog images.

    \end{Verbatim}

    \subsubsection{Import Human Dataset}\label{import-human-dataset}

In the code cell below, we import a dataset of human images, where the
file paths are stored in the numpy array \texttt{human\_files}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{random}
        \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{8675309}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} load filenames in shuffled human dataset}
        \PY{n}{human\PYZus{}files} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{glob}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lfw/*/*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{random}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} print statistics about the dataset}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ total human images.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
There are 13233 total human images.

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 1: Detect Humans

We use OpenCV's implementation of
\href{http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html}{Haar
feature-based cascade classifiers} to detect human faces in images.
OpenCV provides many pre-trained face detectors, stored as XML files on
\href{https://github.com/opencv/opencv/tree/master/data/haarcascades}{github}.
We have downloaded one of these detectors and stored it in the
\texttt{haarcascades} directory.

In the next code cell, we demonstrate how to use this detector to find
human faces in a sample image.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{import} \PY{n+nn}{cv2}                
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}                        
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline                               
        
        \PY{c+c1}{\PYZsh{} extract pre\PYZhy{}trained face detector}
        \PY{n}{face\PYZus{}cascade} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{CascadeClassifier}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{haarcascades/haarcascade\PYZus{}frontalface\PYZus{}alt.xml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} load color (BGR) image}
        \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} convert BGR image to grayscale}
        \PY{n}{gray} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2GRAY}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} find faces in image}
        \PY{n}{faces} \PY{o}{=} \PY{n}{face\PYZus{}cascade}\PY{o}{.}\PY{n}{detectMultiScale}\PY{p}{(}\PY{n}{gray}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} print number of faces detected in the image}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of faces detected:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{faces}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} get bounding box for each detected face}
        \PY{k}{for} \PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{w}\PY{p}{,}\PY{n}{h}\PY{p}{)} \PY{o+ow}{in} \PY{n}{faces}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} add bounding box to color image}
            \PY{n}{cv2}\PY{o}{.}\PY{n}{rectangle}\PY{p}{(}\PY{n}{img}\PY{p}{,}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{x}\PY{o}{+}\PY{n}{w}\PY{p}{,}\PY{n}{y}\PY{o}{+}\PY{n}{h}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{255}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
            
        \PY{c+c1}{\PYZsh{} convert BGR image to RGB for plotting}
        \PY{n}{cv\PYZus{}rgb} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2RGB}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} display the image, along with bounding box}
        \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{cv\PYZus{}rgb}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Number of faces detected: 1

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_5_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Before using any of the face detectors, it is standard procedure to
convert the images to grayscale. The \texttt{detectMultiScale} function
executes the classifier stored in \texttt{face\_cascade} and takes the
grayscale image as a parameter.

In the above code, \texttt{faces} is a numpy array of detected faces,
where each row corresponds to a detected face. Each detected face is a
1D array with four entries that specifies the bounding box of the
detected face. The first two entries in the array (extracted in the
above code as \texttt{x} and \texttt{y}) specify the horizontal and
vertical positions of the top left corner of the bounding box. The last
two entries in the array (extracted here as \texttt{w} and \texttt{h})
specify the width and height of the box.

\subsubsection{Write a Human Face
Detector}\label{write-a-human-face-detector}

We can use this procedure to write a function that returns \texttt{True}
if a human face is detected in an image and \texttt{False} otherwise.
This function, aptly named \texttt{face\_detector}, takes a
string-valued file path to an image as input and appears in the code
block below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} returns \PYZdq{}True\PYZdq{} if face is detected in image stored at img\PYZus{}path}
        \PY{k}{def} \PY{n+nf}{face\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
            \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
            \PY{n}{gray} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2GRAY}\PY{p}{)}
            \PY{n}{faces} \PY{o}{=} \PY{n}{face\PYZus{}cascade}\PY{o}{.}\PY{n}{detectMultiScale}\PY{p}{(}\PY{n}{gray}\PY{p}{)}
            \PY{k}{return} \PY{n+nb}{len}\PY{p}{(}\PY{n}{faces}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Assess the Human Face
Detector}\label{implementation-assess-the-human-face-detector}

\textbf{Question 1:} Use the code cell below to test the performance of
the \texttt{face\_detector} function.\\
- What percentage of the first 100 images in \texttt{human\_files} have
a detected human face?\\
- What percentage of the first 100 images in \texttt{dog\_files} have a
detected human face?

Ideally, we would like 100\% of human images with a detected face and
0\% of dog images with a detected face. You will see that our algorithm
falls short of this goal, but still gives acceptable performance. We
extract the file paths for the first 100 images from each of the
datasets and store them in the numpy arrays \texttt{human\_files\_short}
and \texttt{dog\_files\_short}.

\textbf{Answer:} 99\% of human faces correctly detected, 11\% of dogs as
human faces detected.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{human\PYZus{}files\PYZus{}short} \PY{o}{=} \PY{n}{human\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}
        \PY{n}{dog\PYZus{}files\PYZus{}short} \PY{o}{=} \PY{n}{train\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}
        \PY{c+c1}{\PYZsh{} Do NOT modify the code above this line.}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} TODO: Test the performance of the face\PYZus{}detector algorithm }
        \PY{c+c1}{\PYZsh{}\PYZsh{} on the images in human\PYZus{}files\PYZus{}short and dog\PYZus{}files\PYZus{}short.}
        
        \PY{n}{human\PYZus{}detected} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{dog\PYZus{}detected} \PY{o}{=} \PY{l+m+mi}{0}
        
        \PY{k}{for} \PY{n}{human} \PY{o+ow}{in} \PY{n}{human\PYZus{}files\PYZus{}short}\PY{p}{:}
            \PY{k}{if} \PY{p}{(}\PY{n}{face\PYZus{}detector}\PY{p}{(}\PY{n}{human}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{n}{human\PYZus{}detected} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}        
        \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Humans as humans detected:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{human\PYZus{}detected}\PY{p}{)}
        
        \PY{k}{for} \PY{n}{dog} \PY{o+ow}{in} \PY{n}{dog\PYZus{}files\PYZus{}short}\PY{p}{:}
            \PY{k}{if} \PY{p}{(}\PY{n}{face\PYZus{}detector}\PY{p}{(}\PY{n}{dog}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{n}{dog\PYZus{}detected} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}        
        \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dogs as humans detected:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{dog\PYZus{}detected}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Humans as humans detected: 99
Dogs as humans detected: 11

    \end{Verbatim}

    \textbf{Question 2:} This algorithmic choice necessitates that we
communicate to the user that we accept human images only when they
provide a clear view of a face (otherwise, we risk having unneccessarily
frustrated users!). In your opinion, is this a reasonable expectation to
pose on the user? If not, can you think of a way to detect humans in
images that does not necessitate an image with a clearly presented face?

\textbf{Answer:} It seems to make sense to expect the user to pose. A
face recognition algorithm is not expected to work well if faces are
camouflaged in any way. Another approach for detecting human faces would
be to use the face landmark estimation algorithms that focus on certain
face details. In cases a clear view of a face isn't available, the use
of a CNN would be a better solution as they are able to learn the
features found in images, hence they would still be able to recognize a
human face if visible partially or from a side angle.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 2: Detect Dogs

In this section, we use a pre-trained
\href{http://ethereon.github.io/netscope/\#/gist/db945b393d40bfa26006}{ResNet-50}
model to detect dogs in images. Our first line of code downloads the
ResNet-50 model, along with weights that have been trained on
\href{http://www.image-net.org/}{ImageNet}, a very large, very popular
dataset used for image classification and other vision tasks. ImageNet
contains over 10 million URLs, each linking to an image containing an
object from one of
\href{https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a}{1000
categories}. Given an image, this pre-trained ResNet-50 model returns a
prediction (derived from the available categories in ImageNet) for the
object that is contained in the image.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications}\PY{n+nn}{.}\PY{n+nn}{resnet50} \PY{k}{import} \PY{n}{ResNet50}
        
        \PY{c+c1}{\PYZsh{} define ResNet50 model}
        \PY{n}{ResNet50\PYZus{}model} \PY{o}{=} \PY{n}{ResNet50}\PY{p}{(}\PY{n}{weights}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imagenet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Pre-process the Data}\label{pre-process-the-data}

When using TensorFlow as backend, Keras CNNs require a 4D array (which
we'll also refer to as a 4D tensor) as input, with shape

\[
(\text{nb_samples}, \text{rows}, \text{columns}, \text{channels}),
\]

where \texttt{nb\_samples} corresponds to the total number of images (or
samples), and \texttt{rows}, \texttt{columns}, and \texttt{channels}
correspond to the number of rows, columns, and channels for each image,
respectively.

The \texttt{path\_to\_tensor} function below takes a string-valued file
path to a color image as input and returns a 4D tensor suitable for
supplying to a Keras CNN. The function first loads the image and resizes
it to a square image that is \(224 \times 224\) pixels. Next, the image
is converted to an array, which is then resized to a 4D tensor. In this
case, since we are working with color images, each image has three
channels. Likewise, since we are processing a single image (or sample),
the returned tensor will always have shape

\[
(1, 224, 224, 3).
\]

The \texttt{paths\_to\_tensor} function takes a numpy array of
string-valued image paths as input and returns a 4D tensor with shape

\[
(\text{nb_samples}, 224, 224, 3).
\]

Here, \texttt{nb\_samples} is the number of samples, or number of
images, in the supplied array of image paths. It is best to think of
\texttt{nb\_samples} as the number of 3D tensors (where each 3D tensor
corresponds to a different image) in your dataset!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{image}                  
        \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm}
        
        \PY{k}{def} \PY{n+nf}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} loads RGB image as PIL.Image.Image type}
            \PY{n}{img} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{load\PYZus{}img}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{target\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{224}\PY{p}{,} \PY{l+m+mi}{224}\PY{p}{)}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)}
            \PY{n}{x} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{img\PYZus{}to\PYZus{}array}\PY{p}{(}\PY{n}{img}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{k}{def} \PY{n+nf}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}paths}\PY{p}{)}\PY{p}{:}
            \PY{n}{list\PYZus{}of\PYZus{}tensors} \PY{o}{=} \PY{p}{[}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)} \PY{k}{for} \PY{n}{img\PYZus{}path} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{img\PYZus{}paths}\PY{p}{)}\PY{p}{]}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}tensors}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Making Predictions with
ResNet-50}\label{making-predictions-with-resnet-50}

Getting the 4D tensor ready for ResNet-50, and for any other pre-trained
model in Keras, requires some additional processing. First, the RGB
image is converted to BGR by reordering the channels. All pre-trained
models have the additional normalization step that the mean pixel
(expressed in RGB as \([103.939, 116.779, 123.68]\) and calculated from
all pixels in all images in ImageNet) must be subtracted from every
pixel in each image. This is implemented in the imported function
\texttt{preprocess\_input}. If you're curious, you can check the code
for \texttt{preprocess\_input}
\href{https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py}{here}.

Now that we have a way to format our image for supplying to ResNet-50,
we are now ready to use the model to extract the predictions. This is
accomplished with the \texttt{predict} method, which returns an array
whose \(i\)-th entry is the model's predicted probability that the image
belongs to the \(i\)-th ImageNet category. This is implemented in the
\texttt{ResNet50\_predict\_labels} function below.

By taking the argmax of the predicted probability vector, we obtain an
integer corresponding to the model's predicted object class, which we
can identify with an object category through the use of this
\href{https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a}{dictionary}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications}\PY{n+nn}{.}\PY{n+nn}{resnet50} \PY{k}{import} \PY{n}{preprocess\PYZus{}input}\PY{p}{,} \PY{n}{decode\PYZus{}predictions}
        
        \PY{k}{def} \PY{n+nf}{ResNet50\PYZus{}predict\PYZus{}labels}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} returns prediction vector for image located at img\PYZus{}path}
            \PY{n}{img} \PY{o}{=} \PY{n}{preprocess\PYZus{}input}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{ResNet50\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Write a Dog Detector}\label{write-a-dog-detector}

While looking at the
\href{https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a}{dictionary},
you will notice that the categories corresponding to dogs appear in an
uninterrupted sequence and correspond to dictionary keys 151-268,
inclusive, to include all categories from
\texttt{\textquotesingle{}Chihuahua\textquotesingle{}} to
\texttt{\textquotesingle{}Mexican\ hairless\textquotesingle{}}. Thus, in
order to check to see if an image is predicted to contain a dog by the
pre-trained ResNet-50 model, we need only check if the
\texttt{ResNet50\_predict\_labels} function above returns a value
between 151 and 268 (inclusive).

We use these ideas to complete the \texttt{dog\_detector} function
below, which returns \texttt{True} if a dog is detected in an image (and
\texttt{False} if not).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} returns \PYZdq{}True\PYZdq{} if a dog is detected in the image stored at img\PYZus{}path}
        \PY{k}{def} \PY{n+nf}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
            \PY{n}{prediction} \PY{o}{=} \PY{n}{ResNet50\PYZus{}predict\PYZus{}labels}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
            \PY{k}{return} \PY{p}{(}\PY{p}{(}\PY{n}{prediction} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{268}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{prediction} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{151}\PY{p}{)}\PY{p}{)} 
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Assess the Dog
Detector}\label{implementation-assess-the-dog-detector}

\textbf{Question 3:} Use the code cell below to test the performance of
your \texttt{dog\_detector} function.\\
- What percentage of the images in \texttt{human\_files\_short} have a
detected dog?\\
- What percentage of the images in \texttt{dog\_files\_short} have a
detected dog?

\textbf{Answer:} 100\% of dogs correctly detected, 1\% of humans as dogs
detected.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Test the performance of the dog\PYZus{}detector function}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} on the images in human\PYZus{}files\PYZus{}short and dog\PYZus{}files\PYZus{}short.}
         
         \PY{n}{human\PYZus{}detected} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{n}{dog\PYZus{}detected} \PY{o}{=} \PY{l+m+mi}{0}
         
         \PY{k}{for} \PY{n}{human} \PY{o+ow}{in} \PY{n}{human\PYZus{}files\PYZus{}short}\PY{p}{:}
             \PY{k}{if} \PY{p}{(}\PY{n}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{human}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{human\PYZus{}detected} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}        
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Humans as dogs detected:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{human\PYZus{}detected}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{dog} \PY{o+ow}{in} \PY{n}{dog\PYZus{}files\PYZus{}short}\PY{p}{:}
             \PY{k}{if} \PY{p}{(}\PY{n}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{dog}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{dog\PYZus{}detected} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dogs as dogs detected:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{dog\PYZus{}detected}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Humans as dogs detected: 1
Dogs as dogs detected: 100

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 3: Create a CNN to Classify Dog Breeds (from Scratch)

Now that we have functions for detecting humans and dogs in images, we
need a way to predict breed from images. In this step, you will create a
CNN that classifies dog breeds. You must create your CNN \emph{from
scratch} (so, you can't use transfer learning \emph{yet}!), and you must
attain a test accuracy of at least 1\%. In Step 5 of this notebook, you
will have the opportunity to use transfer learning to create a CNN that
attains greatly improved accuracy.

Be careful with adding too many trainable layers! More parameters means
longer training, which means you are more likely to need a GPU to
accelerate the training process. Thankfully, Keras provides a handy
estimate of the time that each epoch is likely to take; you can
extrapolate this estimate to figure out how long it will take for your
algorithm to train.

We mention that the task of assigning breed to dogs from images is
considered exceptionally challenging. To see why, consider that
\emph{even a human} would have great difficulty in distinguishing
between a Brittany and a Welsh Springer Spaniel.

\begin{longtable}[]{@{}ll@{}}
\toprule
Brittany & Welsh Springer Spaniel\tabularnewline
\midrule
\endhead
&\tabularnewline
\bottomrule
\end{longtable}

It is not difficult to find other dog breed pairs with minimal
inter-class variation (for instance, Curly-Coated Retrievers and
American Water Spaniels).

\begin{longtable}[]{@{}ll@{}}
\toprule
Curly-Coated Retriever & American Water Spaniel\tabularnewline
\midrule
\endhead
&\tabularnewline
\bottomrule
\end{longtable}

Likewise, recall that labradors come in yellow, chocolate, and black.
Your vision-based algorithm will have to conquer this high intra-class
variation to determine how to classify all of these different shades as
the same breed.

\begin{longtable}[]{@{}ll@{}}
\toprule
Yellow Labrador & Chocolate Labrador\tabularnewline
\midrule
\endhead
&\tabularnewline
\bottomrule
\end{longtable}

We also mention that random chance presents an exceptionally low bar:
setting aside the fact that the classes are slightly imabalanced, a
random guess will provide a correct answer roughly 1 in 133 times, which
corresponds to an accuracy of less than 1\%.

Remember that the practice is far ahead of the theory in deep learning.
Experiment with many different architectures, and trust your intuition.
And, of course, have fun!

\subsubsection{Pre-process the Data}\label{pre-process-the-data}

We rescale the images by dividing every pixel in every image by 255.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k}{import} \PY{n}{ImageFile}                            
         \PY{n}{ImageFile}\PY{o}{.}\PY{n}{LOAD\PYZus{}TRUNCATED\PYZus{}IMAGES} \PY{o}{=} \PY{k+kc}{True}                 
         
         \PY{c+c1}{\PYZsh{} pre\PYZhy{}process the data for Keras}
         \PY{n}{train\PYZus{}tensors} \PY{o}{=} \PY{n}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
         \PY{n}{valid\PYZus{}tensors} \PY{o}{=} \PY{n}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{valid\PYZus{}files}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
         \PY{n}{test\PYZus{}tensors} \PY{o}{=} \PY{n}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|██████████| 6680/6680 [00:54<00:00, 123.54it/s]
100\%|██████████| 835/835 [00:06<00:00, 133.43it/s]
100\%|██████████| 836/836 [00:06<00:00, 138.00it/s]

    \end{Verbatim}

    \subsubsection{(IMPLEMENTATION) Model
Architecture}\label{implementation-model-architecture}

Create a CNN to classify dog breed. At the end of your code cell block,
summarize the layers of your model by executing the line:

\begin{verbatim}
    model.summary()
\end{verbatim}

We have imported some Python modules to get you started, but feel free
to import as many modules as you need. If you end up getting stuck,
here's a hint that specifies a model that trains relatively fast on CPU
and attains \textgreater{}1\% test accuracy in 5 epochs:

\begin{figure}
\centering
\includegraphics{images/sample_cnn.png}
\caption{Sample CNN}
\end{figure}

\textbf{Question 4:} Outline the steps you took to get to your final CNN
architecture and your reasoning at each step. If you chose to use the
hinted architecture above, describe why you think that CNN architecture
should work well for the image classification task.

\textbf{Answer:} 1. Create a sequential model as a linear stack of
layers. 2. Add a convolutional layer. For each region the layer performs
a set of math operations to produce a single value in the output feature
map, so the array will be deeper as it passes through. Relu is used as
an activation function, turning all negative values to zero, which works
against the vanishing gradient problem. 3. Add a MaxPooling layer to the
model. This layer takes the convolutional layer as input and returns the
same stack but with a reduced feature map. So the MaxPooling layer is
used to decrease the spatial dimensions. 4. Repeat the above steps with
different filter sizes two times, which should be sufficient for a
reasonable result. 5. Add Dropout to the model to make sure all parts of
the model are trained, not only the ones with high weights. The Dropout
randomly leaves out nodes. The value passed is the probability each node
will be dropped. 6. Add Flatten Layer to convert image matrix input to a
vector. 7. Add fully-connected Dense Layer. 8. Add Dropout again. 9. Add
fully-connected Dense Layer again with 133 units and a Softmax
activation function as this is the final layer and should return a
probability between 0 and 1.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Conv2D}\PY{p}{,} \PY{n}{MaxPooling2D}\PY{p}{,} \PY{n}{GlobalAveragePooling2D}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dropout}\PY{p}{,} \PY{n}{Flatten}\PY{p}{,} \PY{n}{Dense}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Define your architecture.}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{224}\PY{p}{,} \PY{l+m+mi}{224}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.25}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv2d\_1 (Conv2D)            (None, 224, 224, 16)      208       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_2 (MaxPooling2 (None, 112, 112, 16)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_2 (Conv2D)            (None, 112, 112, 32)      2080      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_3 (MaxPooling2 (None, 56, 56, 32)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_3 (Conv2D)            (None, 56, 56, 64)        8256      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_4 (MaxPooling2 (None, 28, 28, 64)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_1 (Dropout)          (None, 28, 28, 64)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_2 (Flatten)          (None, 50176)             0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_1 (Dense)              (None, 500)               25088500  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_2 (Dropout)          (None, 500)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_2 (Dense)              (None, 133)               66633     
=================================================================
Total params: 25,165,677
Trainable params: 25,165,677
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \subsubsection{Compile the Model}\label{compile-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rmsprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Train the
Model}\label{implementation-train-the-model}

Train your model in the code cell below. Use model checkpointing to save
the model that attains the best validation loss.

You are welcome to
\href{https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html}{augment
the training data}, but this is not a requirement.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} Augment the training data}
         \PY{c+c1}{\PYZsh{} Randomly flip images horizontally and shift by 10\PYZpc{} horizontally/vertically}
         
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing}\PY{n+nn}{.}\PY{n+nn}{image} \PY{k}{import} \PY{n}{ImageDataGenerator}
         
         \PY{n}{datagen\PYZus{}train} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}\PY{n}{width\PYZus{}shift\PYZus{}range}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{height\PYZus{}shift\PYZus{}range}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{horizontal\PYZus{}flip}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{datagen\PYZus{}valid} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}\PY{n}{width\PYZus{}shift\PYZus{}range}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{height\PYZus{}shift\PYZus{}range}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{horizontal\PYZus{}flip}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{datagen\PYZus{}train}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}tensors}\PY{p}{)}
         \PY{n}{datagen\PYZus{}valid}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{valid\PYZus{}tensors}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{ModelCheckpoint}  
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: specify the number of epochs that you would like to use to train the model.}
         
         \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{8}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Do NOT modify the code below this line.}
         
         \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.from\PYZus{}scratch.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                        \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}tensors}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} 
                   \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{valid\PYZus{}tensors}\PY{p}{,} \PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{,}
                   \PY{n}{epochs}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/8
4020/6680 [=================>{\ldots}] - ETA: 11:44 - loss: 4.9328 - acc: 0.0000e+ - ETA: 8:50 - loss: 8.3042 - acc: 0.0000e+00 - ETA: 7:47 - loss: 8.0788 - acc: 0.0000e+0 - ETA: 7:16 - loss: 7.3914 - acc: 0.0000e+0 - ETA: 6:57 - loss: 6.9091 - acc: 0.0000e+0 - ETA: 6:45 - loss: 6.6253 - acc: 0.0000e+0 - ETA: 6:35 - loss: 6.4190 - acc: 0.0000e+0 - ETA: 6:27 - loss: 6.2465 - acc: 0.0000e+0 - ETA: 6:22 - loss: 6.1150 - acc: 0.0000e+0 - ETA: 6:16 - loss: 5.9901 - acc: 0.0000e+0 - ETA: 6:12 - loss: 5.8964 - acc: 0.0000e+0 - ETA: 6:09 - loss: 5.8150 - acc: 0.0000e+0 - ETA: 6:05 - loss: 5.7431 - acc: 0.0038    - ETA: 6:03 - loss: 5.6795 - acc: 0.003 - ETA: 6:01 - loss: 5.6310 - acc: 0.003 - ETA: 6:00 - loss: 5.5875 - acc: 0.003 - ETA: 5:58 - loss: 5.5480 - acc: 0.002 - ETA: 5:56 - loss: 5.5116 - acc: 0.002 - ETA: 5:53 - loss: 5.4783 - acc: 0.002 - ETA: 5:51 - loss: 5.4496 - acc: 0.002 - ETA: 5:49 - loss: 5.4232 - acc: 0.002 - ETA: 5:47 - loss: 5.3989 - acc: 0.002 - ETA: 5:45 - loss: 5.3767 - acc: 0.004 - ETA: 5:43 - loss: 5.3565 - acc: 0.004 - ETA: 5:42 - loss: 5.3371 - acc: 0.004 - ETA: 5:40 - loss: 5.3191 - acc: 0.003 - ETA: 5:38 - loss: 5.3039 - acc: 0.005 - ETA: 5:37 - loss: 5.2885 - acc: 0.005 - ETA: 5:35 - loss: 5.2726 - acc: 0.005 - ETA: 5:34 - loss: 5.2609 - acc: 0.005 - ETA: 5:34 - loss: 5.2484 - acc: 0.006 - ETA: 5:32 - loss: 5.2367 - acc: 0.006 - ETA: 5:31 - loss: 5.2268 - acc: 0.006 - ETA: 5:30 - loss: 5.2157 - acc: 0.005 - ETA: 5:29 - loss: 5.2067 - acc: 0.005 - ETA: 5:28 - loss: 5.1986 - acc: 0.005 - ETA: 5:27 - loss: 5.1899 - acc: 0.005 - ETA: 5:25 - loss: 5.1818 - acc: 0.005 - ETA: 5:24 - loss: 5.1749 - acc: 0.005 - ETA: 5:22 - loss: 5.1681 - acc: 0.005 - ETA: 5:21 - loss: 5.1616 - acc: 0.004 - ETA: 5:20 - loss: 5.1554 - acc: 0.004 - ETA: 5:18 - loss: 5.1492 - acc: 0.004 - ETA: 5:17 - loss: 5.1433 - acc: 0.004 - ETA: 5:16 - loss: 5.1379 - acc: 0.004 - ETA: 5:16 - loss: 5.1322 - acc: 0.004 - ETA: 5:15 - loss: 5.1276 - acc: 0.004 - ETA: 5:13 - loss: 5.1227 - acc: 0.004 - ETA: 5:12 - loss: 5.1177 - acc: 0.004 - ETA: 5:11 - loss: 5.1134 - acc: 0.004 - ETA: 5:10 - loss: 5.1091 - acc: 0.003 - ETA: 5:08 - loss: 5.1047 - acc: 0.003 - ETA: 5:07 - loss: 5.1001 - acc: 0.004 - ETA: 5:06 - loss: 5.0962 - acc: 0.004 - ETA: 5:05 - loss: 5.0907 - acc: 0.006 - ETA: 5:04 - loss: 5.0884 - acc: 0.007 - ETA: 5:02 - loss: 5.0849 - acc: 0.007 - ETA: 5:01 - loss: 5.0822 - acc: 0.007 - ETA: 5:00 - loss: 5.0789 - acc: 0.008 - ETA: 4:59 - loss: 5.0759 - acc: 0.008 - ETA: 4:59 - loss: 5.0729 - acc: 0.008 - ETA: 4:57 - loss: 5.0699 - acc: 0.008 - ETA: 4:56 - loss: 5.0665 - acc: 0.007 - ETA: 4:55 - loss: 5.0624 - acc: 0.008 - ETA: 4:54 - loss: 5.0617 - acc: 0.008 - ETA: 4:53 - loss: 5.0583 - acc: 0.008 - ETA: 4:51 - loss: 5.0556 - acc: 0.008 - ETA: 4:50 - loss: 5.0537 - acc: 0.008 - ETA: 4:49 - loss: 5.0514 - acc: 0.008 - ETA: 4:48 - loss: 5.0487 - acc: 0.008 - ETA: 4:47 - loss: 5.0465 - acc: 0.008 - ETA: 4:46 - loss: 5.0437 - acc: 0.008 - ETA: 4:44 - loss: 5.0413 - acc: 0.008 - ETA: 4:43 - loss: 5.0398 - acc: 0.008 - ETA: 4:43 - loss: 5.0375 - acc: 0.008 - ETA: 4:42 - loss: 5.0354 - acc: 0.007 - ETA: 4:41 - loss: 5.0336 - acc: 0.008 - ETA: 4:39 - loss: 5.0310 - acc: 0.008 - ETA: 4:38 - loss: 5.0292 - acc: 0.008 - ETA: 4:37 - loss: 5.0271 - acc: 0.008 - ETA: 4:36 - loss: 5.0252 - acc: 0.008 - ETA: 4:35 - loss: 5.0229 - acc: 0.008 - ETA: 4:34 - loss: 5.0210 - acc: 0.008 - ETA: 4:33 - loss: 5.0199 - acc: 0.008 - ETA: 4:32 - loss: 5.0184 - acc: 0.008 - ETA: 4:30 - loss: 5.0176 - acc: 0.008 - ETA: 4:29 - loss: 5.0164 - acc: 0.008 - ETA: 4:28 - loss: 5.0148 - acc: 0.008 - ETA: 4:27 - loss: 5.0131 - acc: 0.008 - ETA: 4:26 - loss: 5.0124 - acc: 0.008 - ETA: 4:25 - loss: 5.0110 - acc: 0.008 - ETA: 4:24 - loss: 5.0096 - acc: 0.008 - ETA: 4:23 - loss: 5.0082 - acc: 0.008 - ETA: 4:22 - loss: 5.0065 - acc: 0.008 - ETA: 4:21 - loss: 5.0065 - acc: 0.007 - ETA: 4:20 - loss: 5.0053 - acc: 0.007 - ETA: 4:18 - loss: 5.0042 - acc: 0.007 - ETA: 4:17 - loss: 5.0030 - acc: 0.007 - ETA: 4:16 - loss: 5.0019 - acc: 0.007 - ETA: 4:15 - loss: 5.0008 - acc: 0.007 - ETA: 4:14 - loss: 4.9995 - acc: 0.007 - ETA: 4:13 - loss: 4.9985 - acc: 0.007 - ETA: 4:12 - loss: 4.9975 - acc: 0.007 - ETA: 4:11 - loss: 4.9965 - acc: 0.007 - ETA: 4:10 - loss: 4.9949 - acc: 0.007 - ETA: 4:09 - loss: 4.9946 - acc: 0.007 - ETA: 4:08 - loss: 4.9934 - acc: 0.007 - ETA: 4:07 - loss: 4.9926 - acc: 0.006 - ETA: 4:05 - loss: 4.9917 - acc: 0.006 - ETA: 4:04 - loss: 4.9906 - acc: 0.006 - ETA: 4:03 - loss: 4.9895 - acc: 0.006 - ETA: 4:02 - loss: 4.9882 - acc: 0.006 - ETA: 4:01 - loss: 4.9880 - acc: 0.006 - ETA: 4:00 - loss: 4.9872 - acc: 0.006 - ETA: 3:59 - loss: 4.9861 - acc: 0.006 - ETA: 3:57 - loss: 4.9851 - acc: 0.006 - ETA: 3:56 - loss: 4.9841 - acc: 0.006 - ETA: 3:55 - loss: 4.9835 - acc: 0.006 - ETA: 3:54 - loss: 4.9827 - acc: 0.006 - ETA: 3:53 - loss: 4.9820 - acc: 0.006 - ETA: 3:52 - loss: 4.9803 - acc: 0.007 - ETA: 3:51 - loss: 4.9816 - acc: 0.007 - ETA: 3:50 - loss: 4.9807 - acc: 0.006 - ETA: 3:49 - loss: 4.9801 - acc: 0.006 - ETA: 3:48 - loss: 4.9796 - acc: 0.006 - ETA: 3:46 - loss: 4.9786 - acc: 0.006 - ETA: 3:45 - loss: 4.9779 - acc: 0.006 - ETA: 3:44 - loss: 4.9774 - acc: 0.006 - ETA: 3:43 - loss: 4.9764 - acc: 0.006 - ETA: 3:42 - loss: 4.9758 - acc: 0.006 - ETA: 3:41 - loss: 4.9750 - acc: 0.007 - ETA: 3:40 - loss: 4.9739 - acc: 0.007 - ETA: 3:39 - loss: 4.9726 - acc: 0.008 - ETA: 3:38 - loss: 4.9719 - acc: 0.008 - ETA: 3:37 - loss: 4.9714 - acc: 0.008 - ETA: 3:36 - loss: 4.9714 - acc: 0.008 - ETA: 3:35 - loss: 4.9709 - acc: 0.008 - ETA: 3:33 - loss: 4.9702 - acc: 0.008 - ETA: 3:32 - loss: 4.9696 - acc: 0.008 - ETA: 3:31 - loss: 4.9689 - acc: 0.008 - ETA: 3:30 - loss: 4.9680 - acc: 0.008 - ETA: 3:29 - loss: 4.9672 - acc: 0.008 - ETA: 3:28 - loss: 4.9666 - acc: 0.008 - ETA: 3:27 - loss: 4.9661 - acc: 0.008 - ETA: 3:26 - loss: 4.9656 - acc: 0.008 - ETA: 3:25 - loss: 4.9654 - acc: 0.008 - ETA: 3:23 - loss: 4.9649 - acc: 0.008 - ETA: 3:22 - loss: 4.9643 - acc: 0.008 - ETA: 3:21 - loss: 4.9636 - acc: 0.008 - ETA: 3:20 - loss: 4.9627 - acc: 0.008 - ETA: 3:19 - loss: 4.9624 - acc: 0.008 - ETA: 3:18 - loss: 4.9620 - acc: 0.008 - ETA: 3:17 - loss: 4.9615 - acc: 0.008 - ETA: 3:16 - loss: 4.9611 - acc: 0.008 - ETA: 3:15 - loss: 4.9604 - acc: 0.008 - ETA: 3:14 - loss: 4.9605 - acc: 0.008 - ETA: 3:13 - loss: 4.9599 - acc: 0.008 - ETA: 3:11 - loss: 4.9594 - acc: 0.008 - ETA: 3:10 - loss: 4.9589 - acc: 0.008 - ETA: 3:09 - loss: 4.9582 - acc: 0.008 - ETA: 3:08 - loss: 4.9578 - acc: 0.008 - ETA: 3:07 - loss: 4.9571 - acc: 0.009 - ETA: 3:06 - loss: 4.9566 - acc: 0.009 - ETA: 3:05 - loss: 4.9557 - acc: 0.009 - ETA: 3:04 - loss: 4.9559 - acc: 0.009 - ETA: 3:03 - loss: 4.9555 - acc: 0.009 - ETA: 3:02 - loss: 4.9553 - acc: 0.009 - ETA: 3:01 - loss: 4.9549 - acc: 0.009 - ETA: 3:00 - loss: 4.9545 - acc: 0.009 - ETA: 2:58 - loss: 4.9540 - acc: 0.009 - ETA: 2:57 - loss: 4.9535 - acc: 0.009 - ETA: 2:56 - loss: 4.9529 - acc: 0.009 - ETA: 2:55 - loss: 4.9518 - acc: 0.010 - ETA: 2:54 - loss: 4.9510 - acc: 0.010 - ETA: 2:53 - loss: 4.9510 - acc: 0.010 - ETA: 2:52 - loss: 4.9503 - acc: 0.009 - ETA: 2:51 - loss: 4.9494 - acc: 0.009 - ETA: 2:50 - loss: 4.9484 - acc: 0.009 - ETA: 2:49 - loss: 4.9483 - acc: 0.010 - ETA: 2:48 - loss: 4.9477 - acc: 0.010 - ETA: 2:46 - loss: 4.9473 - acc: 0.010 - ETA: 2:45 - loss: 4.9471 - acc: 0.010 - ETA: 2:44 - loss: 4.9466 - acc: 0.010 - ETA: 2:43 - loss: 4.9464 - acc: 0.010 - ETA: 2:42 - loss: 4.9466 - acc: 0.010 - ETA: 2:41 - loss: 4.9461 - acc: 0.010 - ETA: 2:40 - loss: 4.9457 - acc: 0.010 - ETA: 2:39 - loss: 4.9452 - acc: 0.010 - ETA: 2:38 - loss: 4.9447 - acc: 0.010 - ETA: 2:37 - loss: 4.9447 - acc: 0.010 - ETA: 2:35 - loss: 4.9439 - acc: 0.010 - ETA: 2:34 - loss: 4.9436 - acc: 0.010 - ETA: 2:34 - loss: 4.9434 - acc: 0.010 - ETA: 2:32 - loss: 4.9431 - acc: 0.010 - ETA: 2:31 - loss: 4.9426 - acc: 0.010 - ETA: 2:30 - loss: 4.9420 - acc: 0.010 - ETA: 2:29 - loss: 4.9417 - acc: 0.010 - ETA: 2:28 - loss: 4.9417 - acc: 0.010 - ETA: 2:27 - loss: 4.9411 - acc: 0.010 - ETA: 2:26 - loss: 4.9405 - acc: 0.010 - ETA: 2:25 - loss: 4.9402 - acc: 0.01006660/6680 [============================>.] - ETA: 2:24 - loss: 4.9396 - acc: 0.010 - ETA: 2:22 - loss: 4.9390 - acc: 0.010 - ETA: 2:21 - loss: 4.9381 - acc: 0.010 - ETA: 2:20 - loss: 4.9380 - acc: 0.010 - ETA: 2:19 - loss: 4.9373 - acc: 0.010 - ETA: 2:18 - loss: 4.9369 - acc: 0.010 - ETA: 2:17 - loss: 4.9365 - acc: 0.010 - ETA: 2:16 - loss: 4.9357 - acc: 0.011 - ETA: 2:15 - loss: 4.9350 - acc: 0.011 - ETA: 2:14 - loss: 4.9351 - acc: 0.011 - ETA: 2:13 - loss: 4.9341 - acc: 0.011 - ETA: 2:12 - loss: 4.9333 - acc: 0.011 - ETA: 2:10 - loss: 4.9331 - acc: 0.011 - ETA: 2:09 - loss: 4.9329 - acc: 0.010 - ETA: 2:08 - loss: 4.9323 - acc: 0.010 - ETA: 2:07 - loss: 4.9319 - acc: 0.010 - ETA: 2:06 - loss: 4.9319 - acc: 0.010 - ETA: 2:05 - loss: 4.9314 - acc: 0.011 - ETA: 2:04 - loss: 4.9308 - acc: 0.011 - ETA: 2:03 - loss: 4.9310 - acc: 0.011 - ETA: 2:02 - loss: 4.9308 - acc: 0.011 - ETA: 2:01 - loss: 4.9305 - acc: 0.011 - ETA: 2:00 - loss: 4.9302 - acc: 0.011 - ETA: 1:58 - loss: 4.9302 - acc: 0.011 - ETA: 1:57 - loss: 4.9300 - acc: 0.011 - ETA: 1:56 - loss: 4.9294 - acc: 0.011 - ETA: 1:55 - loss: 4.9296 - acc: 0.011 - ETA: 1:54 - loss: 4.9291 - acc: 0.011 - ETA: 1:53 - loss: 4.9285 - acc: 0.011 - ETA: 1:52 - loss: 4.9282 - acc: 0.011 - ETA: 1:51 - loss: 4.9277 - acc: 0.011 - ETA: 1:50 - loss: 4.9277 - acc: 0.012 - ETA: 1:49 - loss: 4.9272 - acc: 0.012 - ETA: 1:47 - loss: 4.9267 - acc: 0.011 - ETA: 1:46 - loss: 4.9268 - acc: 0.011 - ETA: 1:45 - loss: 4.9262 - acc: 0.012 - ETA: 1:44 - loss: 4.9259 - acc: 0.012 - ETA: 1:43 - loss: 4.9251 - acc: 0.011 - ETA: 1:42 - loss: 4.9245 - acc: 0.011 - ETA: 1:41 - loss: 4.9240 - acc: 0.011 - ETA: 1:40 - loss: 4.9243 - acc: 0.011 - ETA: 1:39 - loss: 4.9238 - acc: 0.011 - ETA: 1:38 - loss: 4.9235 - acc: 0.012 - ETA: 1:37 - loss: 4.9234 - acc: 0.012 - ETA: 1:35 - loss: 4.9227 - acc: 0.012 - ETA: 1:34 - loss: 4.9219 - acc: 0.012 - ETA: 1:33 - loss: 4.9210 - acc: 0.012 - ETA: 1:32 - loss: 4.9205 - acc: 0.012 - ETA: 1:31 - loss: 4.9201 - acc: 0.012 - ETA: 1:30 - loss: 4.9194 - acc: 0.012 - ETA: 1:29 - loss: 4.9187 - acc: 0.012 - ETA: 1:28 - loss: 4.9180 - acc: 0.013 - ETA: 1:27 - loss: 4.9187 - acc: 0.013 - ETA: 1:26 - loss: 4.9177 - acc: 0.013 - ETA: 1:24 - loss: 4.9168 - acc: 0.013 - ETA: 1:23 - loss: 4.9165 - acc: 0.013 - ETA: 1:22 - loss: 4.9165 - acc: 0.013 - ETA: 1:21 - loss: 4.9163 - acc: 0.013 - ETA: 1:20 - loss: 4.9156 - acc: 0.013 - ETA: 1:19 - loss: 4.9147 - acc: 0.014 - ETA: 1:18 - loss: 4.9145 - acc: 0.013 - ETA: 1:17 - loss: 4.9143 - acc: 0.013 - ETA: 1:16 - loss: 4.9139 - acc: 0.013 - ETA: 1:15 - loss: 4.9134 - acc: 0.013 - ETA: 1:14 - loss: 4.9121 - acc: 0.013 - ETA: 1:12 - loss: 4.9120 - acc: 0.014 - ETA: 1:11 - loss: 4.9114 - acc: 0.014 - ETA: 1:10 - loss: 4.9108 - acc: 0.014 - ETA: 1:09 - loss: 4.9107 - acc: 0.014 - ETA: 1:08 - loss: 4.9101 - acc: 0.014 - ETA: 1:07 - loss: 4.9102 - acc: 0.014 - ETA: 1:06 - loss: 4.9096 - acc: 0.014 - ETA: 1:05 - loss: 4.9090 - acc: 0.014 - ETA: 1:04 - loss: 4.9085 - acc: 0.014 - ETA: 1:03 - loss: 4.9085 - acc: 0.014 - ETA: 1:02 - loss: 4.9071 - acc: 0.015 - ETA: 1:00 - loss: 4.9064 - acc: 0.014 - ETA: 59s - loss: 4.9063 - acc: 0.014 - ETA: 58s - loss: 4.9061 - acc: 0.01 - ETA: 57s - loss: 4.9061 - acc: 0.01 - ETA: 56s - loss: 4.9058 - acc: 0.01 - ETA: 55s - loss: 4.9051 - acc: 0.01 - ETA: 54s - loss: 4.9048 - acc: 0.01 - ETA: 53s - loss: 4.9052 - acc: 0.01 - ETA: 52s - loss: 4.9052 - acc: 0.01 - ETA: 51s - loss: 4.9048 - acc: 0.01 - ETA: 50s - loss: 4.9046 - acc: 0.01 - ETA: 48s - loss: 4.9045 - acc: 0.01 - ETA: 47s - loss: 4.9045 - acc: 0.01 - ETA: 46s - loss: 4.9044 - acc: 0.01 - ETA: 45s - loss: 4.9039 - acc: 0.01 - ETA: 44s - loss: 4.9038 - acc: 0.01 - ETA: 43s - loss: 4.9036 - acc: 0.01 - ETA: 42s - loss: 4.9033 - acc: 0.01 - ETA: 41s - loss: 4.9035 - acc: 0.01 - ETA: 40s - loss: 4.9029 - acc: 0.01 - ETA: 39s - loss: 4.9028 - acc: 0.01 - ETA: 38s - loss: 4.9028 - acc: 0.01 - ETA: 37s - loss: 4.9026 - acc: 0.01 - ETA: 35s - loss: 4.9021 - acc: 0.01 - ETA: 34s - loss: 4.9018 - acc: 0.01 - ETA: 33s - loss: 4.9010 - acc: 0.01 - ETA: 32s - loss: 4.9008 - acc: 0.01 - ETA: 31s - loss: 4.9000 - acc: 0.01 - ETA: 30s - loss: 4.8984 - acc: 0.01 - ETA: 29s - loss: 4.8978 - acc: 0.01 - ETA: 28s - loss: 4.8975 - acc: 0.01 - ETA: 27s - loss: 4.8968 - acc: 0.01 - ETA: 26s - loss: 4.8966 - acc: 0.01 - ETA: 25s - loss: 4.8957 - acc: 0.01 - ETA: 23s - loss: 4.8956 - acc: 0.01 - ETA: 22s - loss: 4.8962 - acc: 0.01 - ETA: 21s - loss: 4.8959 - acc: 0.01 - ETA: 20s - loss: 4.8951 - acc: 0.01 - ETA: 19s - loss: 4.8944 - acc: 0.01 - ETA: 18s - loss: 4.8940 - acc: 0.01 - ETA: 17s - loss: 4.8935 - acc: 0.01 - ETA: 16s - loss: 4.8933 - acc: 0.01 - ETA: 15s - loss: 4.8930 - acc: 0.01 - ETA: 14s - loss: 4.8918 - acc: 0.01 - ETA: 13s - loss: 4.8914 - acc: 0.01 - ETA: 11s - loss: 4.8909 - acc: 0.01 - ETA: 10s - loss: 4.8907 - acc: 0.01 - ETA: 9s - loss: 4.8901 - acc: 0.0162 - ETA: 8s - loss: 4.8898 - acc: 0.016 - ETA: 7s - loss: 4.8891 - acc: 0.016 - ETA: 6s - loss: 4.8886 - acc: 0.016 - ETA: 5s - loss: 4.8881 - acc: 0.016 - ETA: 4s - loss: 4.8884 - acc: 0.016 - ETA: 3s - loss: 4.8880 - acc: 0.016 - ETA: 2s - loss: 4.8876 - acc: 0.016 - ETA: 1s - loss: 4.8872 - acc: 0.0167Epoch 00001: val\_loss improved from inf to 4.72099, saving model to saved\_models/weights.best.from\_scratch.hdf5
6680/6680 [==============================] - 375s 56ms/step - loss: 4.8866 - acc: 0.0166 - val\_loss: 4.7210 - val\_acc: 0.0251
Epoch 2/8
4080/6680 [=================>{\ldots}] - ETA: 5:46 - loss: 4.8445 - acc: 0.0000e+0 - ETA: 5:53 - loss: 4.7568 - acc: 0.0250    - ETA: 5:51 - loss: 4.7349 - acc: 0.033 - ETA: 5:51 - loss: 4.7075 - acc: 0.037 - ETA: 5:53 - loss: 4.6951 - acc: 0.030 - ETA: 5:51 - loss: 4.7183 - acc: 0.025 - ETA: 5:49 - loss: 4.7146 - acc: 0.021 - ETA: 5:48 - loss: 4.7022 - acc: 0.025 - ETA: 5:49 - loss: 4.6624 - acc: 0.027 - ETA: 5:52 - loss: 4.6413 - acc: 0.035 - ETA: 5:52 - loss: 4.6367 - acc: 0.031 - ETA: 5:50 - loss: 4.6383 - acc: 0.033 - ETA: 5:49 - loss: 4.6236 - acc: 0.034 - ETA: 5:47 - loss: 4.6147 - acc: 0.035 - ETA: 5:46 - loss: 4.6511 - acc: 0.033 - ETA: 5:45 - loss: 4.6526 - acc: 0.031 - ETA: 5:44 - loss: 4.6501 - acc: 0.029 - ETA: 5:42 - loss: 4.6446 - acc: 0.030 - ETA: 5:41 - loss: 4.6521 - acc: 0.028 - ETA: 5:40 - loss: 4.6479 - acc: 0.027 - ETA: 5:38 - loss: 4.6422 - acc: 0.028 - ETA: 5:38 - loss: 4.6302 - acc: 0.031 - ETA: 5:36 - loss: 4.6316 - acc: 0.030 - ETA: 5:36 - loss: 4.6373 - acc: 0.029 - ETA: 5:36 - loss: 4.6317 - acc: 0.032 - ETA: 5:35 - loss: 4.6312 - acc: 0.032 - ETA: 5:34 - loss: 4.6191 - acc: 0.035 - ETA: 5:33 - loss: 4.6281 - acc: 0.033 - ETA: 5:31 - loss: 4.6251 - acc: 0.034 - ETA: 5:30 - loss: 4.6335 - acc: 0.033 - ETA: 5:29 - loss: 4.6390 - acc: 0.032 - ETA: 5:28 - loss: 4.6369 - acc: 0.031 - ETA: 5:27 - loss: 4.6412 - acc: 0.030 - ETA: 5:26 - loss: 4.6416 - acc: 0.032 - ETA: 5:25 - loss: 4.6340 - acc: 0.032 - ETA: 5:24 - loss: 4.6365 - acc: 0.033 - ETA: 5:23 - loss: 4.6414 - acc: 0.032 - ETA: 5:22 - loss: 4.6383 - acc: 0.034 - ETA: 5:21 - loss: 4.6440 - acc: 0.034 - ETA: 5:21 - loss: 4.6436 - acc: 0.036 - ETA: 5:20 - loss: 4.6408 - acc: 0.036 - ETA: 5:19 - loss: 4.6438 - acc: 0.035 - ETA: 5:17 - loss: 4.6487 - acc: 0.036 - ETA: 5:16 - loss: 4.6459 - acc: 0.036 - ETA: 5:15 - loss: 4.6466 - acc: 0.036 - ETA: 5:14 - loss: 4.6390 - acc: 0.037 - ETA: 5:13 - loss: 4.6428 - acc: 0.036 - ETA: 5:12 - loss: 4.6386 - acc: 0.035 - ETA: 5:10 - loss: 4.6387 - acc: 0.034 - ETA: 5:09 - loss: 4.6406 - acc: 0.034 - ETA: 5:08 - loss: 4.6408 - acc: 0.036 - ETA: 5:07 - loss: 4.6388 - acc: 0.035 - ETA: 5:06 - loss: 4.6402 - acc: 0.034 - ETA: 5:05 - loss: 4.6337 - acc: 0.034 - ETA: 5:05 - loss: 4.6358 - acc: 0.033 - ETA: 5:03 - loss: 4.6342 - acc: 0.033 - ETA: 5:02 - loss: 4.6315 - acc: 0.033 - ETA: 5:01 - loss: 4.6307 - acc: 0.032 - ETA: 5:00 - loss: 4.6336 - acc: 0.032 - ETA: 4:59 - loss: 4.6324 - acc: 0.032 - ETA: 4:58 - loss: 4.6326 - acc: 0.033 - ETA: 4:56 - loss: 4.6316 - acc: 0.033 - ETA: 4:55 - loss: 4.6325 - acc: 0.032 - ETA: 4:54 - loss: 4.6339 - acc: 0.032 - ETA: 4:53 - loss: 4.6345 - acc: 0.032 - ETA: 4:52 - loss: 4.6310 - acc: 0.032 - ETA: 4:51 - loss: 4.6276 - acc: 0.032 - ETA: 4:50 - loss: 4.6303 - acc: 0.032 - ETA: 4:50 - loss: 4.6288 - acc: 0.032 - ETA: 4:49 - loss: 4.6208 - acc: 0.032 - ETA: 4:48 - loss: 4.6211 - acc: 0.032 - ETA: 4:46 - loss: 4.6187 - acc: 0.033 - ETA: 4:45 - loss: 4.6185 - acc: 0.032 - ETA: 4:44 - loss: 4.6159 - acc: 0.033 - ETA: 4:43 - loss: 4.6161 - acc: 0.033 - ETA: 4:42 - loss: 4.6144 - acc: 0.033 - ETA: 4:41 - loss: 4.6123 - acc: 0.033 - ETA: 4:39 - loss: 4.6136 - acc: 0.033 - ETA: 4:38 - loss: 4.6116 - acc: 0.033 - ETA: 4:37 - loss: 4.6125 - acc: 0.033 - ETA: 4:36 - loss: 4.6116 - acc: 0.033 - ETA: 4:35 - loss: 4.6100 - acc: 0.032 - ETA: 4:34 - loss: 4.6074 - acc: 0.033 - ETA: 4:33 - loss: 4.6057 - acc: 0.033 - ETA: 4:32 - loss: 4.6062 - acc: 0.032 - ETA: 4:31 - loss: 4.6012 - acc: 0.032 - ETA: 4:30 - loss: 4.6008 - acc: 0.032 - ETA: 4:29 - loss: 4.5995 - acc: 0.032 - ETA: 4:27 - loss: 4.5972 - acc: 0.033 - ETA: 4:26 - loss: 4.5980 - acc: 0.033 - ETA: 4:25 - loss: 4.5950 - acc: 0.033 - ETA: 4:24 - loss: 4.5972 - acc: 0.033 - ETA: 4:23 - loss: 4.5990 - acc: 0.033 - ETA: 4:22 - loss: 4.5969 - acc: 0.033 - ETA: 4:21 - loss: 4.5989 - acc: 0.032 - ETA: 4:19 - loss: 4.6035 - acc: 0.033 - ETA: 4:18 - loss: 4.6038 - acc: 0.033 - ETA: 4:18 - loss: 4.6032 - acc: 0.032 - ETA: 4:17 - loss: 4.6044 - acc: 0.032 - ETA: 4:15 - loss: 4.6048 - acc: 0.033 - ETA: 4:14 - loss: 4.6028 - acc: 0.033 - ETA: 4:13 - loss: 4.5998 - acc: 0.034 - ETA: 4:12 - loss: 4.6010 - acc: 0.035 - ETA: 4:11 - loss: 4.6031 - acc: 0.035 - ETA: 4:10 - loss: 4.6008 - acc: 0.035 - ETA: 4:09 - loss: 4.6035 - acc: 0.034 - ETA: 4:07 - loss: 4.6006 - acc: 0.035 - ETA: 4:06 - loss: 4.5984 - acc: 0.035 - ETA: 4:05 - loss: 4.5998 - acc: 0.034 - ETA: 4:04 - loss: 4.6014 - acc: 0.035 - ETA: 4:03 - loss: 4.6000 - acc: 0.035 - ETA: 4:02 - loss: 4.6017 - acc: 0.035 - ETA: 4:01 - loss: 4.6011 - acc: 0.035 - ETA: 4:00 - loss: 4.6012 - acc: 0.035 - ETA: 3:59 - loss: 4.5987 - acc: 0.037 - ETA: 3:58 - loss: 4.5998 - acc: 0.037 - ETA: 3:56 - loss: 4.5976 - acc: 0.037 - ETA: 3:55 - loss: 4.5972 - acc: 0.036 - ETA: 3:54 - loss: 4.5985 - acc: 0.036 - ETA: 3:53 - loss: 4.5990 - acc: 0.036 - ETA: 3:52 - loss: 4.5966 - acc: 0.037 - ETA: 3:51 - loss: 4.5967 - acc: 0.036 - ETA: 3:50 - loss: 4.5953 - acc: 0.037 - ETA: 3:49 - loss: 4.5940 - acc: 0.037 - ETA: 3:47 - loss: 4.5922 - acc: 0.037 - ETA: 3:46 - loss: 4.5924 - acc: 0.038 - ETA: 3:45 - loss: 4.5921 - acc: 0.039 - ETA: 3:44 - loss: 4.5886 - acc: 0.039 - ETA: 3:43 - loss: 4.5874 - acc: 0.039 - ETA: 3:42 - loss: 4.5860 - acc: 0.038 - ETA: 3:41 - loss: 4.5839 - acc: 0.038 - ETA: 3:40 - loss: 4.5858 - acc: 0.038 - ETA: 3:39 - loss: 4.5848 - acc: 0.038 - ETA: 3:38 - loss: 4.5830 - acc: 0.038 - ETA: 3:36 - loss: 4.5829 - acc: 0.038 - ETA: 3:35 - loss: 4.5821 - acc: 0.037 - ETA: 3:34 - loss: 4.5826 - acc: 0.037 - ETA: 3:33 - loss: 4.5823 - acc: 0.037 - ETA: 3:32 - loss: 4.5829 - acc: 0.038 - ETA: 3:31 - loss: 4.5821 - acc: 0.038 - ETA: 3:30 - loss: 4.5831 - acc: 0.038 - ETA: 3:29 - loss: 4.5825 - acc: 0.038 - ETA: 3:28 - loss: 4.5814 - acc: 0.038 - ETA: 3:27 - loss: 4.5819 - acc: 0.038 - ETA: 3:26 - loss: 4.5819 - acc: 0.037 - ETA: 3:24 - loss: 4.5823 - acc: 0.038 - ETA: 3:23 - loss: 4.5830 - acc: 0.038 - ETA: 3:22 - loss: 4.5831 - acc: 0.038 - ETA: 3:21 - loss: 4.5813 - acc: 0.038 - ETA: 3:20 - loss: 4.5798 - acc: 0.038 - ETA: 3:19 - loss: 4.5779 - acc: 0.039 - ETA: 3:18 - loss: 4.5772 - acc: 0.039 - ETA: 3:17 - loss: 4.5748 - acc: 0.040 - ETA: 3:16 - loss: 4.5756 - acc: 0.039 - ETA: 3:14 - loss: 4.5757 - acc: 0.040 - ETA: 3:13 - loss: 4.5761 - acc: 0.040 - ETA: 3:12 - loss: 4.5745 - acc: 0.040 - ETA: 3:11 - loss: 4.5716 - acc: 0.040 - ETA: 3:10 - loss: 4.5716 - acc: 0.040 - ETA: 3:09 - loss: 4.5728 - acc: 0.040 - ETA: 3:08 - loss: 4.5712 - acc: 0.040 - ETA: 3:07 - loss: 4.5729 - acc: 0.040 - ETA: 3:06 - loss: 4.5732 - acc: 0.039 - ETA: 3:05 - loss: 4.5697 - acc: 0.040 - ETA: 3:04 - loss: 4.5699 - acc: 0.040 - ETA: 3:03 - loss: 4.5710 - acc: 0.041 - ETA: 3:01 - loss: 4.5701 - acc: 0.041 - ETA: 3:00 - loss: 4.5720 - acc: 0.041 - ETA: 2:59 - loss: 4.5712 - acc: 0.041 - ETA: 2:58 - loss: 4.5712 - acc: 0.041 - ETA: 2:57 - loss: 4.5732 - acc: 0.041 - ETA: 2:56 - loss: 4.5725 - acc: 0.041 - ETA: 2:55 - loss: 4.5703 - acc: 0.041 - ETA: 2:54 - loss: 4.5695 - acc: 0.042 - ETA: 2:53 - loss: 4.5712 - acc: 0.041 - ETA: 2:52 - loss: 4.5708 - acc: 0.041 - ETA: 2:51 - loss: 4.5681 - acc: 0.041 - ETA: 2:49 - loss: 4.5665 - acc: 0.041 - ETA: 2:48 - loss: 4.5672 - acc: 0.041 - ETA: 2:47 - loss: 4.5688 - acc: 0.041 - ETA: 2:46 - loss: 4.5681 - acc: 0.042 - ETA: 2:45 - loss: 4.5682 - acc: 0.041 - ETA: 2:44 - loss: 4.5669 - acc: 0.042 - ETA: 2:43 - loss: 4.5658 - acc: 0.042 - ETA: 2:42 - loss: 4.5661 - acc: 0.041 - ETA: 2:41 - loss: 4.5641 - acc: 0.042 - ETA: 2:40 - loss: 4.5635 - acc: 0.042 - ETA: 2:39 - loss: 4.5631 - acc: 0.042 - ETA: 2:38 - loss: 4.5615 - acc: 0.042 - ETA: 2:36 - loss: 4.5599 - acc: 0.042 - ETA: 2:35 - loss: 4.5603 - acc: 0.042 - ETA: 2:34 - loss: 4.5607 - acc: 0.042 - ETA: 2:33 - loss: 4.5597 - acc: 0.042 - ETA: 2:32 - loss: 4.5599 - acc: 0.042 - ETA: 2:31 - loss: 4.5605 - acc: 0.041 - ETA: 2:30 - loss: 4.5601 - acc: 0.041 - ETA: 2:29 - loss: 4.5592 - acc: 0.042 - ETA: 2:28 - loss: 4.5585 - acc: 0.041 - ETA: 2:27 - loss: 4.5581 - acc: 0.042 - ETA: 2:25 - loss: 4.5571 - acc: 0.042 - ETA: 2:24 - loss: 4.5570 - acc: 0.042 - ETA: 2:23 - loss: 4.5571 - acc: 0.042 - ETA: 2:22 - loss: 4.5571 - acc: 0.042 - ETA: 2:21 - loss: 4.5565 - acc: 0.04266660/6680 [============================>.] - ETA: 2:20 - loss: 4.5560 - acc: 0.042 - ETA: 2:19 - loss: 4.5554 - acc: 0.043 - ETA: 2:18 - loss: 4.5555 - acc: 0.043 - ETA: 2:17 - loss: 4.5557 - acc: 0.042 - ETA: 2:16 - loss: 4.5549 - acc: 0.043 - ETA: 2:14 - loss: 4.5548 - acc: 0.043 - ETA: 2:13 - loss: 4.5545 - acc: 0.043 - ETA: 2:12 - loss: 4.5528 - acc: 0.043 - ETA: 2:11 - loss: 4.5523 - acc: 0.043 - ETA: 2:10 - loss: 4.5514 - acc: 0.044 - ETA: 2:09 - loss: 4.5495 - acc: 0.044 - ETA: 2:08 - loss: 4.5502 - acc: 0.044 - ETA: 2:07 - loss: 4.5506 - acc: 0.044 - ETA: 2:06 - loss: 4.5494 - acc: 0.044 - ETA: 2:05 - loss: 4.5497 - acc: 0.044 - ETA: 2:03 - loss: 4.5472 - acc: 0.044 - ETA: 2:02 - loss: 4.5473 - acc: 0.043 - ETA: 2:01 - loss: 4.5474 - acc: 0.043 - ETA: 2:00 - loss: 4.5474 - acc: 0.043 - ETA: 1:59 - loss: 4.5474 - acc: 0.044 - ETA: 1:58 - loss: 4.5466 - acc: 0.044 - ETA: 1:57 - loss: 4.5463 - acc: 0.044 - ETA: 1:56 - loss: 4.5452 - acc: 0.043 - ETA: 1:55 - loss: 4.5451 - acc: 0.043 - ETA: 1:54 - loss: 4.5443 - acc: 0.044 - ETA: 1:52 - loss: 4.5436 - acc: 0.043 - ETA: 1:51 - loss: 4.5430 - acc: 0.043 - ETA: 1:50 - loss: 4.5419 - acc: 0.044 - ETA: 1:49 - loss: 4.5411 - acc: 0.044 - ETA: 1:48 - loss: 4.5415 - acc: 0.044 - ETA: 1:47 - loss: 4.5408 - acc: 0.044 - ETA: 1:46 - loss: 4.5406 - acc: 0.044 - ETA: 1:45 - loss: 4.5396 - acc: 0.044 - ETA: 1:44 - loss: 4.5397 - acc: 0.044 - ETA: 1:43 - loss: 4.5391 - acc: 0.044 - ETA: 1:42 - loss: 4.5397 - acc: 0.044 - ETA: 1:40 - loss: 4.5394 - acc: 0.044 - ETA: 1:39 - loss: 4.5395 - acc: 0.044 - ETA: 1:38 - loss: 4.5374 - acc: 0.044 - ETA: 1:37 - loss: 4.5367 - acc: 0.044 - ETA: 1:36 - loss: 4.5364 - acc: 0.044 - ETA: 1:35 - loss: 4.5354 - acc: 0.044 - ETA: 1:34 - loss: 4.5349 - acc: 0.044 - ETA: 1:33 - loss: 4.5348 - acc: 0.044 - ETA: 1:32 - loss: 4.5347 - acc: 0.044 - ETA: 1:31 - loss: 4.5347 - acc: 0.044 - ETA: 1:30 - loss: 4.5349 - acc: 0.044 - ETA: 1:28 - loss: 4.5339 - acc: 0.044 - ETA: 1:27 - loss: 4.5332 - acc: 0.044 - ETA: 1:26 - loss: 4.5337 - acc: 0.044 - ETA: 1:25 - loss: 4.5326 - acc: 0.044 - ETA: 1:24 - loss: 4.5331 - acc: 0.044 - ETA: 1:23 - loss: 4.5324 - acc: 0.044 - ETA: 1:22 - loss: 4.5323 - acc: 0.044 - ETA: 1:21 - loss: 4.5319 - acc: 0.044 - ETA: 1:20 - loss: 4.5311 - acc: 0.044 - ETA: 1:19 - loss: 4.5314 - acc: 0.043 - ETA: 1:18 - loss: 4.5317 - acc: 0.043 - ETA: 1:16 - loss: 4.5315 - acc: 0.043 - ETA: 1:15 - loss: 4.5317 - acc: 0.043 - ETA: 1:14 - loss: 4.5316 - acc: 0.043 - ETA: 1:13 - loss: 4.5308 - acc: 0.043 - ETA: 1:12 - loss: 4.5312 - acc: 0.043 - ETA: 1:11 - loss: 4.5299 - acc: 0.044 - ETA: 1:10 - loss: 4.5297 - acc: 0.043 - ETA: 1:09 - loss: 4.5295 - acc: 0.043 - ETA: 1:08 - loss: 4.5283 - acc: 0.043 - ETA: 1:07 - loss: 4.5293 - acc: 0.043 - ETA: 1:06 - loss: 4.5285 - acc: 0.044 - ETA: 1:04 - loss: 4.5289 - acc: 0.043 - ETA: 1:03 - loss: 4.5282 - acc: 0.043 - ETA: 1:02 - loss: 4.5287 - acc: 0.043 - ETA: 1:01 - loss: 4.5285 - acc: 0.043 - ETA: 1:00 - loss: 4.5287 - acc: 0.043 - ETA: 59s - loss: 4.5285 - acc: 0.043 - ETA: 58s - loss: 4.5283 - acc: 0.04 - ETA: 57s - loss: 4.5273 - acc: 0.04 - ETA: 56s - loss: 4.5265 - acc: 0.04 - ETA: 55s - loss: 4.5246 - acc: 0.04 - ETA: 54s - loss: 4.5244 - acc: 0.04 - ETA: 53s - loss: 4.5230 - acc: 0.04 - ETA: 51s - loss: 4.5239 - acc: 0.04 - ETA: 50s - loss: 4.5222 - acc: 0.04 - ETA: 49s - loss: 4.5207 - acc: 0.04 - ETA: 48s - loss: 4.5205 - acc: 0.04 - ETA: 47s - loss: 4.5193 - acc: 0.04 - ETA: 46s - loss: 4.5186 - acc: 0.04 - ETA: 45s - loss: 4.5183 - acc: 0.04 - ETA: 44s - loss: 4.5179 - acc: 0.04 - ETA: 43s - loss: 4.5181 - acc: 0.04 - ETA: 42s - loss: 4.5177 - acc: 0.04 - ETA: 41s - loss: 4.5167 - acc: 0.04 - ETA: 40s - loss: 4.5168 - acc: 0.04 - ETA: 39s - loss: 4.5149 - acc: 0.04 - ETA: 37s - loss: 4.5136 - acc: 0.04 - ETA: 36s - loss: 4.5143 - acc: 0.04 - ETA: 35s - loss: 4.5134 - acc: 0.04 - ETA: 34s - loss: 4.5137 - acc: 0.04 - ETA: 33s - loss: 4.5130 - acc: 0.04 - ETA: 32s - loss: 4.5110 - acc: 0.04 - ETA: 31s - loss: 4.5105 - acc: 0.04 - ETA: 30s - loss: 4.5103 - acc: 0.04 - ETA: 29s - loss: 4.5109 - acc: 0.04 - ETA: 28s - loss: 4.5099 - acc: 0.04 - ETA: 27s - loss: 4.5092 - acc: 0.04 - ETA: 25s - loss: 4.5098 - acc: 0.04 - ETA: 24s - loss: 4.5108 - acc: 0.04 - ETA: 23s - loss: 4.5116 - acc: 0.04 - ETA: 22s - loss: 4.5106 - acc: 0.04 - ETA: 21s - loss: 4.5099 - acc: 0.04 - ETA: 20s - loss: 4.5094 - acc: 0.04 - ETA: 19s - loss: 4.5094 - acc: 0.04 - ETA: 18s - loss: 4.5094 - acc: 0.04 - ETA: 17s - loss: 4.5094 - acc: 0.04 - ETA: 16s - loss: 4.5100 - acc: 0.04 - ETA: 15s - loss: 4.5094 - acc: 0.04 - ETA: 14s - loss: 4.5093 - acc: 0.04 - ETA: 12s - loss: 4.5087 - acc: 0.04 - ETA: 11s - loss: 4.5086 - acc: 0.04 - ETA: 10s - loss: 4.5089 - acc: 0.04 - ETA: 9s - loss: 4.5091 - acc: 0.0455 - ETA: 8s - loss: 4.5086 - acc: 0.045 - ETA: 7s - loss: 4.5080 - acc: 0.045 - ETA: 6s - loss: 4.5073 - acc: 0.045 - ETA: 5s - loss: 4.5086 - acc: 0.045 - ETA: 4s - loss: 4.5074 - acc: 0.045 - ETA: 3s - loss: 4.5082 - acc: 0.045 - ETA: 2s - loss: 4.5071 - acc: 0.045 - ETA: 1s - loss: 4.5065 - acc: 0.0455Epoch 00002: val\_loss improved from 4.72099 to 4.31665, saving model to saved\_models/weights.best.from\_scratch.hdf5
6680/6680 [==============================] - 373s 56ms/step - loss: 4.5058 - acc: 0.0455 - val\_loss: 4.3166 - val\_acc: 0.0635
Epoch 3/8
4080/6680 [=================>{\ldots}] - ETA: 5:41 - loss: 4.1815 - acc: 0.050 - ETA: 5:47 - loss: 4.1046 - acc: 0.025 - ETA: 5:47 - loss: 4.1626 - acc: 0.033 - ETA: 5:46 - loss: 4.1280 - acc: 0.062 - ETA: 5:52 - loss: 4.0939 - acc: 0.090 - ETA: 5:56 - loss: 4.1210 - acc: 0.075 - ETA: 5:54 - loss: 4.0551 - acc: 0.085 - ETA: 5:51 - loss: 4.0732 - acc: 0.087 - ETA: 5:50 - loss: 4.1086 - acc: 0.088 - ETA: 5:48 - loss: 4.1299 - acc: 0.085 - ETA: 5:46 - loss: 4.1271 - acc: 0.086 - ETA: 5:45 - loss: 4.1268 - acc: 0.091 - ETA: 5:43 - loss: 4.1382 - acc: 0.092 - ETA: 5:42 - loss: 4.1162 - acc: 0.096 - ETA: 5:41 - loss: 4.0933 - acc: 0.103 - ETA: 5:39 - loss: 4.1019 - acc: 0.103 - ETA: 5:38 - loss: 4.0787 - acc: 0.102 - ETA: 5:38 - loss: 4.0878 - acc: 0.100 - ETA: 5:37 - loss: 4.0828 - acc: 0.100 - ETA: 5:38 - loss: 4.1025 - acc: 0.095 - ETA: 5:38 - loss: 4.1141 - acc: 0.090 - ETA: 5:36 - loss: 4.1165 - acc: 0.088 - ETA: 5:35 - loss: 4.1145 - acc: 0.087 - ETA: 5:34 - loss: 4.1045 - acc: 0.085 - ETA: 5:33 - loss: 4.0891 - acc: 0.088 - ETA: 5:31 - loss: 4.0938 - acc: 0.086 - ETA: 5:30 - loss: 4.0851 - acc: 0.088 - ETA: 5:29 - loss: 4.0810 - acc: 0.091 - ETA: 5:28 - loss: 4.0974 - acc: 0.087 - ETA: 5:27 - loss: 4.1000 - acc: 0.088 - ETA: 5:25 - loss: 4.1038 - acc: 0.091 - ETA: 5:24 - loss: 4.1023 - acc: 0.090 - ETA: 5:23 - loss: 4.0970 - acc: 0.090 - ETA: 5:22 - loss: 4.1108 - acc: 0.091 - ETA: 5:22 - loss: 4.1185 - acc: 0.090 - ETA: 5:21 - loss: 4.1160 - acc: 0.091 - ETA: 5:20 - loss: 4.1248 - acc: 0.090 - ETA: 5:19 - loss: 4.1180 - acc: 0.089 - ETA: 5:17 - loss: 4.1221 - acc: 0.088 - ETA: 5:16 - loss: 4.1343 - acc: 0.086 - ETA: 5:15 - loss: 4.1417 - acc: 0.086 - ETA: 5:13 - loss: 4.1444 - acc: 0.086 - ETA: 5:12 - loss: 4.1322 - acc: 0.088 - ETA: 5:11 - loss: 4.1347 - acc: 0.087 - ETA: 5:10 - loss: 4.1321 - acc: 0.086 - ETA: 5:09 - loss: 4.1320 - acc: 0.085 - ETA: 5:08 - loss: 4.1340 - acc: 0.084 - ETA: 5:07 - loss: 4.1394 - acc: 0.083 - ETA: 5:06 - loss: 4.1418 - acc: 0.082 - ETA: 5:05 - loss: 4.1509 - acc: 0.086 - ETA: 5:04 - loss: 4.1502 - acc: 0.085 - ETA: 5:03 - loss: 4.1464 - acc: 0.084 - ETA: 5:02 - loss: 4.1461 - acc: 0.084 - ETA: 5:01 - loss: 4.1396 - acc: 0.085 - ETA: 5:00 - loss: 4.1377 - acc: 0.084 - ETA: 4:58 - loss: 4.1349 - acc: 0.085 - ETA: 4:57 - loss: 4.1308 - acc: 0.086 - ETA: 4:57 - loss: 4.1373 - acc: 0.087 - ETA: 4:56 - loss: 4.1298 - acc: 0.089 - ETA: 4:55 - loss: 4.1252 - acc: 0.088 - ETA: 4:53 - loss: 4.1240 - acc: 0.087 - ETA: 4:52 - loss: 4.1288 - acc: 0.087 - ETA: 4:51 - loss: 4.1309 - acc: 0.086 - ETA: 4:50 - loss: 4.1318 - acc: 0.085 - ETA: 4:50 - loss: 4.1256 - acc: 0.086 - ETA: 4:49 - loss: 4.1283 - acc: 0.086 - ETA: 4:48 - loss: 4.1299 - acc: 0.087 - ETA: 4:47 - loss: 4.1257 - acc: 0.088 - ETA: 4:46 - loss: 4.1150 - acc: 0.089 - ETA: 4:44 - loss: 4.1115 - acc: 0.090 - ETA: 4:43 - loss: 4.1139 - acc: 0.089 - ETA: 4:42 - loss: 4.1091 - acc: 0.091 - ETA: 4:41 - loss: 4.1132 - acc: 0.089 - ETA: 4:40 - loss: 4.1169 - acc: 0.089 - ETA: 4:39 - loss: 4.1155 - acc: 0.089 - ETA: 4:38 - loss: 4.1107 - acc: 0.090 - ETA: 4:36 - loss: 4.1052 - acc: 0.089 - ETA: 4:35 - loss: 4.1070 - acc: 0.088 - ETA: 4:34 - loss: 4.1115 - acc: 0.089 - ETA: 4:34 - loss: 4.1123 - acc: 0.088 - ETA: 4:33 - loss: 4.1099 - acc: 0.087 - ETA: 4:32 - loss: 4.1087 - acc: 0.087 - ETA: 4:31 - loss: 4.1032 - acc: 0.089 - ETA: 4:29 - loss: 4.1000 - acc: 0.089 - ETA: 4:28 - loss: 4.0989 - acc: 0.089 - ETA: 4:27 - loss: 4.0988 - acc: 0.088 - ETA: 4:26 - loss: 4.0962 - acc: 0.087 - ETA: 4:25 - loss: 4.0994 - acc: 0.087 - ETA: 4:24 - loss: 4.0991 - acc: 0.087 - ETA: 4:23 - loss: 4.0987 - acc: 0.087 - ETA: 4:22 - loss: 4.0998 - acc: 0.086 - ETA: 4:20 - loss: 4.1014 - acc: 0.085 - ETA: 4:19 - loss: 4.1004 - acc: 0.085 - ETA: 4:19 - loss: 4.1005 - acc: 0.086 - ETA: 4:18 - loss: 4.0996 - acc: 0.086 - ETA: 4:17 - loss: 4.0979 - acc: 0.087 - ETA: 4:15 - loss: 4.0984 - acc: 0.086 - ETA: 4:14 - loss: 4.0970 - acc: 0.087 - ETA: 4:13 - loss: 4.1021 - acc: 0.087 - ETA: 4:12 - loss: 4.0972 - acc: 0.088 - ETA: 4:11 - loss: 4.0964 - acc: 0.087 - ETA: 4:10 - loss: 4.0927 - acc: 0.089 - ETA: 4:09 - loss: 4.0938 - acc: 0.089 - ETA: 4:08 - loss: 4.0942 - acc: 0.089 - ETA: 4:06 - loss: 4.0934 - acc: 0.090 - ETA: 4:05 - loss: 4.0886 - acc: 0.091 - ETA: 4:04 - loss: 4.0823 - acc: 0.093 - ETA: 4:03 - loss: 4.0776 - acc: 0.093 - ETA: 4:02 - loss: 4.0867 - acc: 0.092 - ETA: 4:01 - loss: 4.0898 - acc: 0.092 - ETA: 4:00 - loss: 4.0903 - acc: 0.092 - ETA: 3:59 - loss: 4.0874 - acc: 0.093 - ETA: 3:58 - loss: 4.0875 - acc: 0.092 - ETA: 3:57 - loss: 4.0890 - acc: 0.092 - ETA: 3:56 - loss: 4.0881 - acc: 0.092 - ETA: 3:55 - loss: 4.0911 - acc: 0.092 - ETA: 3:54 - loss: 4.0927 - acc: 0.091 - ETA: 3:53 - loss: 4.0916 - acc: 0.091 - ETA: 3:51 - loss: 4.0903 - acc: 0.091 - ETA: 3:50 - loss: 4.0892 - acc: 0.091 - ETA: 3:49 - loss: 4.0884 - acc: 0.092 - ETA: 3:48 - loss: 4.0900 - acc: 0.091 - ETA: 3:47 - loss: 4.0899 - acc: 0.091 - ETA: 3:46 - loss: 4.0889 - acc: 0.091 - ETA: 3:45 - loss: 4.0889 - acc: 0.091 - ETA: 3:44 - loss: 4.0910 - acc: 0.091 - ETA: 3:43 - loss: 4.0884 - acc: 0.092 - ETA: 3:42 - loss: 4.0855 - acc: 0.092 - ETA: 3:41 - loss: 4.0861 - acc: 0.092 - ETA: 3:39 - loss: 4.0848 - acc: 0.093 - ETA: 3:38 - loss: 4.0853 - acc: 0.092 - ETA: 3:37 - loss: 4.0887 - acc: 0.092 - ETA: 3:36 - loss: 4.0868 - acc: 0.092 - ETA: 3:35 - loss: 4.0877 - acc: 0.092 - ETA: 3:34 - loss: 4.0872 - acc: 0.092 - ETA: 3:33 - loss: 4.0874 - acc: 0.092 - ETA: 3:32 - loss: 4.0887 - acc: 0.092 - ETA: 3:31 - loss: 4.0878 - acc: 0.091 - ETA: 3:30 - loss: 4.0846 - acc: 0.092 - ETA: 3:29 - loss: 4.0839 - acc: 0.092 - ETA: 3:28 - loss: 4.0823 - acc: 0.092 - ETA: 3:26 - loss: 4.0823 - acc: 0.093 - ETA: 3:25 - loss: 4.0826 - acc: 0.093 - ETA: 3:24 - loss: 4.0791 - acc: 0.094 - ETA: 3:23 - loss: 4.0790 - acc: 0.093 - ETA: 3:22 - loss: 4.0840 - acc: 0.093 - ETA: 3:21 - loss: 4.0830 - acc: 0.094 - ETA: 3:20 - loss: 4.0804 - acc: 0.094 - ETA: 3:19 - loss: 4.0797 - acc: 0.094 - ETA: 3:18 - loss: 4.0804 - acc: 0.094 - ETA: 3:17 - loss: 4.0819 - acc: 0.095 - ETA: 3:15 - loss: 4.0802 - acc: 0.095 - ETA: 3:14 - loss: 4.0796 - acc: 0.096 - ETA: 3:14 - loss: 4.0837 - acc: 0.095 - ETA: 3:12 - loss: 4.0829 - acc: 0.095 - ETA: 3:11 - loss: 4.0821 - acc: 0.094 - ETA: 3:10 - loss: 4.0835 - acc: 0.094 - ETA: 3:09 - loss: 4.0849 - acc: 0.094 - ETA: 3:08 - loss: 4.0822 - acc: 0.094 - ETA: 3:07 - loss: 4.0817 - acc: 0.094 - ETA: 3:06 - loss: 4.0841 - acc: 0.095 - ETA: 3:05 - loss: 4.0839 - acc: 0.095 - ETA: 3:04 - loss: 4.0842 - acc: 0.095 - ETA: 3:03 - loss: 4.0841 - acc: 0.096 - ETA: 3:01 - loss: 4.0849 - acc: 0.095 - ETA: 3:00 - loss: 4.0854 - acc: 0.095 - ETA: 2:59 - loss: 4.0845 - acc: 0.095 - ETA: 2:58 - loss: 4.0864 - acc: 0.094 - ETA: 2:57 - loss: 4.0880 - acc: 0.094 - ETA: 2:56 - loss: 4.0853 - acc: 0.095 - ETA: 2:55 - loss: 4.0841 - acc: 0.096 - ETA: 2:54 - loss: 4.0855 - acc: 0.095 - ETA: 2:53 - loss: 4.0857 - acc: 0.095 - ETA: 2:52 - loss: 4.0871 - acc: 0.095 - ETA: 2:51 - loss: 4.0878 - acc: 0.095 - ETA: 2:50 - loss: 4.0881 - acc: 0.094 - ETA: 2:49 - loss: 4.0878 - acc: 0.094 - ETA: 2:48 - loss: 4.0854 - acc: 0.095 - ETA: 2:47 - loss: 4.0879 - acc: 0.095 - ETA: 2:45 - loss: 4.0855 - acc: 0.095 - ETA: 2:44 - loss: 4.0851 - acc: 0.095 - ETA: 2:43 - loss: 4.0867 - acc: 0.095 - ETA: 2:42 - loss: 4.0852 - acc: 0.095 - ETA: 2:41 - loss: 4.0866 - acc: 0.095 - ETA: 2:40 - loss: 4.0857 - acc: 0.095 - ETA: 2:39 - loss: 4.0890 - acc: 0.094 - ETA: 2:38 - loss: 4.0881 - acc: 0.094 - ETA: 2:37 - loss: 4.0883 - acc: 0.094 - ETA: 2:36 - loss: 4.0858 - acc: 0.095 - ETA: 2:35 - loss: 4.0864 - acc: 0.095 - ETA: 2:34 - loss: 4.0865 - acc: 0.095 - ETA: 2:33 - loss: 4.0876 - acc: 0.095 - ETA: 2:31 - loss: 4.0879 - acc: 0.094 - ETA: 2:30 - loss: 4.0859 - acc: 0.094 - ETA: 2:29 - loss: 4.0860 - acc: 0.094 - ETA: 2:28 - loss: 4.0873 - acc: 0.094 - ETA: 2:27 - loss: 4.0871 - acc: 0.095 - ETA: 2:26 - loss: 4.0873 - acc: 0.095 - ETA: 2:25 - loss: 4.0871 - acc: 0.094 - ETA: 2:24 - loss: 4.0862 - acc: 0.094 - ETA: 2:23 - loss: 4.0855 - acc: 0.094 - ETA: 2:22 - loss: 4.0849 - acc: 0.094 - ETA: 2:21 - loss: 4.0819 - acc: 0.094 - ETA: 2:20 - loss: 4.0847 - acc: 0.09446660/6680 [============================>.] - ETA: 2:19 - loss: 4.0841 - acc: 0.094 - ETA: 2:17 - loss: 4.0851 - acc: 0.094 - ETA: 2:16 - loss: 4.0829 - acc: 0.095 - ETA: 2:15 - loss: 4.0824 - acc: 0.095 - ETA: 2:14 - loss: 4.0834 - acc: 0.095 - ETA: 2:13 - loss: 4.0828 - acc: 0.095 - ETA: 2:12 - loss: 4.0813 - acc: 0.095 - ETA: 2:11 - loss: 4.0810 - acc: 0.095 - ETA: 2:10 - loss: 4.0810 - acc: 0.095 - ETA: 2:09 - loss: 4.0813 - acc: 0.095 - ETA: 2:08 - loss: 4.0808 - acc: 0.095 - ETA: 2:07 - loss: 4.0820 - acc: 0.095 - ETA: 2:06 - loss: 4.0823 - acc: 0.095 - ETA: 2:04 - loss: 4.0808 - acc: 0.095 - ETA: 2:03 - loss: 4.0807 - acc: 0.095 - ETA: 2:02 - loss: 4.0808 - acc: 0.094 - ETA: 2:01 - loss: 4.0796 - acc: 0.095 - ETA: 2:00 - loss: 4.0792 - acc: 0.095 - ETA: 1:59 - loss: 4.0799 - acc: 0.095 - ETA: 1:58 - loss: 4.0792 - acc: 0.095 - ETA: 1:57 - loss: 4.0794 - acc: 0.095 - ETA: 1:56 - loss: 4.0799 - acc: 0.095 - ETA: 1:55 - loss: 4.0786 - acc: 0.095 - ETA: 1:54 - loss: 4.0796 - acc: 0.095 - ETA: 1:53 - loss: 4.0785 - acc: 0.095 - ETA: 1:52 - loss: 4.0748 - acc: 0.096 - ETA: 1:50 - loss: 4.0746 - acc: 0.096 - ETA: 1:49 - loss: 4.0743 - acc: 0.095 - ETA: 1:48 - loss: 4.0742 - acc: 0.095 - ETA: 1:47 - loss: 4.0751 - acc: 0.095 - ETA: 1:46 - loss: 4.0752 - acc: 0.095 - ETA: 1:45 - loss: 4.0747 - acc: 0.095 - ETA: 1:44 - loss: 4.0749 - acc: 0.095 - ETA: 1:43 - loss: 4.0745 - acc: 0.095 - ETA: 1:42 - loss: 4.0747 - acc: 0.095 - ETA: 1:41 - loss: 4.0736 - acc: 0.095 - ETA: 1:40 - loss: 4.0751 - acc: 0.095 - ETA: 1:39 - loss: 4.0748 - acc: 0.094 - ETA: 1:37 - loss: 4.0753 - acc: 0.094 - ETA: 1:36 - loss: 4.0771 - acc: 0.094 - ETA: 1:35 - loss: 4.0769 - acc: 0.094 - ETA: 1:34 - loss: 4.0773 - acc: 0.094 - ETA: 1:33 - loss: 4.0768 - acc: 0.094 - ETA: 1:32 - loss: 4.0754 - acc: 0.094 - ETA: 1:31 - loss: 4.0751 - acc: 0.095 - ETA: 1:30 - loss: 4.0744 - acc: 0.095 - ETA: 1:29 - loss: 4.0748 - acc: 0.095 - ETA: 1:28 - loss: 4.0753 - acc: 0.095 - ETA: 1:27 - loss: 4.0748 - acc: 0.095 - ETA: 1:26 - loss: 4.0742 - acc: 0.095 - ETA: 1:24 - loss: 4.0736 - acc: 0.095 - ETA: 1:23 - loss: 4.0759 - acc: 0.094 - ETA: 1:22 - loss: 4.0752 - acc: 0.094 - ETA: 1:21 - loss: 4.0751 - acc: 0.094 - ETA: 1:20 - loss: 4.0743 - acc: 0.095 - ETA: 1:19 - loss: 4.0732 - acc: 0.095 - ETA: 1:18 - loss: 4.0736 - acc: 0.095 - ETA: 1:17 - loss: 4.0736 - acc: 0.095 - ETA: 1:16 - loss: 4.0747 - acc: 0.094 - ETA: 1:15 - loss: 4.0759 - acc: 0.094 - ETA: 1:14 - loss: 4.0764 - acc: 0.094 - ETA: 1:13 - loss: 4.0769 - acc: 0.094 - ETA: 1:12 - loss: 4.0761 - acc: 0.094 - ETA: 1:10 - loss: 4.0770 - acc: 0.094 - ETA: 1:09 - loss: 4.0760 - acc: 0.093 - ETA: 1:08 - loss: 4.0773 - acc: 0.093 - ETA: 1:07 - loss: 4.0768 - acc: 0.093 - ETA: 1:06 - loss: 4.0767 - acc: 0.093 - ETA: 1:05 - loss: 4.0761 - acc: 0.093 - ETA: 1:04 - loss: 4.0756 - acc: 0.093 - ETA: 1:03 - loss: 4.0755 - acc: 0.093 - ETA: 1:02 - loss: 4.0749 - acc: 0.094 - ETA: 1:01 - loss: 4.0748 - acc: 0.093 - ETA: 1:00 - loss: 4.0726 - acc: 0.094 - ETA: 59s - loss: 4.0729 - acc: 0.094 - ETA: 58s - loss: 4.0722 - acc: 0.09 - ETA: 57s - loss: 4.0728 - acc: 0.09 - ETA: 55s - loss: 4.0721 - acc: 0.09 - ETA: 54s - loss: 4.0733 - acc: 0.09 - ETA: 53s - loss: 4.0738 - acc: 0.09 - ETA: 52s - loss: 4.0736 - acc: 0.09 - ETA: 51s - loss: 4.0742 - acc: 0.09 - ETA: 50s - loss: 4.0748 - acc: 0.09 - ETA: 49s - loss: 4.0748 - acc: 0.09 - ETA: 48s - loss: 4.0755 - acc: 0.09 - ETA: 47s - loss: 4.0767 - acc: 0.09 - ETA: 46s - loss: 4.0766 - acc: 0.09 - ETA: 45s - loss: 4.0759 - acc: 0.09 - ETA: 44s - loss: 4.0742 - acc: 0.09 - ETA: 43s - loss: 4.0736 - acc: 0.09 - ETA: 41s - loss: 4.0734 - acc: 0.09 - ETA: 40s - loss: 4.0731 - acc: 0.09 - ETA: 39s - loss: 4.0720 - acc: 0.09 - ETA: 38s - loss: 4.0739 - acc: 0.09 - ETA: 37s - loss: 4.0729 - acc: 0.09 - ETA: 36s - loss: 4.0747 - acc: 0.09 - ETA: 35s - loss: 4.0756 - acc: 0.09 - ETA: 34s - loss: 4.0755 - acc: 0.09 - ETA: 33s - loss: 4.0748 - acc: 0.09 - ETA: 32s - loss: 4.0736 - acc: 0.09 - ETA: 31s - loss: 4.0732 - acc: 0.09 - ETA: 30s - loss: 4.0728 - acc: 0.09 - ETA: 29s - loss: 4.0727 - acc: 0.09 - ETA: 27s - loss: 4.0724 - acc: 0.09 - ETA: 26s - loss: 4.0717 - acc: 0.09 - ETA: 25s - loss: 4.0720 - acc: 0.09 - ETA: 24s - loss: 4.0712 - acc: 0.09 - ETA: 23s - loss: 4.0709 - acc: 0.09 - ETA: 22s - loss: 4.0704 - acc: 0.09 - ETA: 21s - loss: 4.0725 - acc: 0.09 - ETA: 20s - loss: 4.0727 - acc: 0.09 - ETA: 19s - loss: 4.0722 - acc: 0.09 - ETA: 18s - loss: 4.0714 - acc: 0.09 - ETA: 17s - loss: 4.0714 - acc: 0.09 - ETA: 16s - loss: 4.0704 - acc: 0.09 - ETA: 15s - loss: 4.0694 - acc: 0.09 - ETA: 13s - loss: 4.0693 - acc: 0.09 - ETA: 12s - loss: 4.0719 - acc: 0.09 - ETA: 11s - loss: 4.0704 - acc: 0.09 - ETA: 10s - loss: 4.0706 - acc: 0.09 - ETA: 9s - loss: 4.0704 - acc: 0.0949 - ETA: 8s - loss: 4.0692 - acc: 0.094 - ETA: 7s - loss: 4.0713 - acc: 0.094 - ETA: 6s - loss: 4.0705 - acc: 0.094 - ETA: 5s - loss: 4.0697 - acc: 0.093 - ETA: 4s - loss: 4.0696 - acc: 0.094 - ETA: 3s - loss: 4.0689 - acc: 0.094 - ETA: 2s - loss: 4.0690 - acc: 0.094 - ETA: 1s - loss: 4.0694 - acc: 0.0940Epoch 00003: val\_loss improved from 4.31665 to 4.28037, saving model to saved\_models/weights.best.from\_scratch.hdf5
6680/6680 [==============================] - 371s 56ms/step - loss: 4.0677 - acc: 0.0943 - val\_loss: 4.2804 - val\_acc: 0.0731
Epoch 4/8
4080/6680 [=================>{\ldots}] - ETA: 5:41 - loss: 3.5619 - acc: 0.250 - ETA: 5:57 - loss: 3.4974 - acc: 0.225 - ETA: 6:08 - loss: 3.5628 - acc: 0.233 - ETA: 6:03 - loss: 3.4740 - acc: 0.275 - ETA: 5:59 - loss: 3.4889 - acc: 0.260 - ETA: 5:57 - loss: 3.5261 - acc: 0.225 - ETA: 5:55 - loss: 3.6040 - acc: 0.214 - ETA: 5:52 - loss: 3.5728 - acc: 0.200 - ETA: 5:52 - loss: 3.5829 - acc: 0.183 - ETA: 5:50 - loss: 3.5570 - acc: 0.180 - ETA: 5:48 - loss: 3.5443 - acc: 0.186 - ETA: 5:47 - loss: 3.5542 - acc: 0.179 - ETA: 5:45 - loss: 3.5463 - acc: 0.176 - ETA: 5:44 - loss: 3.5264 - acc: 0.182 - ETA: 5:42 - loss: 3.4898 - acc: 0.190 - ETA: 5:41 - loss: 3.5024 - acc: 0.190 - ETA: 5:41 - loss: 3.4866 - acc: 0.197 - ETA: 5:42 - loss: 3.4936 - acc: 0.194 - ETA: 5:40 - loss: 3.4699 - acc: 0.197 - ETA: 5:39 - loss: 3.5018 - acc: 0.195 - ETA: 5:38 - loss: 3.4962 - acc: 0.197 - ETA: 5:36 - loss: 3.5093 - acc: 0.195 - ETA: 5:35 - loss: 3.5011 - acc: 0.193 - ETA: 5:34 - loss: 3.4842 - acc: 0.200 - ETA: 5:32 - loss: 3.4995 - acc: 0.196 - ETA: 5:31 - loss: 3.5222 - acc: 0.188 - ETA: 5:30 - loss: 3.5182 - acc: 0.188 - ETA: 5:29 - loss: 3.5101 - acc: 0.189 - ETA: 5:27 - loss: 3.5034 - acc: 0.186 - ETA: 5:26 - loss: 3.5071 - acc: 0.185 - ETA: 5:25 - loss: 3.5034 - acc: 0.182 - ETA: 5:25 - loss: 3.5053 - acc: 0.185 - ETA: 5:24 - loss: 3.5121 - acc: 0.184 - ETA: 5:23 - loss: 3.5233 - acc: 0.182 - ETA: 5:22 - loss: 3.5276 - acc: 0.182 - ETA: 5:21 - loss: 3.5245 - acc: 0.183 - ETA: 5:20 - loss: 3.5137 - acc: 0.183 - ETA: 5:18 - loss: 3.5159 - acc: 0.182 - ETA: 5:17 - loss: 3.5274 - acc: 0.180 - ETA: 5:16 - loss: 3.5232 - acc: 0.183 - ETA: 5:15 - loss: 3.5214 - acc: 0.182 - ETA: 5:14 - loss: 3.5172 - acc: 0.184 - ETA: 5:13 - loss: 3.5213 - acc: 0.183 - ETA: 5:12 - loss: 3.5110 - acc: 0.180 - ETA: 5:11 - loss: 3.5007 - acc: 0.183 - ETA: 5:10 - loss: 3.5145 - acc: 0.180 - ETA: 5:10 - loss: 3.5116 - acc: 0.181 - ETA: 5:10 - loss: 3.5210 - acc: 0.179 - ETA: 5:09 - loss: 3.5044 - acc: 0.181 - ETA: 5:08 - loss: 3.5072 - acc: 0.181 - ETA: 5:06 - loss: 3.5078 - acc: 0.181 - ETA: 5:05 - loss: 3.5154 - acc: 0.178 - ETA: 5:04 - loss: 3.5198 - acc: 0.177 - ETA: 5:03 - loss: 3.5185 - acc: 0.178 - ETA: 5:02 - loss: 3.5203 - acc: 0.177 - ETA: 5:01 - loss: 3.5131 - acc: 0.178 - ETA: 4:59 - loss: 3.5110 - acc: 0.178 - ETA: 4:58 - loss: 3.5083 - acc: 0.176 - ETA: 4:57 - loss: 3.5144 - acc: 0.176 - ETA: 4:56 - loss: 3.5136 - acc: 0.178 - ETA: 4:55 - loss: 3.5092 - acc: 0.177 - ETA: 4:54 - loss: 3.5030 - acc: 0.179 - ETA: 4:53 - loss: 3.5032 - acc: 0.181 - ETA: 4:52 - loss: 3.4964 - acc: 0.183 - ETA: 4:51 - loss: 3.5046 - acc: 0.184 - ETA: 4:50 - loss: 3.4989 - acc: 0.183 - ETA: 4:49 - loss: 3.4960 - acc: 0.182 - ETA: 4:47 - loss: 3.4926 - acc: 0.183 - ETA: 4:46 - loss: 3.4999 - acc: 0.183 - ETA: 4:45 - loss: 3.4945 - acc: 0.184 - ETA: 4:44 - loss: 3.4892 - acc: 0.185 - ETA: 4:43 - loss: 3.4840 - acc: 0.184 - ETA: 4:42 - loss: 3.4848 - acc: 0.184 - ETA: 4:41 - loss: 3.4883 - acc: 0.184 - ETA: 4:40 - loss: 3.4907 - acc: 0.185 - ETA: 4:39 - loss: 3.4902 - acc: 0.186 - ETA: 4:38 - loss: 3.4881 - acc: 0.186 - ETA: 4:37 - loss: 3.4855 - acc: 0.186 - ETA: 4:36 - loss: 3.4909 - acc: 0.186 - ETA: 4:34 - loss: 3.4880 - acc: 0.187 - ETA: 4:33 - loss: 3.4928 - acc: 0.186 - ETA: 4:32 - loss: 3.4887 - acc: 0.186 - ETA: 4:31 - loss: 3.4806 - acc: 0.188 - ETA: 4:30 - loss: 3.4819 - acc: 0.188 - ETA: 4:28 - loss: 3.4735 - acc: 0.188 - ETA: 4:27 - loss: 3.4708 - acc: 0.189 - ETA: 4:26 - loss: 3.4707 - acc: 0.190 - ETA: 4:25 - loss: 3.4680 - acc: 0.191 - ETA: 4:23 - loss: 3.4721 - acc: 0.191 - ETA: 4:22 - loss: 3.4685 - acc: 0.193 - ETA: 4:21 - loss: 3.4678 - acc: 0.194 - ETA: 4:20 - loss: 3.4661 - acc: 0.195 - ETA: 4:19 - loss: 3.4665 - acc: 0.195 - ETA: 4:18 - loss: 3.4752 - acc: 0.195 - ETA: 4:17 - loss: 3.4713 - acc: 0.196 - ETA: 4:16 - loss: 3.4656 - acc: 0.196 - ETA: 4:14 - loss: 3.4583 - acc: 0.198 - ETA: 4:13 - loss: 3.4614 - acc: 0.196 - ETA: 4:12 - loss: 3.4587 - acc: 0.197 - ETA: 4:11 - loss: 3.4579 - acc: 0.197 - ETA: 4:10 - loss: 3.4571 - acc: 0.197 - ETA: 4:09 - loss: 3.4561 - acc: 0.196 - ETA: 4:07 - loss: 3.4599 - acc: 0.195 - ETA: 4:06 - loss: 3.4611 - acc: 0.195 - ETA: 4:05 - loss: 3.4604 - acc: 0.196 - ETA: 4:04 - loss: 3.4559 - acc: 0.198 - ETA: 4:03 - loss: 3.4584 - acc: 0.197 - ETA: 4:02 - loss: 3.4560 - acc: 0.197 - ETA: 4:01 - loss: 3.4581 - acc: 0.197 - ETA: 4:00 - loss: 3.4590 - acc: 0.197 - ETA: 3:58 - loss: 3.4562 - acc: 0.198 - ETA: 3:57 - loss: 3.4593 - acc: 0.197 - ETA: 3:56 - loss: 3.4592 - acc: 0.198 - ETA: 3:55 - loss: 3.4561 - acc: 0.197 - ETA: 3:54 - loss: 3.4566 - acc: 0.197 - ETA: 3:53 - loss: 3.4607 - acc: 0.196 - ETA: 3:52 - loss: 3.4549 - acc: 0.197 - ETA: 3:51 - loss: 3.4587 - acc: 0.197 - ETA: 3:49 - loss: 3.4633 - acc: 0.197 - ETA: 3:48 - loss: 3.4630 - acc: 0.196 - ETA: 3:47 - loss: 3.4668 - acc: 0.195 - ETA: 3:46 - loss: 3.4639 - acc: 0.195 - ETA: 3:45 - loss: 3.4627 - acc: 0.195 - ETA: 3:44 - loss: 3.4608 - acc: 0.195 - ETA: 3:43 - loss: 3.4619 - acc: 0.195 - ETA: 3:42 - loss: 3.4638 - acc: 0.195 - ETA: 3:41 - loss: 3.4590 - acc: 0.196 - ETA: 3:40 - loss: 3.4617 - acc: 0.195 - ETA: 3:38 - loss: 3.4639 - acc: 0.195 - ETA: 3:37 - loss: 3.4636 - acc: 0.195 - ETA: 3:36 - loss: 3.4651 - acc: 0.195 - ETA: 3:35 - loss: 3.4693 - acc: 0.195 - ETA: 3:34 - loss: 3.4687 - acc: 0.195 - ETA: 3:33 - loss: 3.4688 - acc: 0.196 - ETA: 3:32 - loss: 3.4691 - acc: 0.195 - ETA: 3:31 - loss: 3.4674 - acc: 0.197 - ETA: 3:30 - loss: 3.4650 - acc: 0.196 - ETA: 3:29 - loss: 3.4663 - acc: 0.196 - ETA: 3:28 - loss: 3.4665 - acc: 0.197 - ETA: 3:26 - loss: 3.4652 - acc: 0.196 - ETA: 3:25 - loss: 3.4640 - acc: 0.197 - ETA: 3:24 - loss: 3.4638 - acc: 0.196 - ETA: 3:23 - loss: 3.4635 - acc: 0.195 - ETA: 3:22 - loss: 3.4631 - acc: 0.195 - ETA: 3:21 - loss: 3.4614 - acc: 0.196 - ETA: 3:20 - loss: 3.4622 - acc: 0.195 - ETA: 3:19 - loss: 3.4617 - acc: 0.195 - ETA: 3:18 - loss: 3.4614 - acc: 0.195 - ETA: 3:17 - loss: 3.4646 - acc: 0.195 - ETA: 3:15 - loss: 3.4638 - acc: 0.195 - ETA: 3:14 - loss: 3.4630 - acc: 0.195 - ETA: 3:13 - loss: 3.4610 - acc: 0.195 - ETA: 3:12 - loss: 3.4616 - acc: 0.195 - ETA: 3:11 - loss: 3.4614 - acc: 0.195 - ETA: 3:10 - loss: 3.4596 - acc: 0.196 - ETA: 3:09 - loss: 3.4570 - acc: 0.197 - ETA: 3:08 - loss: 3.4577 - acc: 0.197 - ETA: 3:07 - loss: 3.4586 - acc: 0.197 - ETA: 3:06 - loss: 3.4586 - acc: 0.197 - ETA: 3:05 - loss: 3.4592 - acc: 0.197 - ETA: 3:04 - loss: 3.4585 - acc: 0.196 - ETA: 3:03 - loss: 3.4559 - acc: 0.197 - ETA: 3:02 - loss: 3.4587 - acc: 0.196 - ETA: 3:00 - loss: 3.4559 - acc: 0.198 - ETA: 2:59 - loss: 3.4595 - acc: 0.197 - ETA: 2:58 - loss: 3.4625 - acc: 0.196 - ETA: 2:57 - loss: 3.4625 - acc: 0.197 - ETA: 2:56 - loss: 3.4641 - acc: 0.197 - ETA: 2:55 - loss: 3.4606 - acc: 0.197 - ETA: 2:54 - loss: 3.4582 - acc: 0.198 - ETA: 2:53 - loss: 3.4580 - acc: 0.198 - ETA: 2:52 - loss: 3.4562 - acc: 0.199 - ETA: 2:51 - loss: 3.4556 - acc: 0.199 - ETA: 2:50 - loss: 3.4577 - acc: 0.199 - ETA: 2:49 - loss: 3.4565 - acc: 0.199 - ETA: 2:48 - loss: 3.4558 - acc: 0.200 - ETA: 2:47 - loss: 3.4580 - acc: 0.199 - ETA: 2:45 - loss: 3.4604 - acc: 0.199 - ETA: 2:44 - loss: 3.4613 - acc: 0.198 - ETA: 2:43 - loss: 3.4604 - acc: 0.199 - ETA: 2:42 - loss: 3.4608 - acc: 0.199 - ETA: 2:41 - loss: 3.4598 - acc: 0.200 - ETA: 2:40 - loss: 3.4595 - acc: 0.200 - ETA: 2:39 - loss: 3.4605 - acc: 0.199 - ETA: 2:38 - loss: 3.4586 - acc: 0.200 - ETA: 2:37 - loss: 3.4581 - acc: 0.200 - ETA: 2:36 - loss: 3.4593 - acc: 0.200 - ETA: 2:35 - loss: 3.4605 - acc: 0.200 - ETA: 2:34 - loss: 3.4603 - acc: 0.200 - ETA: 2:33 - loss: 3.4598 - acc: 0.199 - ETA: 2:32 - loss: 3.4633 - acc: 0.198 - ETA: 2:30 - loss: 3.4658 - acc: 0.198 - ETA: 2:29 - loss: 3.4671 - acc: 0.198 - ETA: 2:28 - loss: 3.4665 - acc: 0.198 - ETA: 2:27 - loss: 3.4672 - acc: 0.198 - ETA: 2:26 - loss: 3.4644 - acc: 0.199 - ETA: 2:25 - loss: 3.4690 - acc: 0.198 - ETA: 2:24 - loss: 3.4691 - acc: 0.198 - ETA: 2:23 - loss: 3.4676 - acc: 0.199 - ETA: 2:22 - loss: 3.4685 - acc: 0.199 - ETA: 2:21 - loss: 3.4683 - acc: 0.199 - ETA: 2:20 - loss: 3.4694 - acc: 0.198 - ETA: 2:19 - loss: 3.4728 - acc: 0.198 - ETA: 2:18 - loss: 3.4712 - acc: 0.19856660/6680 [============================>.] - ETA: 2:17 - loss: 3.4747 - acc: 0.197 - ETA: 2:15 - loss: 3.4740 - acc: 0.197 - ETA: 2:14 - loss: 3.4744 - acc: 0.197 - ETA: 2:13 - loss: 3.4772 - acc: 0.196 - ETA: 2:12 - loss: 3.4771 - acc: 0.196 - ETA: 2:11 - loss: 3.4757 - acc: 0.196 - ETA: 2:10 - loss: 3.4717 - acc: 0.196 - ETA: 2:09 - loss: 3.4715 - acc: 0.196 - ETA: 2:08 - loss: 3.4709 - acc: 0.196 - ETA: 2:07 - loss: 3.4699 - acc: 0.196 - ETA: 2:06 - loss: 3.4683 - acc: 0.196 - ETA: 2:05 - loss: 3.4652 - acc: 0.197 - ETA: 2:04 - loss: 3.4630 - acc: 0.197 - ETA: 2:03 - loss: 3.4647 - acc: 0.197 - ETA: 2:02 - loss: 3.4637 - acc: 0.197 - ETA: 2:01 - loss: 3.4641 - acc: 0.196 - ETA: 1:59 - loss: 3.4625 - acc: 0.197 - ETA: 1:58 - loss: 3.4600 - acc: 0.197 - ETA: 1:57 - loss: 3.4590 - acc: 0.197 - ETA: 1:56 - loss: 3.4590 - acc: 0.197 - ETA: 1:55 - loss: 3.4575 - acc: 0.197 - ETA: 1:54 - loss: 3.4594 - acc: 0.197 - ETA: 1:53 - loss: 3.4576 - acc: 0.197 - ETA: 1:52 - loss: 3.4546 - acc: 0.197 - ETA: 1:51 - loss: 3.4564 - acc: 0.196 - ETA: 1:50 - loss: 3.4562 - acc: 0.196 - ETA: 1:49 - loss: 3.4563 - acc: 0.196 - ETA: 1:48 - loss: 3.4565 - acc: 0.196 - ETA: 1:47 - loss: 3.4569 - acc: 0.196 - ETA: 1:46 - loss: 3.4560 - acc: 0.196 - ETA: 1:44 - loss: 3.4570 - acc: 0.196 - ETA: 1:43 - loss: 3.4555 - acc: 0.196 - ETA: 1:42 - loss: 3.4551 - acc: 0.196 - ETA: 1:41 - loss: 3.4563 - acc: 0.195 - ETA: 1:40 - loss: 3.4538 - acc: 0.196 - ETA: 1:39 - loss: 3.4561 - acc: 0.196 - ETA: 1:38 - loss: 3.4584 - acc: 0.195 - ETA: 1:37 - loss: 3.4575 - acc: 0.196 - ETA: 1:36 - loss: 3.4576 - acc: 0.196 - ETA: 1:35 - loss: 3.4557 - acc: 0.196 - ETA: 1:34 - loss: 3.4575 - acc: 0.196 - ETA: 1:33 - loss: 3.4581 - acc: 0.196 - ETA: 1:32 - loss: 3.4565 - acc: 0.197 - ETA: 1:31 - loss: 3.4548 - acc: 0.197 - ETA: 1:30 - loss: 3.4575 - acc: 0.197 - ETA: 1:29 - loss: 3.4564 - acc: 0.197 - ETA: 1:27 - loss: 3.4569 - acc: 0.197 - ETA: 1:26 - loss: 3.4552 - acc: 0.198 - ETA: 1:25 - loss: 3.4529 - acc: 0.197 - ETA: 1:24 - loss: 3.4527 - acc: 0.197 - ETA: 1:23 - loss: 3.4516 - acc: 0.198 - ETA: 1:22 - loss: 3.4522 - acc: 0.198 - ETA: 1:21 - loss: 3.4523 - acc: 0.198 - ETA: 1:20 - loss: 3.4513 - acc: 0.198 - ETA: 1:19 - loss: 3.4512 - acc: 0.198 - ETA: 1:18 - loss: 3.4506 - acc: 0.198 - ETA: 1:17 - loss: 3.4507 - acc: 0.198 - ETA: 1:16 - loss: 3.4505 - acc: 0.197 - ETA: 1:15 - loss: 3.4511 - acc: 0.197 - ETA: 1:14 - loss: 3.4496 - acc: 0.197 - ETA: 1:13 - loss: 3.4496 - acc: 0.197 - ETA: 1:12 - loss: 3.4514 - acc: 0.197 - ETA: 1:10 - loss: 3.4514 - acc: 0.197 - ETA: 1:09 - loss: 3.4520 - acc: 0.197 - ETA: 1:08 - loss: 3.4533 - acc: 0.197 - ETA: 1:07 - loss: 3.4555 - acc: 0.197 - ETA: 1:06 - loss: 3.4565 - acc: 0.197 - ETA: 1:05 - loss: 3.4570 - acc: 0.196 - ETA: 1:04 - loss: 3.4555 - acc: 0.196 - ETA: 1:03 - loss: 3.4565 - acc: 0.196 - ETA: 1:02 - loss: 3.4569 - acc: 0.196 - ETA: 1:01 - loss: 3.4568 - acc: 0.196 - ETA: 1:00 - loss: 3.4573 - acc: 0.196 - ETA: 59s - loss: 3.4551 - acc: 0.195 - ETA: 58s - loss: 3.4551 - acc: 0.19 - ETA: 57s - loss: 3.4542 - acc: 0.19 - ETA: 56s - loss: 3.4547 - acc: 0.19 - ETA: 55s - loss: 3.4562 - acc: 0.19 - ETA: 54s - loss: 3.4546 - acc: 0.19 - ETA: 52s - loss: 3.4548 - acc: 0.19 - ETA: 51s - loss: 3.4557 - acc: 0.19 - ETA: 50s - loss: 3.4567 - acc: 0.19 - ETA: 49s - loss: 3.4573 - acc: 0.19 - ETA: 48s - loss: 3.4577 - acc: 0.19 - ETA: 47s - loss: 3.4580 - acc: 0.19 - ETA: 46s - loss: 3.4573 - acc: 0.19 - ETA: 45s - loss: 3.4556 - acc: 0.19 - ETA: 44s - loss: 3.4565 - acc: 0.19 - ETA: 43s - loss: 3.4559 - acc: 0.19 - ETA: 42s - loss: 3.4566 - acc: 0.19 - ETA: 41s - loss: 3.4555 - acc: 0.19 - ETA: 40s - loss: 3.4569 - acc: 0.19 - ETA: 39s - loss: 3.4573 - acc: 0.19 - ETA: 38s - loss: 3.4560 - acc: 0.19 - ETA: 37s - loss: 3.4550 - acc: 0.19 - ETA: 35s - loss: 3.4559 - acc: 0.19 - ETA: 34s - loss: 3.4565 - acc: 0.19 - ETA: 33s - loss: 3.4570 - acc: 0.19 - ETA: 32s - loss: 3.4564 - acc: 0.19 - ETA: 31s - loss: 3.4566 - acc: 0.19 - ETA: 30s - loss: 3.4581 - acc: 0.19 - ETA: 29s - loss: 3.4593 - acc: 0.19 - ETA: 28s - loss: 3.4591 - acc: 0.19 - ETA: 27s - loss: 3.4598 - acc: 0.19 - ETA: 26s - loss: 3.4616 - acc: 0.19 - ETA: 25s - loss: 3.4609 - acc: 0.19 - ETA: 24s - loss: 3.4616 - acc: 0.19 - ETA: 23s - loss: 3.4616 - acc: 0.19 - ETA: 22s - loss: 3.4614 - acc: 0.19 - ETA: 21s - loss: 3.4625 - acc: 0.19 - ETA: 20s - loss: 3.4612 - acc: 0.19 - ETA: 19s - loss: 3.4591 - acc: 0.19 - ETA: 17s - loss: 3.4571 - acc: 0.19 - ETA: 16s - loss: 3.4579 - acc: 0.19 - ETA: 15s - loss: 3.4581 - acc: 0.19 - ETA: 14s - loss: 3.4572 - acc: 0.19 - ETA: 13s - loss: 3.4579 - acc: 0.19 - ETA: 12s - loss: 3.4605 - acc: 0.19 - ETA: 11s - loss: 3.4620 - acc: 0.19 - ETA: 10s - loss: 3.4617 - acc: 0.19 - ETA: 9s - loss: 3.4606 - acc: 0.1972 - ETA: 8s - loss: 3.4583 - acc: 0.197 - ETA: 7s - loss: 3.4591 - acc: 0.197 - ETA: 6s - loss: 3.4590 - acc: 0.197 - ETA: 5s - loss: 3.4589 - acc: 0.197 - ETA: 4s - loss: 3.4580 - acc: 0.197 - ETA: 3s - loss: 3.4572 - acc: 0.197 - ETA: 2s - loss: 3.4566 - acc: 0.197 - ETA: 1s - loss: 3.4576 - acc: 0.1977Epoch 00004: val\_loss improved from 4.28037 to 4.13762, saving model to saved\_models/weights.best.from\_scratch.hdf5
6680/6680 [==============================] - 365s 55ms/step - loss: 3.4572 - acc: 0.1979 - val\_loss: 4.1376 - val\_acc: 0.0910
Epoch 5/8
4080/6680 [=================>{\ldots}] - ETA: 5:31 - loss: 2.9629 - acc: 0.250 - ETA: 5:35 - loss: 2.5860 - acc: 0.350 - ETA: 5:35 - loss: 2.6469 - acc: 0.333 - ETA: 5:35 - loss: 2.6172 - acc: 0.325 - ETA: 5:40 - loss: 2.5980 - acc: 0.330 - ETA: 5:46 - loss: 2.7206 - acc: 0.300 - ETA: 5:45 - loss: 2.7034 - acc: 0.321 - ETA: 5:42 - loss: 2.6094 - acc: 0.356 - ETA: 5:41 - loss: 2.5752 - acc: 0.361 - ETA: 5:40 - loss: 2.5963 - acc: 0.370 - ETA: 5:38 - loss: 2.6462 - acc: 0.359 - ETA: 5:38 - loss: 2.6427 - acc: 0.370 - ETA: 5:38 - loss: 2.6164 - acc: 0.380 - ETA: 5:37 - loss: 2.5908 - acc: 0.392 - ETA: 5:36 - loss: 2.5932 - acc: 0.390 - ETA: 5:34 - loss: 2.5895 - acc: 0.384 - ETA: 5:34 - loss: 2.5891 - acc: 0.388 - ETA: 5:33 - loss: 2.5650 - acc: 0.391 - ETA: 5:32 - loss: 2.5594 - acc: 0.386 - ETA: 5:31 - loss: 2.5602 - acc: 0.382 - ETA: 5:32 - loss: 2.5543 - acc: 0.383 - ETA: 5:31 - loss: 2.5710 - acc: 0.377 - ETA: 5:30 - loss: 2.5929 - acc: 0.371 - ETA: 5:29 - loss: 2.5941 - acc: 0.372 - ETA: 5:28 - loss: 2.6112 - acc: 0.372 - ETA: 5:27 - loss: 2.6061 - acc: 0.371 - ETA: 5:26 - loss: 2.5920 - acc: 0.374 - ETA: 5:25 - loss: 2.5875 - acc: 0.378 - ETA: 5:24 - loss: 2.5852 - acc: 0.379 - ETA: 5:23 - loss: 2.6113 - acc: 0.373 - ETA: 5:22 - loss: 2.5948 - acc: 0.377 - ETA: 5:21 - loss: 2.5865 - acc: 0.376 - ETA: 5:20 - loss: 2.5989 - acc: 0.375 - ETA: 5:19 - loss: 2.5930 - acc: 0.379 - ETA: 5:19 - loss: 2.5829 - acc: 0.382 - ETA: 5:18 - loss: 2.5794 - acc: 0.380 - ETA: 5:17 - loss: 2.5916 - acc: 0.377 - ETA: 5:16 - loss: 2.5937 - acc: 0.378 - ETA: 5:15 - loss: 2.5795 - acc: 0.382 - ETA: 5:14 - loss: 2.5837 - acc: 0.380 - ETA: 5:13 - loss: 2.5805 - acc: 0.379 - ETA: 5:12 - loss: 2.5798 - acc: 0.377 - ETA: 5:11 - loss: 2.5772 - acc: 0.377 - ETA: 5:10 - loss: 2.5964 - acc: 0.375 - ETA: 5:09 - loss: 2.5923 - acc: 0.376 - ETA: 5:08 - loss: 2.5832 - acc: 0.378 - ETA: 5:07 - loss: 2.5746 - acc: 0.380 - ETA: 5:05 - loss: 2.5618 - acc: 0.382 - ETA: 5:04 - loss: 2.5623 - acc: 0.379 - ETA: 5:04 - loss: 2.5742 - acc: 0.378 - ETA: 5:03 - loss: 2.5709 - acc: 0.380 - ETA: 5:02 - loss: 2.5684 - acc: 0.380 - ETA: 5:00 - loss: 2.5712 - acc: 0.381 - ETA: 4:59 - loss: 2.5838 - acc: 0.376 - ETA: 4:58 - loss: 2.5823 - acc: 0.376 - ETA: 4:57 - loss: 2.5728 - acc: 0.376 - ETA: 4:55 - loss: 2.5822 - acc: 0.373 - ETA: 4:54 - loss: 2.5764 - acc: 0.373 - ETA: 4:53 - loss: 2.5752 - acc: 0.374 - ETA: 4:52 - loss: 2.5668 - acc: 0.375 - ETA: 4:51 - loss: 2.5699 - acc: 0.377 - ETA: 4:49 - loss: 2.5722 - acc: 0.375 - ETA: 4:48 - loss: 2.5636 - acc: 0.378 - ETA: 4:47 - loss: 2.5645 - acc: 0.378 - ETA: 4:46 - loss: 2.5681 - acc: 0.376 - ETA: 4:46 - loss: 2.5732 - acc: 0.376 - ETA: 4:45 - loss: 2.5705 - acc: 0.376 - ETA: 4:44 - loss: 2.5712 - acc: 0.374 - ETA: 4:43 - loss: 2.5665 - acc: 0.376 - ETA: 4:42 - loss: 2.5659 - acc: 0.373 - ETA: 4:41 - loss: 2.5631 - acc: 0.373 - ETA: 4:40 - loss: 2.5764 - acc: 0.372 - ETA: 4:39 - loss: 2.5810 - acc: 0.372 - ETA: 4:38 - loss: 2.5795 - acc: 0.373 - ETA: 4:37 - loss: 2.5871 - acc: 0.371 - ETA: 4:35 - loss: 2.5897 - acc: 0.371 - ETA: 4:34 - loss: 2.5816 - acc: 0.372 - ETA: 4:33 - loss: 2.5795 - acc: 0.373 - ETA: 4:32 - loss: 2.5778 - acc: 0.372 - ETA: 4:31 - loss: 2.5788 - acc: 0.372 - ETA: 4:30 - loss: 2.5808 - acc: 0.372 - ETA: 4:29 - loss: 2.5752 - acc: 0.372 - ETA: 4:28 - loss: 2.5678 - acc: 0.374 - ETA: 4:27 - loss: 2.5691 - acc: 0.373 - ETA: 4:26 - loss: 2.5688 - acc: 0.372 - ETA: 4:24 - loss: 2.5726 - acc: 0.373 - ETA: 4:23 - loss: 2.5753 - acc: 0.371 - ETA: 4:22 - loss: 2.5753 - acc: 0.372 - ETA: 4:21 - loss: 2.5793 - acc: 0.369 - ETA: 4:20 - loss: 2.5740 - acc: 0.371 - ETA: 4:19 - loss: 2.5796 - acc: 0.370 - ETA: 4:18 - loss: 2.5757 - acc: 0.370 - ETA: 4:16 - loss: 2.5796 - acc: 0.369 - ETA: 4:15 - loss: 2.5789 - acc: 0.369 - ETA: 4:14 - loss: 2.5804 - acc: 0.368 - ETA: 4:13 - loss: 2.5820 - acc: 0.369 - ETA: 4:12 - loss: 2.5880 - acc: 0.368 - ETA: 4:11 - loss: 2.5906 - acc: 0.367 - ETA: 4:10 - loss: 2.5945 - acc: 0.368 - ETA: 4:09 - loss: 2.5904 - acc: 0.369 - ETA: 4:08 - loss: 2.5854 - acc: 0.369 - ETA: 4:07 - loss: 2.5840 - acc: 0.370 - ETA: 4:06 - loss: 2.5874 - acc: 0.369 - ETA: 4:05 - loss: 2.5927 - acc: 0.367 - ETA: 4:03 - loss: 2.5913 - acc: 0.367 - ETA: 4:02 - loss: 2.5934 - acc: 0.367 - ETA: 4:01 - loss: 2.5952 - acc: 0.366 - ETA: 4:00 - loss: 2.5959 - acc: 0.365 - ETA: 3:59 - loss: 2.5952 - acc: 0.365 - ETA: 3:58 - loss: 2.5934 - acc: 0.365 - ETA: 3:57 - loss: 2.5946 - acc: 0.365 - ETA: 3:56 - loss: 2.5912 - acc: 0.365 - ETA: 3:55 - loss: 2.5912 - acc: 0.365 - ETA: 3:54 - loss: 2.5922 - acc: 0.364 - ETA: 3:53 - loss: 2.5928 - acc: 0.365 - ETA: 3:52 - loss: 2.5922 - acc: 0.364 - ETA: 3:50 - loss: 2.5925 - acc: 0.364 - ETA: 3:49 - loss: 2.5936 - acc: 0.364 - ETA: 3:48 - loss: 2.5983 - acc: 0.363 - ETA: 3:47 - loss: 2.6001 - acc: 0.363 - ETA: 3:46 - loss: 2.6005 - acc: 0.364 - ETA: 3:45 - loss: 2.6035 - acc: 0.363 - ETA: 3:44 - loss: 2.6019 - acc: 0.363 - ETA: 3:43 - loss: 2.6047 - acc: 0.362 - ETA: 3:42 - loss: 2.6030 - acc: 0.362 - ETA: 3:41 - loss: 2.6009 - acc: 0.363 - ETA: 3:40 - loss: 2.5978 - acc: 0.364 - ETA: 3:39 - loss: 2.5965 - acc: 0.364 - ETA: 3:37 - loss: 2.5951 - acc: 0.365 - ETA: 3:36 - loss: 2.5927 - acc: 0.366 - ETA: 3:35 - loss: 2.5925 - acc: 0.366 - ETA: 3:34 - loss: 2.5911 - acc: 0.366 - ETA: 3:33 - loss: 2.5902 - acc: 0.365 - ETA: 3:32 - loss: 2.5897 - acc: 0.365 - ETA: 3:31 - loss: 2.5899 - acc: 0.365 - ETA: 3:30 - loss: 2.5913 - acc: 0.365 - ETA: 3:29 - loss: 2.5921 - acc: 0.365 - ETA: 3:28 - loss: 2.5878 - acc: 0.365 - ETA: 3:26 - loss: 2.5879 - acc: 0.364 - ETA: 3:25 - loss: 2.5968 - acc: 0.363 - ETA: 3:24 - loss: 2.5953 - acc: 0.364 - ETA: 3:23 - loss: 2.5948 - acc: 0.365 - ETA: 3:22 - loss: 2.5948 - acc: 0.365 - ETA: 3:21 - loss: 2.5979 - acc: 0.364 - ETA: 3:20 - loss: 2.5934 - acc: 0.365 - ETA: 3:19 - loss: 2.5896 - acc: 0.366 - ETA: 3:18 - loss: 2.5902 - acc: 0.366 - ETA: 3:17 - loss: 2.5931 - acc: 0.365 - ETA: 3:16 - loss: 2.5930 - acc: 0.365 - ETA: 3:15 - loss: 2.5868 - acc: 0.366 - ETA: 3:14 - loss: 2.5881 - acc: 0.366 - ETA: 3:13 - loss: 2.5865 - acc: 0.365 - ETA: 3:11 - loss: 2.5891 - acc: 0.365 - ETA: 3:10 - loss: 2.5924 - acc: 0.364 - ETA: 3:09 - loss: 2.5933 - acc: 0.364 - ETA: 3:08 - loss: 2.5924 - acc: 0.365 - ETA: 3:08 - loss: 2.5933 - acc: 0.364 - ETA: 3:06 - loss: 2.5937 - acc: 0.364 - ETA: 3:05 - loss: 2.5910 - acc: 0.365 - ETA: 3:04 - loss: 2.5903 - acc: 0.365 - ETA: 3:03 - loss: 2.5904 - acc: 0.364 - ETA: 3:02 - loss: 2.5927 - acc: 0.364 - ETA: 3:01 - loss: 2.5942 - acc: 0.363 - ETA: 3:00 - loss: 2.5976 - acc: 0.363 - ETA: 2:59 - loss: 2.5965 - acc: 0.363 - ETA: 2:58 - loss: 2.5985 - acc: 0.363 - ETA: 2:57 - loss: 2.5990 - acc: 0.362 - ETA: 2:56 - loss: 2.5977 - acc: 0.363 - ETA: 2:55 - loss: 2.6005 - acc: 0.362 - ETA: 2:54 - loss: 2.5983 - acc: 0.363 - ETA: 2:53 - loss: 2.5989 - acc: 0.362 - ETA: 2:52 - loss: 2.5996 - acc: 0.362 - ETA: 2:51 - loss: 2.6010 - acc: 0.363 - ETA: 2:49 - loss: 2.6001 - acc: 0.363 - ETA: 2:48 - loss: 2.6019 - acc: 0.362 - ETA: 2:47 - loss: 2.6029 - acc: 0.362 - ETA: 2:46 - loss: 2.6015 - acc: 0.362 - ETA: 2:45 - loss: 2.6008 - acc: 0.362 - ETA: 2:44 - loss: 2.6047 - acc: 0.362 - ETA: 2:43 - loss: 2.6039 - acc: 0.362 - ETA: 2:42 - loss: 2.6009 - acc: 0.363 - ETA: 2:41 - loss: 2.6029 - acc: 0.362 - ETA: 2:40 - loss: 2.6003 - acc: 0.363 - ETA: 2:39 - loss: 2.6036 - acc: 0.362 - ETA: 2:38 - loss: 2.6052 - acc: 0.361 - ETA: 2:37 - loss: 2.6057 - acc: 0.361 - ETA: 2:36 - loss: 2.6082 - acc: 0.361 - ETA: 2:35 - loss: 2.6079 - acc: 0.361 - ETA: 2:34 - loss: 2.6067 - acc: 0.361 - ETA: 2:32 - loss: 2.6064 - acc: 0.361 - ETA: 2:31 - loss: 2.6047 - acc: 0.361 - ETA: 2:30 - loss: 2.6032 - acc: 0.360 - ETA: 2:29 - loss: 2.6024 - acc: 0.361 - ETA: 2:28 - loss: 2.6011 - acc: 0.361 - ETA: 2:27 - loss: 2.5997 - acc: 0.361 - ETA: 2:26 - loss: 2.6039 - acc: 0.360 - ETA: 2:25 - loss: 2.6072 - acc: 0.359 - ETA: 2:24 - loss: 2.6121 - acc: 0.359 - ETA: 2:23 - loss: 2.6126 - acc: 0.359 - ETA: 2:22 - loss: 2.6138 - acc: 0.358 - ETA: 2:21 - loss: 2.6146 - acc: 0.358 - ETA: 2:20 - loss: 2.6118 - acc: 0.358 - ETA: 2:19 - loss: 2.6100 - acc: 0.359 - ETA: 2:17 - loss: 2.6128 - acc: 0.35866660/6680 [============================>.] - ETA: 2:16 - loss: 2.6140 - acc: 0.358 - ETA: 2:15 - loss: 2.6140 - acc: 0.358 - ETA: 2:14 - loss: 2.6173 - acc: 0.357 - ETA: 2:13 - loss: 2.6173 - acc: 0.357 - ETA: 2:12 - loss: 2.6173 - acc: 0.358 - ETA: 2:11 - loss: 2.6218 - acc: 0.358 - ETA: 2:10 - loss: 2.6211 - acc: 0.358 - ETA: 2:09 - loss: 2.6220 - acc: 0.358 - ETA: 2:08 - loss: 2.6208 - acc: 0.358 - ETA: 2:07 - loss: 2.6177 - acc: 0.358 - ETA: 2:06 - loss: 2.6181 - acc: 0.358 - ETA: 2:05 - loss: 2.6202 - acc: 0.358 - ETA: 2:04 - loss: 2.6215 - acc: 0.357 - ETA: 2:03 - loss: 2.6237 - acc: 0.357 - ETA: 2:02 - loss: 2.6264 - acc: 0.357 - ETA: 2:00 - loss: 2.6287 - acc: 0.357 - ETA: 1:59 - loss: 2.6282 - acc: 0.357 - ETA: 1:58 - loss: 2.6281 - acc: 0.357 - ETA: 1:57 - loss: 2.6305 - acc: 0.357 - ETA: 1:56 - loss: 2.6364 - acc: 0.356 - ETA: 1:55 - loss: 2.6366 - acc: 0.356 - ETA: 1:54 - loss: 2.6365 - acc: 0.356 - ETA: 1:53 - loss: 2.6357 - acc: 0.357 - ETA: 1:52 - loss: 2.6349 - acc: 0.357 - ETA: 1:51 - loss: 2.6365 - acc: 0.357 - ETA: 1:50 - loss: 2.6380 - acc: 0.357 - ETA: 1:49 - loss: 2.6395 - acc: 0.356 - ETA: 1:48 - loss: 2.6395 - acc: 0.356 - ETA: 1:47 - loss: 2.6369 - acc: 0.356 - ETA: 1:46 - loss: 2.6407 - acc: 0.356 - ETA: 1:44 - loss: 2.6395 - acc: 0.356 - ETA: 1:43 - loss: 2.6388 - acc: 0.355 - ETA: 1:42 - loss: 2.6374 - acc: 0.355 - ETA: 1:41 - loss: 2.6385 - acc: 0.355 - ETA: 1:40 - loss: 2.6384 - acc: 0.354 - ETA: 1:39 - loss: 2.6399 - acc: 0.354 - ETA: 1:38 - loss: 2.6402 - acc: 0.353 - ETA: 1:37 - loss: 2.6421 - acc: 0.352 - ETA: 1:36 - loss: 2.6417 - acc: 0.352 - ETA: 1:35 - loss: 2.6404 - acc: 0.352 - ETA: 1:34 - loss: 2.6401 - acc: 0.352 - ETA: 1:33 - loss: 2.6402 - acc: 0.352 - ETA: 1:32 - loss: 2.6407 - acc: 0.351 - ETA: 1:31 - loss: 2.6466 - acc: 0.350 - ETA: 1:30 - loss: 2.6457 - acc: 0.351 - ETA: 1:29 - loss: 2.6451 - acc: 0.351 - ETA: 1:27 - loss: 2.6447 - acc: 0.352 - ETA: 1:26 - loss: 2.6445 - acc: 0.351 - ETA: 1:25 - loss: 2.6440 - acc: 0.352 - ETA: 1:24 - loss: 2.6443 - acc: 0.352 - ETA: 1:23 - loss: 2.6431 - acc: 0.352 - ETA: 1:22 - loss: 2.6419 - acc: 0.352 - ETA: 1:21 - loss: 2.6426 - acc: 0.352 - ETA: 1:20 - loss: 2.6404 - acc: 0.352 - ETA: 1:19 - loss: 2.6435 - acc: 0.352 - ETA: 1:18 - loss: 2.6426 - acc: 0.352 - ETA: 1:17 - loss: 2.6423 - acc: 0.352 - ETA: 1:16 - loss: 2.6417 - acc: 0.352 - ETA: 1:15 - loss: 2.6412 - acc: 0.352 - ETA: 1:14 - loss: 2.6428 - acc: 0.352 - ETA: 1:13 - loss: 2.6447 - acc: 0.351 - ETA: 1:12 - loss: 2.6467 - acc: 0.351 - ETA: 1:10 - loss: 2.6474 - acc: 0.351 - ETA: 1:09 - loss: 2.6483 - acc: 0.350 - ETA: 1:08 - loss: 2.6474 - acc: 0.350 - ETA: 1:07 - loss: 2.6506 - acc: 0.350 - ETA: 1:06 - loss: 2.6487 - acc: 0.350 - ETA: 1:05 - loss: 2.6471 - acc: 0.350 - ETA: 1:04 - loss: 2.6491 - acc: 0.350 - ETA: 1:03 - loss: 2.6474 - acc: 0.350 - ETA: 1:02 - loss: 2.6482 - acc: 0.350 - ETA: 1:01 - loss: 2.6464 - acc: 0.350 - ETA: 1:00 - loss: 2.6455 - acc: 0.350 - ETA: 59s - loss: 2.6488 - acc: 0.350 - ETA: 58s - loss: 2.6502 - acc: 0.34 - ETA: 57s - loss: 2.6509 - acc: 0.34 - ETA: 56s - loss: 2.6521 - acc: 0.34 - ETA: 55s - loss: 2.6547 - acc: 0.34 - ETA: 54s - loss: 2.6550 - acc: 0.34 - ETA: 52s - loss: 2.6571 - acc: 0.34 - ETA: 51s - loss: 2.6587 - acc: 0.34 - ETA: 50s - loss: 2.6571 - acc: 0.34 - ETA: 49s - loss: 2.6548 - acc: 0.34 - ETA: 48s - loss: 2.6564 - acc: 0.34 - ETA: 47s - loss: 2.6581 - acc: 0.34 - ETA: 46s - loss: 2.6579 - acc: 0.34 - ETA: 45s - loss: 2.6574 - acc: 0.34 - ETA: 44s - loss: 2.6577 - acc: 0.34 - ETA: 43s - loss: 2.6554 - acc: 0.34 - ETA: 42s - loss: 2.6541 - acc: 0.34 - ETA: 41s - loss: 2.6551 - acc: 0.34 - ETA: 40s - loss: 2.6557 - acc: 0.34 - ETA: 39s - loss: 2.6530 - acc: 0.34 - ETA: 38s - loss: 2.6530 - acc: 0.34 - ETA: 37s - loss: 2.6537 - acc: 0.34 - ETA: 36s - loss: 2.6536 - acc: 0.34 - ETA: 34s - loss: 2.6539 - acc: 0.34 - ETA: 33s - loss: 2.6548 - acc: 0.34 - ETA: 32s - loss: 2.6541 - acc: 0.34 - ETA: 31s - loss: 2.6547 - acc: 0.34 - ETA: 30s - loss: 2.6548 - acc: 0.34 - ETA: 29s - loss: 2.6539 - acc: 0.34 - ETA: 28s - loss: 2.6533 - acc: 0.34 - ETA: 27s - loss: 2.6535 - acc: 0.34 - ETA: 26s - loss: 2.6526 - acc: 0.34 - ETA: 25s - loss: 2.6514 - acc: 0.34 - ETA: 24s - loss: 2.6498 - acc: 0.34 - ETA: 23s - loss: 2.6488 - acc: 0.35 - ETA: 22s - loss: 2.6506 - acc: 0.34 - ETA: 21s - loss: 2.6517 - acc: 0.34 - ETA: 20s - loss: 2.6524 - acc: 0.34 - ETA: 19s - loss: 2.6492 - acc: 0.34 - ETA: 18s - loss: 2.6496 - acc: 0.35 - ETA: 16s - loss: 2.6498 - acc: 0.35 - ETA: 15s - loss: 2.6519 - acc: 0.35 - ETA: 14s - loss: 2.6542 - acc: 0.34 - ETA: 13s - loss: 2.6540 - acc: 0.34 - ETA: 12s - loss: 2.6540 - acc: 0.34 - ETA: 11s - loss: 2.6527 - acc: 0.35 - ETA: 10s - loss: 2.6540 - acc: 0.35 - ETA: 9s - loss: 2.6559 - acc: 0.3503 - ETA: 8s - loss: 2.6574 - acc: 0.349 - ETA: 7s - loss: 2.6580 - acc: 0.350 - ETA: 6s - loss: 2.6566 - acc: 0.350 - ETA: 5s - loss: 2.6568 - acc: 0.350 - ETA: 4s - loss: 2.6575 - acc: 0.350 - ETA: 3s - loss: 2.6602 - acc: 0.350 - ETA: 2s - loss: 2.6613 - acc: 0.350 - ETA: 1s - loss: 2.6603 - acc: 0.3506Epoch 00005: val\_loss did not improve
6680/6680 [==============================] - 365s 55ms/step - loss: 2.6582 - acc: 0.3509 - val\_loss: 4.3202 - val\_acc: 0.0970
Epoch 6/8
4080/6680 [=================>{\ldots}] - ETA: 5:45 - loss: 1.8797 - acc: 0.600 - ETA: 5:43 - loss: 1.8885 - acc: 0.600 - ETA: 5:42 - loss: 1.8828 - acc: 0.566 - ETA: 5:39 - loss: 1.7646 - acc: 0.600 - ETA: 5:37 - loss: 1.7993 - acc: 0.600 - ETA: 5:36 - loss: 1.7468 - acc: 0.616 - ETA: 5:35 - loss: 1.8309 - acc: 0.578 - ETA: 5:38 - loss: 1.8302 - acc: 0.581 - ETA: 5:41 - loss: 1.8040 - acc: 0.583 - ETA: 5:40 - loss: 1.7801 - acc: 0.580 - ETA: 5:38 - loss: 1.8251 - acc: 0.568 - ETA: 5:37 - loss: 1.8224 - acc: 0.570 - ETA: 5:35 - loss: 1.7771 - acc: 0.573 - ETA: 5:34 - loss: 1.7736 - acc: 0.575 - ETA: 5:33 - loss: 1.7381 - acc: 0.583 - ETA: 5:32 - loss: 1.7554 - acc: 0.575 - ETA: 5:30 - loss: 1.7598 - acc: 0.573 - ETA: 5:29 - loss: 1.7427 - acc: 0.572 - ETA: 5:28 - loss: 1.7515 - acc: 0.568 - ETA: 5:27 - loss: 1.7646 - acc: 0.562 - ETA: 5:26 - loss: 1.7648 - acc: 0.564 - ETA: 5:24 - loss: 1.7462 - acc: 0.572 - ETA: 5:24 - loss: 1.7563 - acc: 0.576 - ETA: 5:24 - loss: 1.7386 - acc: 0.577 - ETA: 5:23 - loss: 1.7305 - acc: 0.578 - ETA: 5:22 - loss: 1.7140 - acc: 0.578 - ETA: 5:21 - loss: 1.7304 - acc: 0.577 - ETA: 5:20 - loss: 1.7428 - acc: 0.575 - ETA: 5:18 - loss: 1.7441 - acc: 0.569 - ETA: 5:17 - loss: 1.7419 - acc: 0.565 - ETA: 5:16 - loss: 1.7258 - acc: 0.572 - ETA: 5:15 - loss: 1.7255 - acc: 0.575 - ETA: 5:14 - loss: 1.7104 - acc: 0.580 - ETA: 5:12 - loss: 1.7275 - acc: 0.577 - ETA: 5:11 - loss: 1.7407 - acc: 0.572 - ETA: 5:10 - loss: 1.7357 - acc: 0.575 - ETA: 5:09 - loss: 1.7143 - acc: 0.579 - ETA: 5:08 - loss: 1.7020 - acc: 0.580 - ETA: 5:07 - loss: 1.6959 - acc: 0.580 - ETA: 5:08 - loss: 1.7048 - acc: 0.581 - ETA: 5:07 - loss: 1.6925 - acc: 0.584 - ETA: 5:06 - loss: 1.7161 - acc: 0.579 - ETA: 5:05 - loss: 1.7131 - acc: 0.577 - ETA: 5:03 - loss: 1.7255 - acc: 0.571 - ETA: 5:02 - loss: 1.7293 - acc: 0.571 - ETA: 5:01 - loss: 1.7225 - acc: 0.571 - ETA: 5:00 - loss: 1.7192 - acc: 0.570 - ETA: 4:59 - loss: 1.7285 - acc: 0.568 - ETA: 4:58 - loss: 1.7206 - acc: 0.568 - ETA: 4:57 - loss: 1.7110 - acc: 0.570 - ETA: 4:55 - loss: 1.7189 - acc: 0.568 - ETA: 4:54 - loss: 1.7150 - acc: 0.569 - ETA: 4:53 - loss: 1.7144 - acc: 0.570 - ETA: 4:52 - loss: 1.7226 - acc: 0.568 - ETA: 4:52 - loss: 1.7192 - acc: 0.571 - ETA: 4:51 - loss: 1.7241 - acc: 0.571 - ETA: 4:50 - loss: 1.7223 - acc: 0.571 - ETA: 4:49 - loss: 1.7239 - acc: 0.570 - ETA: 4:47 - loss: 1.7198 - acc: 0.571 - ETA: 4:46 - loss: 1.7183 - acc: 0.570 - ETA: 4:45 - loss: 1.7132 - acc: 0.571 - ETA: 4:44 - loss: 1.7118 - acc: 0.571 - ETA: 4:43 - loss: 1.7168 - acc: 0.572 - ETA: 4:42 - loss: 1.7140 - acc: 0.571 - ETA: 4:41 - loss: 1.7156 - acc: 0.571 - ETA: 4:40 - loss: 1.7237 - acc: 0.570 - ETA: 4:39 - loss: 1.7246 - acc: 0.571 - ETA: 4:38 - loss: 1.7234 - acc: 0.569 - ETA: 4:37 - loss: 1.7259 - acc: 0.568 - ETA: 4:36 - loss: 1.7255 - acc: 0.567 - ETA: 4:35 - loss: 1.7341 - acc: 0.566 - ETA: 4:34 - loss: 1.7378 - acc: 0.566 - ETA: 4:33 - loss: 1.7340 - acc: 0.567 - ETA: 4:32 - loss: 1.7355 - acc: 0.566 - ETA: 4:31 - loss: 1.7234 - acc: 0.568 - ETA: 4:29 - loss: 1.7294 - acc: 0.567 - ETA: 4:28 - loss: 1.7251 - acc: 0.567 - ETA: 4:27 - loss: 1.7136 - acc: 0.571 - ETA: 4:26 - loss: 1.7144 - acc: 0.571 - ETA: 4:25 - loss: 1.7156 - acc: 0.570 - ETA: 4:24 - loss: 1.7194 - acc: 0.571 - ETA: 4:23 - loss: 1.7144 - acc: 0.573 - ETA: 4:22 - loss: 1.7189 - acc: 0.572 - ETA: 4:21 - loss: 1.7152 - acc: 0.572 - ETA: 4:20 - loss: 1.7206 - acc: 0.571 - ETA: 4:19 - loss: 1.7156 - acc: 0.572 - ETA: 4:18 - loss: 1.7139 - acc: 0.572 - ETA: 4:17 - loss: 1.7172 - acc: 0.572 - ETA: 4:16 - loss: 1.7179 - acc: 0.571 - ETA: 4:15 - loss: 1.7208 - acc: 0.571 - ETA: 4:14 - loss: 1.7211 - acc: 0.572 - ETA: 4:13 - loss: 1.7158 - acc: 0.573 - ETA: 4:11 - loss: 1.7206 - acc: 0.573 - ETA: 4:10 - loss: 1.7181 - acc: 0.575 - ETA: 4:09 - loss: 1.7193 - acc: 0.573 - ETA: 4:08 - loss: 1.7169 - acc: 0.574 - ETA: 4:07 - loss: 1.7133 - acc: 0.575 - ETA: 4:06 - loss: 1.7135 - acc: 0.575 - ETA: 4:05 - loss: 1.7157 - acc: 0.574 - ETA: 4:04 - loss: 1.7169 - acc: 0.573 - ETA: 4:03 - loss: 1.7110 - acc: 0.575 - ETA: 4:02 - loss: 1.7121 - acc: 0.575 - ETA: 4:01 - loss: 1.7093 - acc: 0.574 - ETA: 4:00 - loss: 1.7108 - acc: 0.574 - ETA: 3:59 - loss: 1.7090 - acc: 0.574 - ETA: 3:58 - loss: 1.7084 - acc: 0.575 - ETA: 3:57 - loss: 1.7067 - acc: 0.575 - ETA: 3:56 - loss: 1.7075 - acc: 0.575 - ETA: 3:55 - loss: 1.7121 - acc: 0.575 - ETA: 3:53 - loss: 1.7132 - acc: 0.575 - ETA: 3:52 - loss: 1.7139 - acc: 0.574 - ETA: 3:51 - loss: 1.7135 - acc: 0.575 - ETA: 3:50 - loss: 1.7172 - acc: 0.574 - ETA: 3:49 - loss: 1.7137 - acc: 0.575 - ETA: 3:48 - loss: 1.7149 - acc: 0.574 - ETA: 3:47 - loss: 1.7207 - acc: 0.573 - ETA: 3:46 - loss: 1.7270 - acc: 0.572 - ETA: 3:45 - loss: 1.7246 - acc: 0.572 - ETA: 3:44 - loss: 1.7278 - acc: 0.571 - ETA: 3:43 - loss: 1.7261 - acc: 0.572 - ETA: 3:42 - loss: 1.7273 - acc: 0.571 - ETA: 3:41 - loss: 1.7306 - acc: 0.570 - ETA: 3:40 - loss: 1.7293 - acc: 0.572 - ETA: 3:39 - loss: 1.7293 - acc: 0.571 - ETA: 3:38 - loss: 1.7281 - acc: 0.572 - ETA: 3:37 - loss: 1.7261 - acc: 0.571 - ETA: 3:36 - loss: 1.7344 - acc: 0.570 - ETA: 3:35 - loss: 1.7409 - acc: 0.568 - ETA: 3:33 - loss: 1.7395 - acc: 0.567 - ETA: 3:32 - loss: 1.7354 - acc: 0.568 - ETA: 3:32 - loss: 1.7397 - acc: 0.568 - ETA: 3:31 - loss: 1.7393 - acc: 0.569 - ETA: 3:29 - loss: 1.7436 - acc: 0.567 - ETA: 3:28 - loss: 1.7403 - acc: 0.569 - ETA: 3:27 - loss: 1.7372 - acc: 0.570 - ETA: 3:26 - loss: 1.7357 - acc: 0.570 - ETA: 3:25 - loss: 1.7316 - acc: 0.571 - ETA: 3:24 - loss: 1.7311 - acc: 0.572 - ETA: 3:23 - loss: 1.7293 - acc: 0.573 - ETA: 3:22 - loss: 1.7329 - acc: 0.572 - ETA: 3:21 - loss: 1.7366 - acc: 0.571 - ETA: 3:20 - loss: 1.7326 - acc: 0.571 - ETA: 3:19 - loss: 1.7308 - acc: 0.571 - ETA: 3:18 - loss: 1.7396 - acc: 0.570 - ETA: 3:17 - loss: 1.7455 - acc: 0.570 - ETA: 3:16 - loss: 1.7423 - acc: 0.571 - ETA: 3:15 - loss: 1.7422 - acc: 0.570 - ETA: 3:14 - loss: 1.7401 - acc: 0.571 - ETA: 3:13 - loss: 1.7366 - acc: 0.572 - ETA: 3:12 - loss: 1.7359 - acc: 0.572 - ETA: 3:11 - loss: 1.7325 - acc: 0.572 - ETA: 3:10 - loss: 1.7364 - acc: 0.572 - ETA: 3:08 - loss: 1.7419 - acc: 0.571 - ETA: 3:07 - loss: 1.7382 - acc: 0.571 - ETA: 3:06 - loss: 1.7371 - acc: 0.573 - ETA: 3:05 - loss: 1.7346 - acc: 0.573 - ETA: 3:04 - loss: 1.7337 - acc: 0.573 - ETA: 3:03 - loss: 1.7305 - acc: 0.573 - ETA: 3:02 - loss: 1.7315 - acc: 0.573 - ETA: 3:01 - loss: 1.7298 - acc: 0.573 - ETA: 3:00 - loss: 1.7284 - acc: 0.573 - ETA: 2:59 - loss: 1.7321 - acc: 0.571 - ETA: 2:58 - loss: 1.7277 - acc: 0.572 - ETA: 2:57 - loss: 1.7285 - acc: 0.572 - ETA: 2:56 - loss: 1.7300 - acc: 0.572 - ETA: 2:55 - loss: 1.7271 - acc: 0.572 - ETA: 2:54 - loss: 1.7274 - acc: 0.572 - ETA: 2:53 - loss: 1.7245 - acc: 0.572 - ETA: 2:52 - loss: 1.7239 - acc: 0.572 - ETA: 2:51 - loss: 1.7262 - acc: 0.572 - ETA: 2:50 - loss: 1.7280 - acc: 0.571 - ETA: 2:49 - loss: 1.7286 - acc: 0.572 - ETA: 2:48 - loss: 1.7257 - acc: 0.572 - ETA: 2:47 - loss: 1.7261 - acc: 0.571 - ETA: 2:46 - loss: 1.7242 - acc: 0.572 - ETA: 2:45 - loss: 1.7244 - acc: 0.572 - ETA: 2:44 - loss: 1.7221 - acc: 0.572 - ETA: 2:43 - loss: 1.7227 - acc: 0.572 - ETA: 2:42 - loss: 1.7239 - acc: 0.572 - ETA: 2:40 - loss: 1.7244 - acc: 0.571 - ETA: 2:39 - loss: 1.7219 - acc: 0.572 - ETA: 2:38 - loss: 1.7186 - acc: 0.573 - ETA: 2:37 - loss: 1.7227 - acc: 0.572 - ETA: 2:36 - loss: 1.7250 - acc: 0.571 - ETA: 2:35 - loss: 1.7236 - acc: 0.571 - ETA: 2:34 - loss: 1.7217 - acc: 0.572 - ETA: 2:33 - loss: 1.7213 - acc: 0.572 - ETA: 2:32 - loss: 1.7239 - acc: 0.571 - ETA: 2:31 - loss: 1.7236 - acc: 0.571 - ETA: 2:30 - loss: 1.7226 - acc: 0.571 - ETA: 2:29 - loss: 1.7190 - acc: 0.572 - ETA: 2:28 - loss: 1.7162 - acc: 0.572 - ETA: 2:27 - loss: 1.7154 - acc: 0.572 - ETA: 2:26 - loss: 1.7176 - acc: 0.571 - ETA: 2:25 - loss: 1.7198 - acc: 0.571 - ETA: 2:24 - loss: 1.7252 - acc: 0.569 - ETA: 2:23 - loss: 1.7254 - acc: 0.569 - ETA: 2:22 - loss: 1.7235 - acc: 0.569 - ETA: 2:21 - loss: 1.7259 - acc: 0.569 - ETA: 2:20 - loss: 1.7282 - acc: 0.568 - ETA: 2:18 - loss: 1.7291 - acc: 0.568 - ETA: 2:17 - loss: 1.7351 - acc: 0.567 - ETA: 2:16 - loss: 1.7370 - acc: 0.566 - ETA: 2:15 - loss: 1.7374 - acc: 0.56676660/6680 [============================>.] - ETA: 2:14 - loss: 1.7380 - acc: 0.566 - ETA: 2:13 - loss: 1.7432 - acc: 0.565 - ETA: 2:12 - loss: 1.7435 - acc: 0.564 - ETA: 2:11 - loss: 1.7420 - acc: 0.564 - ETA: 2:10 - loss: 1.7408 - acc: 0.565 - ETA: 2:09 - loss: 1.7437 - acc: 0.564 - ETA: 2:08 - loss: 1.7443 - acc: 0.564 - ETA: 2:07 - loss: 1.7438 - acc: 0.565 - ETA: 2:06 - loss: 1.7426 - acc: 0.565 - ETA: 2:05 - loss: 1.7426 - acc: 0.565 - ETA: 2:04 - loss: 1.7437 - acc: 0.565 - ETA: 2:03 - loss: 1.7428 - acc: 0.565 - ETA: 2:02 - loss: 1.7430 - acc: 0.565 - ETA: 2:01 - loss: 1.7412 - acc: 0.566 - ETA: 2:00 - loss: 1.7407 - acc: 0.566 - ETA: 1:59 - loss: 1.7405 - acc: 0.567 - ETA: 1:58 - loss: 1.7414 - acc: 0.566 - ETA: 1:56 - loss: 1.7437 - acc: 0.566 - ETA: 1:55 - loss: 1.7461 - acc: 0.565 - ETA: 1:54 - loss: 1.7459 - acc: 0.565 - ETA: 1:53 - loss: 1.7462 - acc: 0.564 - ETA: 1:52 - loss: 1.7468 - acc: 0.564 - ETA: 1:51 - loss: 1.7477 - acc: 0.564 - ETA: 1:50 - loss: 1.7492 - acc: 0.564 - ETA: 1:49 - loss: 1.7495 - acc: 0.563 - ETA: 1:48 - loss: 1.7512 - acc: 0.562 - ETA: 1:47 - loss: 1.7522 - acc: 0.562 - ETA: 1:46 - loss: 1.7523 - acc: 0.563 - ETA: 1:45 - loss: 1.7505 - acc: 0.563 - ETA: 1:44 - loss: 1.7520 - acc: 0.563 - ETA: 1:43 - loss: 1.7504 - acc: 0.563 - ETA: 1:42 - loss: 1.7512 - acc: 0.563 - ETA: 1:41 - loss: 1.7527 - acc: 0.563 - ETA: 1:40 - loss: 1.7518 - acc: 0.563 - ETA: 1:39 - loss: 1.7517 - acc: 0.563 - ETA: 1:38 - loss: 1.7518 - acc: 0.562 - ETA: 1:37 - loss: 1.7519 - acc: 0.562 - ETA: 1:36 - loss: 1.7546 - acc: 0.561 - ETA: 1:35 - loss: 1.7536 - acc: 0.562 - ETA: 1:34 - loss: 1.7539 - acc: 0.562 - ETA: 1:32 - loss: 1.7513 - acc: 0.563 - ETA: 1:31 - loss: 1.7500 - acc: 0.563 - ETA: 1:30 - loss: 1.7496 - acc: 0.563 - ETA: 1:29 - loss: 1.7493 - acc: 0.563 - ETA: 1:28 - loss: 1.7524 - acc: 0.562 - ETA: 1:27 - loss: 1.7527 - acc: 0.562 - ETA: 1:26 - loss: 1.7559 - acc: 0.561 - ETA: 1:25 - loss: 1.7538 - acc: 0.561 - ETA: 1:24 - loss: 1.7557 - acc: 0.561 - ETA: 1:23 - loss: 1.7567 - acc: 0.561 - ETA: 1:22 - loss: 1.7564 - acc: 0.560 - ETA: 1:21 - loss: 1.7558 - acc: 0.560 - ETA: 1:20 - loss: 1.7560 - acc: 0.560 - ETA: 1:19 - loss: 1.7589 - acc: 0.560 - ETA: 1:18 - loss: 1.7630 - acc: 0.559 - ETA: 1:17 - loss: 1.7658 - acc: 0.559 - ETA: 1:16 - loss: 1.7661 - acc: 0.558 - ETA: 1:15 - loss: 1.7665 - acc: 0.558 - ETA: 1:14 - loss: 1.7692 - acc: 0.558 - ETA: 1:13 - loss: 1.7682 - acc: 0.558 - ETA: 1:12 - loss: 1.7683 - acc: 0.558 - ETA: 1:10 - loss: 1.7696 - acc: 0.558 - ETA: 1:09 - loss: 1.7673 - acc: 0.559 - ETA: 1:08 - loss: 1.7658 - acc: 0.559 - ETA: 1:07 - loss: 1.7647 - acc: 0.559 - ETA: 1:06 - loss: 1.7633 - acc: 0.559 - ETA: 1:05 - loss: 1.7639 - acc: 0.559 - ETA: 1:04 - loss: 1.7646 - acc: 0.559 - ETA: 1:03 - loss: 1.7673 - acc: 0.558 - ETA: 1:02 - loss: 1.7668 - acc: 0.558 - ETA: 1:01 - loss: 1.7645 - acc: 0.559 - ETA: 1:00 - loss: 1.7639 - acc: 0.559 - ETA: 59s - loss: 1.7638 - acc: 0.559 - ETA: 58s - loss: 1.7643 - acc: 0.55 - ETA: 57s - loss: 1.7635 - acc: 0.55 - ETA: 56s - loss: 1.7649 - acc: 0.55 - ETA: 55s - loss: 1.7649 - acc: 0.55 - ETA: 54s - loss: 1.7649 - acc: 0.55 - ETA: 53s - loss: 1.7649 - acc: 0.55 - ETA: 52s - loss: 1.7635 - acc: 0.55 - ETA: 51s - loss: 1.7629 - acc: 0.55 - ETA: 50s - loss: 1.7634 - acc: 0.55 - ETA: 49s - loss: 1.7638 - acc: 0.55 - ETA: 48s - loss: 1.7645 - acc: 0.55 - ETA: 47s - loss: 1.7665 - acc: 0.55 - ETA: 45s - loss: 1.7672 - acc: 0.55 - ETA: 44s - loss: 1.7675 - acc: 0.55 - ETA: 43s - loss: 1.7680 - acc: 0.55 - ETA: 42s - loss: 1.7706 - acc: 0.55 - ETA: 41s - loss: 1.7705 - acc: 0.55 - ETA: 40s - loss: 1.7715 - acc: 0.55 - ETA: 39s - loss: 1.7728 - acc: 0.55 - ETA: 38s - loss: 1.7740 - acc: 0.55 - ETA: 37s - loss: 1.7749 - acc: 0.55 - ETA: 36s - loss: 1.7757 - acc: 0.55 - ETA: 35s - loss: 1.7749 - acc: 0.55 - ETA: 34s - loss: 1.7747 - acc: 0.55 - ETA: 33s - loss: 1.7752 - acc: 0.55 - ETA: 32s - loss: 1.7733 - acc: 0.55 - ETA: 31s - loss: 1.7709 - acc: 0.55 - ETA: 30s - loss: 1.7707 - acc: 0.55 - ETA: 29s - loss: 1.7703 - acc: 0.55 - ETA: 28s - loss: 1.7709 - acc: 0.55 - ETA: 27s - loss: 1.7729 - acc: 0.55 - ETA: 26s - loss: 1.7738 - acc: 0.55 - ETA: 25s - loss: 1.7725 - acc: 0.55 - ETA: 24s - loss: 1.7730 - acc: 0.55 - ETA: 22s - loss: 1.7740 - acc: 0.55 - ETA: 21s - loss: 1.7752 - acc: 0.55 - ETA: 20s - loss: 1.7753 - acc: 0.55 - ETA: 19s - loss: 1.7758 - acc: 0.55 - ETA: 18s - loss: 1.7756 - acc: 0.55 - ETA: 17s - loss: 1.7793 - acc: 0.55 - ETA: 16s - loss: 1.7791 - acc: 0.55 - ETA: 15s - loss: 1.7773 - acc: 0.55 - ETA: 14s - loss: 1.7792 - acc: 0.55 - ETA: 13s - loss: 1.7794 - acc: 0.55 - ETA: 12s - loss: 1.7797 - acc: 0.55 - ETA: 11s - loss: 1.7777 - acc: 0.55 - ETA: 10s - loss: 1.7794 - acc: 0.55 - ETA: 9s - loss: 1.7823 - acc: 0.5531 - ETA: 8s - loss: 1.7827 - acc: 0.552 - ETA: 7s - loss: 1.7833 - acc: 0.552 - ETA: 6s - loss: 1.7847 - acc: 0.552 - ETA: 5s - loss: 1.7854 - acc: 0.552 - ETA: 4s - loss: 1.7829 - acc: 0.552 - ETA: 3s - loss: 1.7833 - acc: 0.552 - ETA: 2s - loss: 1.7866 - acc: 0.552 - ETA: 1s - loss: 1.7887 - acc: 0.5517Epoch 00006: val\_loss did not improve
6680/6680 [==============================] - 360s 54ms/step - loss: 1.7915 - acc: 0.5512 - val\_loss: 4.6563 - val\_acc: 0.0922
Epoch 7/8
4080/6680 [=================>{\ldots}] - ETA: 6:26 - loss: 1.0490 - acc: 0.750 - ETA: 6:09 - loss: 1.0670 - acc: 0.750 - ETA: 6:00 - loss: 1.1226 - acc: 0.716 - ETA: 5:53 - loss: 1.1194 - acc: 0.712 - ETA: 5:49 - loss: 1.0741 - acc: 0.730 - ETA: 5:46 - loss: 1.1038 - acc: 0.700 - ETA: 5:45 - loss: 1.1007 - acc: 0.692 - ETA: 5:42 - loss: 1.0948 - acc: 0.687 - ETA: 5:40 - loss: 1.1285 - acc: 0.666 - ETA: 5:40 - loss: 1.1208 - acc: 0.670 - ETA: 5:38 - loss: 1.0930 - acc: 0.681 - ETA: 5:36 - loss: 1.1500 - acc: 0.670 - ETA: 5:35 - loss: 1.1745 - acc: 0.669 - ETA: 5:34 - loss: 1.1789 - acc: 0.660 - ETA: 5:32 - loss: 1.1928 - acc: 0.663 - ETA: 5:33 - loss: 1.1681 - acc: 0.668 - ETA: 5:33 - loss: 1.1400 - acc: 0.682 - ETA: 5:32 - loss: 1.1348 - acc: 0.680 - ETA: 5:30 - loss: 1.1222 - acc: 0.686 - ETA: 5:29 - loss: 1.1159 - acc: 0.685 - ETA: 5:28 - loss: 1.1237 - acc: 0.683 - ETA: 5:27 - loss: 1.1267 - acc: 0.684 - ETA: 5:25 - loss: 1.1220 - acc: 0.684 - ETA: 5:24 - loss: 1.1188 - acc: 0.685 - ETA: 5:23 - loss: 1.1272 - acc: 0.678 - ETA: 5:22 - loss: 1.1313 - acc: 0.678 - ETA: 5:20 - loss: 1.1192 - acc: 0.679 - ETA: 5:19 - loss: 1.1154 - acc: 0.682 - ETA: 5:18 - loss: 1.1192 - acc: 0.681 - ETA: 5:17 - loss: 1.1239 - acc: 0.681 - ETA: 5:16 - loss: 1.1184 - acc: 0.683 - ETA: 5:17 - loss: 1.1189 - acc: 0.684 - ETA: 5:16 - loss: 1.1139 - acc: 0.683 - ETA: 5:14 - loss: 1.1063 - acc: 0.682 - ETA: 5:13 - loss: 1.1140 - acc: 0.680 - ETA: 5:12 - loss: 1.0927 - acc: 0.687 - ETA: 5:11 - loss: 1.0769 - acc: 0.693 - ETA: 5:09 - loss: 1.0691 - acc: 0.693 - ETA: 5:09 - loss: 1.0565 - acc: 0.698 - ETA: 5:08 - loss: 1.0563 - acc: 0.697 - ETA: 5:07 - loss: 1.0759 - acc: 0.692 - ETA: 5:06 - loss: 1.0698 - acc: 0.695 - ETA: 5:05 - loss: 1.0730 - acc: 0.695 - ETA: 5:04 - loss: 1.0678 - acc: 0.696 - ETA: 5:03 - loss: 1.0596 - acc: 0.698 - ETA: 5:02 - loss: 1.0700 - acc: 0.696 - ETA: 5:01 - loss: 1.0741 - acc: 0.694 - ETA: 5:00 - loss: 1.0745 - acc: 0.695 - ETA: 4:59 - loss: 1.0877 - acc: 0.694 - ETA: 4:58 - loss: 1.0807 - acc: 0.697 - ETA: 4:57 - loss: 1.0773 - acc: 0.699 - ETA: 4:55 - loss: 1.0737 - acc: 0.700 - ETA: 4:54 - loss: 1.0884 - acc: 0.695 - ETA: 4:53 - loss: 1.0894 - acc: 0.696 - ETA: 4:52 - loss: 1.0826 - acc: 0.699 - ETA: 4:51 - loss: 1.0987 - acc: 0.695 - ETA: 4:50 - loss: 1.0947 - acc: 0.695 - ETA: 4:49 - loss: 1.0911 - acc: 0.698 - ETA: 4:48 - loss: 1.0947 - acc: 0.698 - ETA: 4:47 - loss: 1.0962 - acc: 0.697 - ETA: 4:46 - loss: 1.1000 - acc: 0.697 - ETA: 4:45 - loss: 1.1052 - acc: 0.696 - ETA: 4:44 - loss: 1.1124 - acc: 0.693 - ETA: 4:43 - loss: 1.1131 - acc: 0.694 - ETA: 4:42 - loss: 1.1170 - acc: 0.693 - ETA: 4:41 - loss: 1.1176 - acc: 0.691 - ETA: 4:40 - loss: 1.1181 - acc: 0.691 - ETA: 4:39 - loss: 1.1153 - acc: 0.692 - ETA: 4:37 - loss: 1.1104 - acc: 0.692 - ETA: 4:36 - loss: 1.1125 - acc: 0.692 - ETA: 4:35 - loss: 1.1066 - acc: 0.693 - ETA: 4:34 - loss: 1.1205 - acc: 0.691 - ETA: 4:33 - loss: 1.1251 - acc: 0.689 - ETA: 4:32 - loss: 1.1224 - acc: 0.689 - ETA: 4:31 - loss: 1.1180 - acc: 0.690 - ETA: 4:30 - loss: 1.1184 - acc: 0.689 - ETA: 4:29 - loss: 1.1103 - acc: 0.691 - ETA: 4:28 - loss: 1.1144 - acc: 0.691 - ETA: 4:27 - loss: 1.1111 - acc: 0.691 - ETA: 4:26 - loss: 1.1176 - acc: 0.689 - ETA: 4:25 - loss: 1.1143 - acc: 0.690 - ETA: 4:24 - loss: 1.1155 - acc: 0.688 - ETA: 4:23 - loss: 1.1181 - acc: 0.688 - ETA: 4:22 - loss: 1.1205 - acc: 0.688 - ETA: 4:20 - loss: 1.1206 - acc: 0.688 - ETA: 4:19 - loss: 1.1190 - acc: 0.689 - ETA: 4:18 - loss: 1.1180 - acc: 0.688 - ETA: 4:17 - loss: 1.1120 - acc: 0.690 - ETA: 4:16 - loss: 1.1113 - acc: 0.691 - ETA: 4:15 - loss: 1.1108 - acc: 0.691 - ETA: 4:14 - loss: 1.1156 - acc: 0.690 - ETA: 4:13 - loss: 1.1125 - acc: 0.690 - ETA: 4:12 - loss: 1.1149 - acc: 0.689 - ETA: 4:11 - loss: 1.1159 - acc: 0.689 - ETA: 4:10 - loss: 1.1163 - acc: 0.689 - ETA: 4:09 - loss: 1.1196 - acc: 0.689 - ETA: 4:08 - loss: 1.1198 - acc: 0.688 - ETA: 4:07 - loss: 1.1203 - acc: 0.687 - ETA: 4:06 - loss: 1.1221 - acc: 0.685 - ETA: 4:05 - loss: 1.1269 - acc: 0.683 - ETA: 4:04 - loss: 1.1321 - acc: 0.681 - ETA: 4:03 - loss: 1.1379 - acc: 0.680 - ETA: 4:02 - loss: 1.1420 - acc: 0.679 - ETA: 4:01 - loss: 1.1375 - acc: 0.680 - ETA: 3:59 - loss: 1.1418 - acc: 0.679 - ETA: 3:58 - loss: 1.1377 - acc: 0.681 - ETA: 3:57 - loss: 1.1412 - acc: 0.679 - ETA: 3:57 - loss: 1.1425 - acc: 0.679 - ETA: 3:56 - loss: 1.1444 - acc: 0.679 - ETA: 3:54 - loss: 1.1405 - acc: 0.681 - ETA: 3:53 - loss: 1.1384 - acc: 0.682 - ETA: 3:52 - loss: 1.1400 - acc: 0.683 - ETA: 3:51 - loss: 1.1342 - acc: 0.685 - ETA: 3:50 - loss: 1.1277 - acc: 0.687 - ETA: 3:49 - loss: 1.1331 - acc: 0.686 - ETA: 3:48 - loss: 1.1331 - acc: 0.687 - ETA: 3:47 - loss: 1.1319 - acc: 0.686 - ETA: 3:46 - loss: 1.1343 - acc: 0.686 - ETA: 3:45 - loss: 1.1297 - acc: 0.687 - ETA: 3:44 - loss: 1.1259 - acc: 0.687 - ETA: 3:42 - loss: 1.1282 - acc: 0.686 - ETA: 3:41 - loss: 1.1232 - acc: 0.687 - ETA: 3:41 - loss: 1.1221 - acc: 0.689 - ETA: 3:40 - loss: 1.1204 - acc: 0.689 - ETA: 3:38 - loss: 1.1167 - acc: 0.689 - ETA: 3:37 - loss: 1.1171 - acc: 0.689 - ETA: 3:36 - loss: 1.1159 - acc: 0.690 - ETA: 3:35 - loss: 1.1148 - acc: 0.691 - ETA: 3:34 - loss: 1.1174 - acc: 0.689 - ETA: 3:33 - loss: 1.1175 - acc: 0.689 - ETA: 3:32 - loss: 1.1176 - acc: 0.690 - ETA: 3:31 - loss: 1.1181 - acc: 0.690 - ETA: 3:30 - loss: 1.1148 - acc: 0.691 - ETA: 3:29 - loss: 1.1149 - acc: 0.691 - ETA: 3:28 - loss: 1.1209 - acc: 0.690 - ETA: 3:27 - loss: 1.1216 - acc: 0.690 - ETA: 3:26 - loss: 1.1203 - acc: 0.691 - ETA: 3:25 - loss: 1.1187 - acc: 0.691 - ETA: 3:24 - loss: 1.1187 - acc: 0.691 - ETA: 3:23 - loss: 1.1176 - acc: 0.692 - ETA: 3:22 - loss: 1.1170 - acc: 0.692 - ETA: 3:21 - loss: 1.1159 - acc: 0.692 - ETA: 3:19 - loss: 1.1208 - acc: 0.691 - ETA: 3:18 - loss: 1.1233 - acc: 0.691 - ETA: 3:17 - loss: 1.1246 - acc: 0.691 - ETA: 3:16 - loss: 1.1247 - acc: 0.691 - ETA: 3:15 - loss: 1.1245 - acc: 0.691 - ETA: 3:14 - loss: 1.1208 - acc: 0.692 - ETA: 3:13 - loss: 1.1206 - acc: 0.693 - ETA: 3:12 - loss: 1.1185 - acc: 0.693 - ETA: 3:11 - loss: 1.1207 - acc: 0.693 - ETA: 3:10 - loss: 1.1204 - acc: 0.693 - ETA: 3:09 - loss: 1.1215 - acc: 0.693 - ETA: 3:08 - loss: 1.1205 - acc: 0.693 - ETA: 3:07 - loss: 1.1239 - acc: 0.692 - ETA: 3:06 - loss: 1.1223 - acc: 0.693 - ETA: 3:05 - loss: 1.1250 - acc: 0.692 - ETA: 3:04 - loss: 1.1243 - acc: 0.692 - ETA: 3:03 - loss: 1.1244 - acc: 0.692 - ETA: 3:02 - loss: 1.1242 - acc: 0.693 - ETA: 3:01 - loss: 1.1248 - acc: 0.693 - ETA: 3:00 - loss: 1.1255 - acc: 0.693 - ETA: 2:59 - loss: 1.1281 - acc: 0.693 - ETA: 2:58 - loss: 1.1262 - acc: 0.693 - ETA: 2:56 - loss: 1.1248 - acc: 0.693 - ETA: 2:55 - loss: 1.1253 - acc: 0.692 - ETA: 2:54 - loss: 1.1224 - acc: 0.693 - ETA: 2:53 - loss: 1.1224 - acc: 0.692 - ETA: 2:52 - loss: 1.1246 - acc: 0.692 - ETA: 2:51 - loss: 1.1217 - acc: 0.693 - ETA: 2:50 - loss: 1.1212 - acc: 0.693 - ETA: 2:49 - loss: 1.1212 - acc: 0.693 - ETA: 2:48 - loss: 1.1210 - acc: 0.693 - ETA: 2:47 - loss: 1.1191 - acc: 0.693 - ETA: 2:46 - loss: 1.1205 - acc: 0.694 - ETA: 2:45 - loss: 1.1264 - acc: 0.693 - ETA: 2:44 - loss: 1.1261 - acc: 0.694 - ETA: 2:43 - loss: 1.1286 - acc: 0.693 - ETA: 2:42 - loss: 1.1290 - acc: 0.693 - ETA: 2:41 - loss: 1.1284 - acc: 0.693 - ETA: 2:40 - loss: 1.1287 - acc: 0.693 - ETA: 2:39 - loss: 1.1293 - acc: 0.693 - ETA: 2:38 - loss: 1.1289 - acc: 0.693 - ETA: 2:37 - loss: 1.1284 - acc: 0.694 - ETA: 2:36 - loss: 1.1338 - acc: 0.693 - ETA: 2:35 - loss: 1.1350 - acc: 0.692 - ETA: 2:33 - loss: 1.1318 - acc: 0.693 - ETA: 2:32 - loss: 1.1292 - acc: 0.694 - ETA: 2:31 - loss: 1.1299 - acc: 0.693 - ETA: 2:30 - loss: 1.1276 - acc: 0.693 - ETA: 2:29 - loss: 1.1302 - acc: 0.693 - ETA: 2:28 - loss: 1.1275 - acc: 0.694 - ETA: 2:27 - loss: 1.1290 - acc: 0.694 - ETA: 2:26 - loss: 1.1299 - acc: 0.693 - ETA: 2:25 - loss: 1.1300 - acc: 0.693 - ETA: 2:24 - loss: 1.1331 - acc: 0.692 - ETA: 2:23 - loss: 1.1331 - acc: 0.692 - ETA: 2:22 - loss: 1.1360 - acc: 0.692 - ETA: 2:21 - loss: 1.1342 - acc: 0.692 - ETA: 2:20 - loss: 1.1357 - acc: 0.692 - ETA: 2:19 - loss: 1.1350 - acc: 0.692 - ETA: 2:18 - loss: 1.1358 - acc: 0.691 - ETA: 2:17 - loss: 1.1376 - acc: 0.691 - ETA: 2:16 - loss: 1.1371 - acc: 0.69126660/6680 [============================>.] - ETA: 2:15 - loss: 1.1375 - acc: 0.691 - ETA: 2:13 - loss: 1.1372 - acc: 0.691 - ETA: 2:12 - loss: 1.1374 - acc: 0.691 - ETA: 2:11 - loss: 1.1363 - acc: 0.691 - ETA: 2:10 - loss: 1.1354 - acc: 0.691 - ETA: 2:09 - loss: 1.1348 - acc: 0.691 - ETA: 2:08 - loss: 1.1342 - acc: 0.691 - ETA: 2:07 - loss: 1.1344 - acc: 0.692 - ETA: 2:06 - loss: 1.1336 - acc: 0.692 - ETA: 2:05 - loss: 1.1315 - acc: 0.693 - ETA: 2:04 - loss: 1.1298 - acc: 0.693 - ETA: 2:03 - loss: 1.1300 - acc: 0.693 - ETA: 2:02 - loss: 1.1308 - acc: 0.693 - ETA: 2:01 - loss: 1.1291 - acc: 0.693 - ETA: 2:00 - loss: 1.1304 - acc: 0.693 - ETA: 1:59 - loss: 1.1293 - acc: 0.694 - ETA: 1:58 - loss: 1.1302 - acc: 0.693 - ETA: 1:57 - loss: 1.1326 - acc: 0.693 - ETA: 1:56 - loss: 1.1328 - acc: 0.693 - ETA: 1:55 - loss: 1.1318 - acc: 0.694 - ETA: 1:54 - loss: 1.1331 - acc: 0.694 - ETA: 1:53 - loss: 1.1341 - acc: 0.693 - ETA: 1:51 - loss: 1.1331 - acc: 0.694 - ETA: 1:50 - loss: 1.1325 - acc: 0.694 - ETA: 1:49 - loss: 1.1315 - acc: 0.694 - ETA: 1:48 - loss: 1.1321 - acc: 0.693 - ETA: 1:47 - loss: 1.1332 - acc: 0.693 - ETA: 1:46 - loss: 1.1353 - acc: 0.692 - ETA: 1:45 - loss: 1.1362 - acc: 0.692 - ETA: 1:44 - loss: 1.1346 - acc: 0.692 - ETA: 1:43 - loss: 1.1386 - acc: 0.692 - ETA: 1:42 - loss: 1.1381 - acc: 0.692 - ETA: 1:41 - loss: 1.1389 - acc: 0.692 - ETA: 1:40 - loss: 1.1406 - acc: 0.691 - ETA: 1:39 - loss: 1.1443 - acc: 0.691 - ETA: 1:38 - loss: 1.1442 - acc: 0.690 - ETA: 1:37 - loss: 1.1433 - acc: 0.691 - ETA: 1:36 - loss: 1.1462 - acc: 0.690 - ETA: 1:35 - loss: 1.1455 - acc: 0.690 - ETA: 1:34 - loss: 1.1445 - acc: 0.691 - ETA: 1:33 - loss: 1.1453 - acc: 0.691 - ETA: 1:32 - loss: 1.1463 - acc: 0.690 - ETA: 1:31 - loss: 1.1466 - acc: 0.690 - ETA: 1:30 - loss: 1.1469 - acc: 0.690 - ETA: 1:28 - loss: 1.1448 - acc: 0.691 - ETA: 1:27 - loss: 1.1472 - acc: 0.690 - ETA: 1:26 - loss: 1.1475 - acc: 0.690 - ETA: 1:25 - loss: 1.1474 - acc: 0.690 - ETA: 1:24 - loss: 1.1478 - acc: 0.691 - ETA: 1:23 - loss: 1.1463 - acc: 0.691 - ETA: 1:22 - loss: 1.1476 - acc: 0.691 - ETA: 1:21 - loss: 1.1513 - acc: 0.691 - ETA: 1:20 - loss: 1.1506 - acc: 0.691 - ETA: 1:19 - loss: 1.1514 - acc: 0.691 - ETA: 1:18 - loss: 1.1486 - acc: 0.692 - ETA: 1:17 - loss: 1.1475 - acc: 0.693 - ETA: 1:16 - loss: 1.1456 - acc: 0.693 - ETA: 1:15 - loss: 1.1454 - acc: 0.693 - ETA: 1:14 - loss: 1.1462 - acc: 0.693 - ETA: 1:13 - loss: 1.1452 - acc: 0.693 - ETA: 1:12 - loss: 1.1429 - acc: 0.694 - ETA: 1:11 - loss: 1.1415 - acc: 0.695 - ETA: 1:10 - loss: 1.1429 - acc: 0.694 - ETA: 1:09 - loss: 1.1423 - acc: 0.695 - ETA: 1:08 - loss: 1.1419 - acc: 0.695 - ETA: 1:06 - loss: 1.1433 - acc: 0.694 - ETA: 1:05 - loss: 1.1418 - acc: 0.694 - ETA: 1:04 - loss: 1.1445 - acc: 0.694 - ETA: 1:03 - loss: 1.1458 - acc: 0.693 - ETA: 1:02 - loss: 1.1425 - acc: 0.694 - ETA: 1:01 - loss: 1.1431 - acc: 0.694 - ETA: 1:00 - loss: 1.1445 - acc: 0.694 - ETA: 59s - loss: 1.1438 - acc: 0.694 - ETA: 58s - loss: 1.1464 - acc: 0.69 - ETA: 57s - loss: 1.1467 - acc: 0.69 - ETA: 56s - loss: 1.1478 - acc: 0.69 - ETA: 55s - loss: 1.1470 - acc: 0.69 - ETA: 54s - loss: 1.1466 - acc: 0.69 - ETA: 53s - loss: 1.1457 - acc: 0.69 - ETA: 52s - loss: 1.1474 - acc: 0.69 - ETA: 51s - loss: 1.1492 - acc: 0.69 - ETA: 50s - loss: 1.1494 - acc: 0.69 - ETA: 49s - loss: 1.1484 - acc: 0.69 - ETA: 48s - loss: 1.1492 - acc: 0.69 - ETA: 47s - loss: 1.1487 - acc: 0.69 - ETA: 46s - loss: 1.1491 - acc: 0.69 - ETA: 44s - loss: 1.1465 - acc: 0.69 - ETA: 43s - loss: 1.1469 - acc: 0.69 - ETA: 42s - loss: 1.1488 - acc: 0.69 - ETA: 41s - loss: 1.1495 - acc: 0.69 - ETA: 40s - loss: 1.1476 - acc: 0.69 - ETA: 39s - loss: 1.1495 - acc: 0.69 - ETA: 38s - loss: 1.1506 - acc: 0.69 - ETA: 37s - loss: 1.1506 - acc: 0.69 - ETA: 36s - loss: 1.1529 - acc: 0.69 - ETA: 35s - loss: 1.1529 - acc: 0.69 - ETA: 34s - loss: 1.1532 - acc: 0.69 - ETA: 33s - loss: 1.1525 - acc: 0.69 - ETA: 32s - loss: 1.1533 - acc: 0.69 - ETA: 31s - loss: 1.1526 - acc: 0.69 - ETA: 30s - loss: 1.1540 - acc: 0.69 - ETA: 29s - loss: 1.1540 - acc: 0.69 - ETA: 28s - loss: 1.1547 - acc: 0.69 - ETA: 27s - loss: 1.1543 - acc: 0.69 - ETA: 26s - loss: 1.1542 - acc: 0.69 - ETA: 25s - loss: 1.1561 - acc: 0.69 - ETA: 24s - loss: 1.1560 - acc: 0.69 - ETA: 23s - loss: 1.1583 - acc: 0.69 - ETA: 21s - loss: 1.1564 - acc: 0.69 - ETA: 20s - loss: 1.1547 - acc: 0.69 - ETA: 19s - loss: 1.1576 - acc: 0.69 - ETA: 18s - loss: 1.1567 - acc: 0.69 - ETA: 17s - loss: 1.1563 - acc: 0.69 - ETA: 16s - loss: 1.1565 - acc: 0.69 - ETA: 15s - loss: 1.1567 - acc: 0.69 - ETA: 14s - loss: 1.1573 - acc: 0.69 - ETA: 13s - loss: 1.1569 - acc: 0.69 - ETA: 12s - loss: 1.1557 - acc: 0.69 - ETA: 11s - loss: 1.1565 - acc: 0.69 - ETA: 10s - loss: 1.1559 - acc: 0.69 - ETA: 9s - loss: 1.1560 - acc: 0.6945 - ETA: 8s - loss: 1.1570 - acc: 0.694 - ETA: 7s - loss: 1.1561 - acc: 0.694 - ETA: 6s - loss: 1.1573 - acc: 0.694 - ETA: 5s - loss: 1.1565 - acc: 0.694 - ETA: 4s - loss: 1.1575 - acc: 0.694 - ETA: 3s - loss: 1.1562 - acc: 0.694 - ETA: 2s - loss: 1.1559 - acc: 0.694 - ETA: 1s - loss: 1.1543 - acc: 0.6949Epoch 00007: val\_loss did not improve
6680/6680 [==============================] - 361s 54ms/step - loss: 1.1555 - acc: 0.6945 - val\_loss: 5.3512 - val\_acc: 0.0946
Epoch 8/8
4080/6680 [=================>{\ldots}] - ETA: 5:53 - loss: 0.6008 - acc: 0.800 - ETA: 5:53 - loss: 0.4967 - acc: 0.875 - ETA: 5:53 - loss: 0.5402 - acc: 0.833 - ETA: 5:52 - loss: 0.6441 - acc: 0.825 - ETA: 5:50 - loss: 0.6464 - acc: 0.810 - ETA: 5:48 - loss: 0.7020 - acc: 0.791 - ETA: 5:49 - loss: 0.6805 - acc: 0.792 - ETA: 5:52 - loss: 0.6671 - acc: 0.787 - ETA: 5:51 - loss: 0.7237 - acc: 0.766 - ETA: 5:48 - loss: 0.7195 - acc: 0.770 - ETA: 5:46 - loss: 0.6862 - acc: 0.786 - ETA: 5:44 - loss: 0.6784 - acc: 0.795 - ETA: 5:42 - loss: 0.6698 - acc: 0.792 - ETA: 5:41 - loss: 0.6648 - acc: 0.796 - ETA: 5:39 - loss: 0.6326 - acc: 0.806 - ETA: 5:38 - loss: 0.6164 - acc: 0.812 - ETA: 5:36 - loss: 0.5956 - acc: 0.820 - ETA: 5:35 - loss: 0.5838 - acc: 0.827 - ETA: 5:33 - loss: 0.5680 - acc: 0.834 - ETA: 5:32 - loss: 0.5643 - acc: 0.832 - ETA: 5:31 - loss: 0.5537 - acc: 0.838 - ETA: 5:30 - loss: 0.5638 - acc: 0.836 - ETA: 5:30 - loss: 0.5583 - acc: 0.834 - ETA: 5:30 - loss: 0.5697 - acc: 0.831 - ETA: 5:28 - loss: 0.5941 - acc: 0.830 - ETA: 5:27 - loss: 0.5902 - acc: 0.828 - ETA: 5:25 - loss: 0.6031 - acc: 0.827 - ETA: 5:24 - loss: 0.6049 - acc: 0.828 - ETA: 5:22 - loss: 0.6105 - acc: 0.831 - ETA: 5:21 - loss: 0.6027 - acc: 0.831 - ETA: 5:20 - loss: 0.6137 - acc: 0.830 - ETA: 5:19 - loss: 0.6267 - acc: 0.829 - ETA: 5:18 - loss: 0.6331 - acc: 0.828 - ETA: 5:16 - loss: 0.6364 - acc: 0.825 - ETA: 5:15 - loss: 0.6398 - acc: 0.822 - ETA: 5:14 - loss: 0.6429 - acc: 0.820 - ETA: 5:13 - loss: 0.6377 - acc: 0.821 - ETA: 5:14 - loss: 0.6402 - acc: 0.818 - ETA: 5:13 - loss: 0.6410 - acc: 0.820 - ETA: 5:12 - loss: 0.6499 - acc: 0.817 - ETA: 5:11 - loss: 0.6576 - acc: 0.817 - ETA: 5:10 - loss: 0.6661 - acc: 0.814 - ETA: 5:09 - loss: 0.6617 - acc: 0.814 - ETA: 5:08 - loss: 0.6626 - acc: 0.814 - ETA: 5:07 - loss: 0.6568 - acc: 0.816 - ETA: 5:06 - loss: 0.6559 - acc: 0.817 - ETA: 5:05 - loss: 0.6570 - acc: 0.817 - ETA: 5:04 - loss: 0.6543 - acc: 0.817 - ETA: 5:02 - loss: 0.6544 - acc: 0.817 - ETA: 5:01 - loss: 0.6519 - acc: 0.818 - ETA: 5:00 - loss: 0.6567 - acc: 0.816 - ETA: 4:59 - loss: 0.6534 - acc: 0.817 - ETA: 4:59 - loss: 0.6564 - acc: 0.817 - ETA: 4:57 - loss: 0.6515 - acc: 0.818 - ETA: 4:56 - loss: 0.6455 - acc: 0.820 - ETA: 4:55 - loss: 0.6504 - acc: 0.820 - ETA: 4:54 - loss: 0.6478 - acc: 0.821 - ETA: 4:53 - loss: 0.6588 - acc: 0.819 - ETA: 4:52 - loss: 0.6669 - acc: 0.818 - ETA: 4:51 - loss: 0.6658 - acc: 0.819 - ETA: 4:50 - loss: 0.6621 - acc: 0.819 - ETA: 4:48 - loss: 0.6655 - acc: 0.818 - ETA: 4:47 - loss: 0.6706 - acc: 0.815 - ETA: 4:46 - loss: 0.6662 - acc: 0.817 - ETA: 4:45 - loss: 0.6645 - acc: 0.818 - ETA: 4:44 - loss: 0.6704 - acc: 0.816 - ETA: 4:43 - loss: 0.6697 - acc: 0.816 - ETA: 4:42 - loss: 0.6625 - acc: 0.818 - ETA: 4:41 - loss: 0.6634 - acc: 0.818 - ETA: 4:40 - loss: 0.6562 - acc: 0.821 - ETA: 4:39 - loss: 0.6588 - acc: 0.821 - ETA: 4:38 - loss: 0.6560 - acc: 0.820 - ETA: 4:37 - loss: 0.6598 - acc: 0.819 - ETA: 4:36 - loss: 0.6604 - acc: 0.818 - ETA: 4:34 - loss: 0.6664 - acc: 0.816 - ETA: 4:33 - loss: 0.6698 - acc: 0.814 - ETA: 4:32 - loss: 0.6753 - acc: 0.813 - ETA: 4:31 - loss: 0.6741 - acc: 0.814 - ETA: 4:30 - loss: 0.6714 - acc: 0.815 - ETA: 4:29 - loss: 0.6695 - acc: 0.815 - ETA: 4:27 - loss: 0.6753 - acc: 0.814 - ETA: 4:26 - loss: 0.6720 - acc: 0.815 - ETA: 4:26 - loss: 0.6723 - acc: 0.815 - ETA: 4:25 - loss: 0.6694 - acc: 0.816 - ETA: 4:24 - loss: 0.6715 - acc: 0.816 - ETA: 4:23 - loss: 0.6695 - acc: 0.818 - ETA: 4:22 - loss: 0.6758 - acc: 0.818 - ETA: 4:20 - loss: 0.6785 - acc: 0.817 - ETA: 4:19 - loss: 0.6824 - acc: 0.815 - ETA: 4:18 - loss: 0.6844 - acc: 0.815 - ETA: 4:17 - loss: 0.6819 - acc: 0.815 - ETA: 4:16 - loss: 0.6773 - acc: 0.816 - ETA: 4:15 - loss: 0.6778 - acc: 0.816 - ETA: 4:14 - loss: 0.6774 - acc: 0.816 - ETA: 4:12 - loss: 0.6788 - acc: 0.815 - ETA: 4:11 - loss: 0.6783 - acc: 0.816 - ETA: 4:10 - loss: 0.6846 - acc: 0.815 - ETA: 4:09 - loss: 0.6817 - acc: 0.816 - ETA: 4:08 - loss: 0.6789 - acc: 0.817 - ETA: 4:07 - loss: 0.6801 - acc: 0.816 - ETA: 4:06 - loss: 0.6850 - acc: 0.816 - ETA: 4:05 - loss: 0.6826 - acc: 0.816 - ETA: 4:04 - loss: 0.6830 - acc: 0.816 - ETA: 4:03 - loss: 0.6841 - acc: 0.814 - ETA: 4:02 - loss: 0.6884 - acc: 0.814 - ETA: 4:01 - loss: 0.6865 - acc: 0.815 - ETA: 3:59 - loss: 0.6851 - acc: 0.815 - ETA: 3:58 - loss: 0.6827 - acc: 0.816 - ETA: 3:57 - loss: 0.6819 - acc: 0.816 - ETA: 3:56 - loss: 0.6828 - acc: 0.815 - ETA: 3:55 - loss: 0.6842 - acc: 0.814 - ETA: 3:54 - loss: 0.6819 - acc: 0.815 - ETA: 3:53 - loss: 0.6809 - acc: 0.815 - ETA: 3:52 - loss: 0.6793 - acc: 0.815 - ETA: 3:51 - loss: 0.6776 - acc: 0.815 - ETA: 3:50 - loss: 0.6806 - acc: 0.815 - ETA: 3:49 - loss: 0.6824 - acc: 0.814 - ETA: 3:48 - loss: 0.6836 - acc: 0.814 - ETA: 3:47 - loss: 0.6830 - acc: 0.814 - ETA: 3:46 - loss: 0.6851 - acc: 0.814 - ETA: 3:45 - loss: 0.6872 - acc: 0.813 - ETA: 3:44 - loss: 0.6878 - acc: 0.813 - ETA: 3:43 - loss: 0.6859 - acc: 0.814 - ETA: 3:41 - loss: 0.6850 - acc: 0.814 - ETA: 3:40 - loss: 0.6855 - acc: 0.814 - ETA: 3:39 - loss: 0.6856 - acc: 0.814 - ETA: 3:38 - loss: 0.6855 - acc: 0.813 - ETA: 3:37 - loss: 0.6826 - acc: 0.814 - ETA: 3:36 - loss: 0.6801 - acc: 0.815 - ETA: 3:35 - loss: 0.6771 - acc: 0.815 - ETA: 3:34 - loss: 0.6733 - acc: 0.817 - ETA: 3:33 - loss: 0.6758 - acc: 0.817 - ETA: 3:32 - loss: 0.6761 - acc: 0.816 - ETA: 3:31 - loss: 0.6744 - acc: 0.817 - ETA: 3:30 - loss: 0.6744 - acc: 0.817 - ETA: 3:29 - loss: 0.6794 - acc: 0.817 - ETA: 3:28 - loss: 0.6789 - acc: 0.816 - ETA: 3:27 - loss: 0.6784 - acc: 0.817 - ETA: 3:25 - loss: 0.6820 - acc: 0.816 - ETA: 3:24 - loss: 0.6859 - acc: 0.815 - ETA: 3:23 - loss: 0.6873 - acc: 0.815 - ETA: 3:22 - loss: 0.6897 - acc: 0.815 - ETA: 3:21 - loss: 0.6875 - acc: 0.816 - ETA: 3:20 - loss: 0.6842 - acc: 0.817 - ETA: 3:19 - loss: 0.6858 - acc: 0.817 - ETA: 3:18 - loss: 0.6843 - acc: 0.817 - ETA: 3:17 - loss: 0.6859 - acc: 0.817 - ETA: 3:16 - loss: 0.6866 - acc: 0.817 - ETA: 3:15 - loss: 0.6879 - acc: 0.816 - ETA: 3:14 - loss: 0.6869 - acc: 0.817 - ETA: 3:13 - loss: 0.6853 - acc: 0.817 - ETA: 3:12 - loss: 0.6856 - acc: 0.817 - ETA: 3:11 - loss: 0.6852 - acc: 0.816 - ETA: 3:10 - loss: 0.6816 - acc: 0.817 - ETA: 3:09 - loss: 0.6827 - acc: 0.816 - ETA: 3:08 - loss: 0.6812 - acc: 0.817 - ETA: 3:06 - loss: 0.6816 - acc: 0.816 - ETA: 3:05 - loss: 0.6820 - acc: 0.816 - ETA: 3:04 - loss: 0.6880 - acc: 0.815 - ETA: 3:03 - loss: 0.6864 - acc: 0.815 - ETA: 3:02 - loss: 0.6952 - acc: 0.814 - ETA: 3:01 - loss: 0.6970 - acc: 0.814 - ETA: 3:00 - loss: 0.6977 - acc: 0.814 - ETA: 2:59 - loss: 0.6966 - acc: 0.814 - ETA: 2:58 - loss: 0.6997 - acc: 0.814 - ETA: 2:57 - loss: 0.7033 - acc: 0.813 - ETA: 2:56 - loss: 0.7006 - acc: 0.814 - ETA: 2:55 - loss: 0.6998 - acc: 0.814 - ETA: 2:54 - loss: 0.7034 - acc: 0.813 - ETA: 2:53 - loss: 0.7021 - acc: 0.813 - ETA: 2:52 - loss: 0.7029 - acc: 0.812 - ETA: 2:51 - loss: 0.7069 - acc: 0.812 - ETA: 2:49 - loss: 0.7054 - acc: 0.812 - ETA: 2:48 - loss: 0.7078 - acc: 0.812 - ETA: 2:48 - loss: 0.7055 - acc: 0.813 - ETA: 2:46 - loss: 0.7058 - acc: 0.813 - ETA: 2:45 - loss: 0.7067 - acc: 0.813 - ETA: 2:44 - loss: 0.7073 - acc: 0.813 - ETA: 2:43 - loss: 0.7066 - acc: 0.813 - ETA: 2:42 - loss: 0.7085 - acc: 0.813 - ETA: 2:41 - loss: 0.7100 - acc: 0.813 - ETA: 2:40 - loss: 0.7097 - acc: 0.812 - ETA: 2:39 - loss: 0.7068 - acc: 0.813 - ETA: 2:38 - loss: 0.7073 - acc: 0.813 - ETA: 2:37 - loss: 0.7063 - acc: 0.814 - ETA: 2:36 - loss: 0.7092 - acc: 0.813 - ETA: 2:35 - loss: 0.7110 - acc: 0.813 - ETA: 2:34 - loss: 0.7125 - acc: 0.813 - ETA: 2:33 - loss: 0.7119 - acc: 0.814 - ETA: 2:32 - loss: 0.7119 - acc: 0.813 - ETA: 2:31 - loss: 0.7128 - acc: 0.813 - ETA: 2:29 - loss: 0.7119 - acc: 0.813 - ETA: 2:28 - loss: 0.7148 - acc: 0.813 - ETA: 2:27 - loss: 0.7167 - acc: 0.812 - ETA: 2:26 - loss: 0.7179 - acc: 0.811 - ETA: 2:25 - loss: 0.7210 - acc: 0.811 - ETA: 2:24 - loss: 0.7217 - acc: 0.811 - ETA: 2:23 - loss: 0.7218 - acc: 0.811 - ETA: 2:22 - loss: 0.7242 - acc: 0.812 - ETA: 2:21 - loss: 0.7252 - acc: 0.812 - ETA: 2:20 - loss: 0.7260 - acc: 0.811 - ETA: 2:19 - loss: 0.7271 - acc: 0.811 - ETA: 2:18 - loss: 0.7264 - acc: 0.811 - ETA: 2:17 - loss: 0.7252 - acc: 0.81206660/6680 [============================>.] - ETA: 2:16 - loss: 0.7253 - acc: 0.811 - ETA: 2:15 - loss: 0.7244 - acc: 0.812 - ETA: 2:14 - loss: 0.7247 - acc: 0.812 - ETA: 2:13 - loss: 0.7270 - acc: 0.811 - ETA: 2:11 - loss: 0.7278 - acc: 0.811 - ETA: 2:10 - loss: 0.7310 - acc: 0.810 - ETA: 2:09 - loss: 0.7353 - acc: 0.810 - ETA: 2:08 - loss: 0.7342 - acc: 0.810 - ETA: 2:07 - loss: 0.7351 - acc: 0.809 - ETA: 2:06 - loss: 0.7360 - acc: 0.809 - ETA: 2:05 - loss: 0.7350 - acc: 0.809 - ETA: 2:04 - loss: 0.7349 - acc: 0.809 - ETA: 2:03 - loss: 0.7348 - acc: 0.809 - ETA: 2:02 - loss: 0.7339 - acc: 0.809 - ETA: 2:01 - loss: 0.7348 - acc: 0.809 - ETA: 2:00 - loss: 0.7389 - acc: 0.808 - ETA: 1:59 - loss: 0.7385 - acc: 0.808 - ETA: 1:58 - loss: 0.7394 - acc: 0.809 - ETA: 1:57 - loss: 0.7418 - acc: 0.808 - ETA: 1:56 - loss: 0.7415 - acc: 0.808 - ETA: 1:54 - loss: 0.7417 - acc: 0.808 - ETA: 1:53 - loss: 0.7417 - acc: 0.808 - ETA: 1:52 - loss: 0.7413 - acc: 0.807 - ETA: 1:51 - loss: 0.7403 - acc: 0.807 - ETA: 1:50 - loss: 0.7418 - acc: 0.807 - ETA: 1:49 - loss: 0.7456 - acc: 0.806 - ETA: 1:48 - loss: 0.7450 - acc: 0.806 - ETA: 1:47 - loss: 0.7445 - acc: 0.806 - ETA: 1:46 - loss: 0.7441 - acc: 0.806 - ETA: 1:45 - loss: 0.7444 - acc: 0.806 - ETA: 1:44 - loss: 0.7472 - acc: 0.806 - ETA: 1:43 - loss: 0.7480 - acc: 0.805 - ETA: 1:42 - loss: 0.7464 - acc: 0.806 - ETA: 1:41 - loss: 0.7472 - acc: 0.805 - ETA: 1:40 - loss: 0.7466 - acc: 0.806 - ETA: 1:39 - loss: 0.7452 - acc: 0.806 - ETA: 1:38 - loss: 0.7487 - acc: 0.805 - ETA: 1:36 - loss: 0.7486 - acc: 0.805 - ETA: 1:35 - loss: 0.7480 - acc: 0.805 - ETA: 1:34 - loss: 0.7498 - acc: 0.804 - ETA: 1:33 - loss: 0.7515 - acc: 0.804 - ETA: 1:32 - loss: 0.7505 - acc: 0.804 - ETA: 1:31 - loss: 0.7510 - acc: 0.804 - ETA: 1:30 - loss: 0.7510 - acc: 0.804 - ETA: 1:29 - loss: 0.7514 - acc: 0.803 - ETA: 1:28 - loss: 0.7506 - acc: 0.804 - ETA: 1:27 - loss: 0.7498 - acc: 0.804 - ETA: 1:26 - loss: 0.7521 - acc: 0.803 - ETA: 1:25 - loss: 0.7519 - acc: 0.803 - ETA: 1:24 - loss: 0.7512 - acc: 0.803 - ETA: 1:23 - loss: 0.7537 - acc: 0.802 - ETA: 1:22 - loss: 0.7531 - acc: 0.803 - ETA: 1:21 - loss: 0.7517 - acc: 0.803 - ETA: 1:20 - loss: 0.7516 - acc: 0.803 - ETA: 1:19 - loss: 0.7508 - acc: 0.804 - ETA: 1:17 - loss: 0.7504 - acc: 0.803 - ETA: 1:16 - loss: 0.7499 - acc: 0.803 - ETA: 1:15 - loss: 0.7494 - acc: 0.804 - ETA: 1:14 - loss: 0.7518 - acc: 0.803 - ETA: 1:13 - loss: 0.7530 - acc: 0.803 - ETA: 1:12 - loss: 0.7530 - acc: 0.803 - ETA: 1:11 - loss: 0.7530 - acc: 0.803 - ETA: 1:10 - loss: 0.7528 - acc: 0.803 - ETA: 1:09 - loss: 0.7519 - acc: 0.803 - ETA: 1:08 - loss: 0.7556 - acc: 0.802 - ETA: 1:07 - loss: 0.7557 - acc: 0.802 - ETA: 1:06 - loss: 0.7561 - acc: 0.802 - ETA: 1:05 - loss: 0.7548 - acc: 0.802 - ETA: 1:04 - loss: 0.7573 - acc: 0.802 - ETA: 1:03 - loss: 0.7571 - acc: 0.802 - ETA: 1:02 - loss: 0.7578 - acc: 0.802 - ETA: 1:01 - loss: 0.7576 - acc: 0.802 - ETA: 1:00 - loss: 0.7583 - acc: 0.801 - ETA: 59s - loss: 0.7598 - acc: 0.802 - ETA: 57s - loss: 0.7618 - acc: 0.80 - ETA: 56s - loss: 0.7608 - acc: 0.80 - ETA: 55s - loss: 0.7635 - acc: 0.80 - ETA: 54s - loss: 0.7635 - acc: 0.80 - ETA: 53s - loss: 0.7633 - acc: 0.80 - ETA: 52s - loss: 0.7620 - acc: 0.80 - ETA: 51s - loss: 0.7623 - acc: 0.80 - ETA: 50s - loss: 0.7628 - acc: 0.80 - ETA: 49s - loss: 0.7638 - acc: 0.80 - ETA: 48s - loss: 0.7631 - acc: 0.80 - ETA: 47s - loss: 0.7651 - acc: 0.80 - ETA: 46s - loss: 0.7660 - acc: 0.80 - ETA: 45s - loss: 0.7669 - acc: 0.80 - ETA: 44s - loss: 0.7662 - acc: 0.80 - ETA: 43s - loss: 0.7657 - acc: 0.80 - ETA: 42s - loss: 0.7647 - acc: 0.80 - ETA: 41s - loss: 0.7636 - acc: 0.80 - ETA: 40s - loss: 0.7644 - acc: 0.80 - ETA: 39s - loss: 0.7652 - acc: 0.80 - ETA: 37s - loss: 0.7658 - acc: 0.79 - ETA: 36s - loss: 0.7651 - acc: 0.80 - ETA: 35s - loss: 0.7654 - acc: 0.80 - ETA: 34s - loss: 0.7678 - acc: 0.79 - ETA: 33s - loss: 0.7683 - acc: 0.79 - ETA: 32s - loss: 0.7690 - acc: 0.79 - ETA: 31s - loss: 0.7681 - acc: 0.79 - ETA: 30s - loss: 0.7667 - acc: 0.80 - ETA: 29s - loss: 0.7658 - acc: 0.80 - ETA: 28s - loss: 0.7650 - acc: 0.80 - ETA: 27s - loss: 0.7647 - acc: 0.80 - ETA: 26s - loss: 0.7651 - acc: 0.80 - ETA: 25s - loss: 0.7678 - acc: 0.79 - ETA: 24s - loss: 0.7675 - acc: 0.79 - ETA: 23s - loss: 0.7668 - acc: 0.80 - ETA: 22s - loss: 0.7678 - acc: 0.79 - ETA: 21s - loss: 0.7692 - acc: 0.79 - ETA: 20s - loss: 0.7681 - acc: 0.80 - ETA: 18s - loss: 0.7677 - acc: 0.80 - ETA: 17s - loss: 0.7663 - acc: 0.80 - ETA: 16s - loss: 0.7658 - acc: 0.80 - ETA: 15s - loss: 0.7656 - acc: 0.80 - ETA: 14s - loss: 0.7651 - acc: 0.80 - ETA: 13s - loss: 0.7678 - acc: 0.80 - ETA: 12s - loss: 0.7678 - acc: 0.80 - ETA: 11s - loss: 0.7682 - acc: 0.79 - ETA: 10s - loss: 0.7685 - acc: 0.79 - ETA: 9s - loss: 0.7675 - acc: 0.7998 - ETA: 8s - loss: 0.7665 - acc: 0.800 - ETA: 7s - loss: 0.7691 - acc: 0.799 - ETA: 6s - loss: 0.7698 - acc: 0.799 - ETA: 5s - loss: 0.7713 - acc: 0.798 - ETA: 4s - loss: 0.7725 - acc: 0.798 - ETA: 3s - loss: 0.7743 - acc: 0.798 - ETA: 2s - loss: 0.7764 - acc: 0.797 - ETA: 1s - loss: 0.7760 - acc: 0.7977Epoch 00008: val\_loss did not improve
6680/6680 [==============================] - 363s 54ms/step - loss: 0.7756 - acc: 0.7981 - val\_loss: 5.5194 - val\_acc: 0.0838

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} <keras.callbacks.History at 0x242b1df89b0>
\end{Verbatim}
            
    \subsubsection{Load the Model with the Best Validation
Loss}\label{load-the-model-with-the-best-validation-loss}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.from\PYZus{}scratch.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Test the Model}\label{test-the-model}

Try out your model on the test dataset of dog images. Ensure that your
test accuracy is greater than 1\%.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{c+c1}{\PYZsh{} get index of predicted dog breed for each image in test set}
         \PY{n}{dog\PYZus{}breed\PYZus{}predictions} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{tensor}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{tensor} \PY{o+ow}{in} \PY{n}{test\PYZus{}tensors}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} report test accuracy}
         \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{dog\PYZus{}breed\PYZus{}predictions}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{dog\PYZus{}breed\PYZus{}predictions}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test accuracy: 11.4833\%

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 4: Use a CNN to Classify Dog Breeds

To reduce training time without sacrificing accuracy, we show you how to
train a CNN using transfer learning. In the following step, you will get
a chance to use transfer learning to train your own CNN.

\subsubsection{Obtain Bottleneck
Features}\label{obtain-bottleneck-features}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{bottleneck\PYZus{}features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bottleneck\PYZus{}features/DogVGG16Data.npz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{train\PYZus{}VGG16} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{valid\PYZus{}VGG16} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{test\PYZus{}VGG16} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \subsubsection{Model Architecture}\label{model-architecture}

The model uses the the pre-trained VGG-16 model as a fixed feature
extractor, where the last convolutional output of VGG-16 is fed as input
to our model. We only add a global average pooling layer and a fully
connected layer, where the latter contains one node for each dog
category and is equipped with a softmax.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{VGG16\PYZus{}model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{train\PYZus{}VGG16}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
global\_average\_pooling2d\_1 ( (None, 512)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_3 (Dense)              (None, 133)               68229     
=================================================================
Total params: 68,229
Trainable params: 68,229
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \subsubsection{Compile the Model}\label{compile-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rmsprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Train the Model}\label{train-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.VGG16.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                        \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}VGG16}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} 
                   \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{valid\PYZus{}VGG16}\PY{p}{,} \PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{,}
                   \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/20
6560/6680 [============================>.] - ETA: 2:21 - loss: 15.5751 - acc: 0.0000e+ - ETA: 17s - loss: 14.7440 - acc: 0.0056     - ETA: 9s - loss: 14.2843 - acc: 0.036 - ETA: 6s - loss: 14.3401 - acc: 0.03 - ETA: 5s - loss: 14.2108 - acc: 0.03 - ETA: 4s - loss: 14.0623 - acc: 0.03 - ETA: 3s - loss: 13.9093 - acc: 0.03 - ETA: 3s - loss: 13.7663 - acc: 0.03 - ETA: 3s - loss: 13.5451 - acc: 0.04 - ETA: 2s - loss: 13.3860 - acc: 0.04 - ETA: 2s - loss: 13.2664 - acc: 0.05 - ETA: 2s - loss: 13.0881 - acc: 0.05 - ETA: 2s - loss: 13.0488 - acc: 0.06 - ETA: 2s - loss: 12.9213 - acc: 0.06 - ETA: 1s - loss: 12.7655 - acc: 0.07 - ETA: 1s - loss: 12.6626 - acc: 0.07 - ETA: 1s - loss: 12.5752 - acc: 0.07 - ETA: 1s - loss: 12.4983 - acc: 0.08 - ETA: 1s - loss: 12.3963 - acc: 0.08 - ETA: 1s - loss: 12.2541 - acc: 0.08 - ETA: 1s - loss: 12.2082 - acc: 0.09 - ETA: 1s - loss: 12.1339 - acc: 0.09 - ETA: 1s - loss: 12.0725 - acc: 0.09 - ETA: 1s - loss: 11.9798 - acc: 0.10 - ETA: 0s - loss: 11.9030 - acc: 0.10 - ETA: 0s - loss: 11.8071 - acc: 0.11 - ETA: 0s - loss: 11.7457 - acc: 0.11 - ETA: 0s - loss: 11.6885 - acc: 0.11 - ETA: 0s - loss: 11.6287 - acc: 0.12 - ETA: 0s - loss: 11.5719 - acc: 0.12 - ETA: 0s - loss: 11.5195 - acc: 0.12 - ETA: 0s - loss: 11.4311 - acc: 0.13 - ETA: 0s - loss: 11.3690 - acc: 0.13 - ETA: 0s - loss: 11.3165 - acc: 0.13 - ETA: 0s - loss: 11.2642 - acc: 0.14 - ETA: 0s - loss: 11.2182 - acc: 0.14 - ETA: 0s - loss: 11.1643 - acc: 0.14 - ETA: 0s - loss: 11.1179 - acc: 0.1508Epoch 00001: val\_loss improved from inf to 9.30728, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 3s 401us/step - loss: 11.0818 - acc: 0.1531 - val\_loss: 9.3073 - val\_acc: 0.2814
Epoch 2/20
6580/6680 [============================>.] - ETA: 2s - loss: 12.3017 - acc: 0.20 - ETA: 1s - loss: 8.5567 - acc: 0.3727 - ETA: 1s - loss: 8.2196 - acc: 0.371 - ETA: 1s - loss: 8.3411 - acc: 0.358 - ETA: 1s - loss: 8.3040 - acc: 0.361 - ETA: 1s - loss: 8.3826 - acc: 0.353 - ETA: 1s - loss: 8.4168 - acc: 0.350 - ETA: 1s - loss: 8.4070 - acc: 0.348 - ETA: 1s - loss: 8.5106 - acc: 0.343 - ETA: 1s - loss: 8.3882 - acc: 0.355 - ETA: 1s - loss: 8.4433 - acc: 0.354 - ETA: 1s - loss: 8.4724 - acc: 0.350 - ETA: 1s - loss: 8.5248 - acc: 0.347 - ETA: 1s - loss: 8.5319 - acc: 0.346 - ETA: 1s - loss: 8.5352 - acc: 0.345 - ETA: 1s - loss: 8.5298 - acc: 0.344 - ETA: 1s - loss: 8.5576 - acc: 0.342 - ETA: 1s - loss: 8.5307 - acc: 0.344 - ETA: 0s - loss: 8.5957 - acc: 0.342 - ETA: 0s - loss: 8.5841 - acc: 0.344 - ETA: 0s - loss: 8.5815 - acc: 0.344 - ETA: 0s - loss: 8.5471 - acc: 0.346 - ETA: 0s - loss: 8.5140 - acc: 0.349 - ETA: 0s - loss: 8.4929 - acc: 0.350 - ETA: 0s - loss: 8.4818 - acc: 0.352 - ETA: 0s - loss: 8.4547 - acc: 0.354 - ETA: 0s - loss: 8.4473 - acc: 0.352 - ETA: 0s - loss: 8.4450 - acc: 0.352 - ETA: 0s - loss: 8.4574 - acc: 0.351 - ETA: 0s - loss: 8.4620 - acc: 0.351 - ETA: 0s - loss: 8.4461 - acc: 0.352 - ETA: 0s - loss: 8.4472 - acc: 0.352 - ETA: 0s - loss: 8.4595 - acc: 0.352 - ETA: 0s - loss: 8.4668 - acc: 0.352 - ETA: 0s - loss: 8.4463 - acc: 0.353 - ETA: 0s - loss: 8.4606 - acc: 0.353 - ETA: 0s - loss: 8.4654 - acc: 0.3529Epoch 00002: val\_loss improved from 9.30728 to 8.28147, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 305us/step - loss: 8.4780 - acc: 0.3522 - val\_loss: 8.2815 - val\_acc: 0.3665
Epoch 3/20
6660/6680 [============================>.] - ETA: 2s - loss: 10.4878 - acc: 0.25 - ETA: 1s - loss: 8.2760 - acc: 0.4182 - ETA: 1s - loss: 7.8598 - acc: 0.445 - ETA: 1s - loss: 8.1612 - acc: 0.417 - ETA: 1s - loss: 8.1520 - acc: 0.417 - ETA: 1s - loss: 7.9674 - acc: 0.432 - ETA: 1s - loss: 7.9063 - acc: 0.435 - ETA: 1s - loss: 7.9259 - acc: 0.437 - ETA: 1s - loss: 7.9703 - acc: 0.430 - ETA: 1s - loss: 8.1274 - acc: 0.420 - ETA: 1s - loss: 8.0495 - acc: 0.425 - ETA: 1s - loss: 8.0638 - acc: 0.421 - ETA: 1s - loss: 8.0439 - acc: 0.420 - ETA: 1s - loss: 8.0373 - acc: 0.421 - ETA: 1s - loss: 8.0498 - acc: 0.420 - ETA: 1s - loss: 7.9719 - acc: 0.423 - ETA: 1s - loss: 7.9000 - acc: 0.429 - ETA: 1s - loss: 7.8999 - acc: 0.428 - ETA: 1s - loss: 7.8890 - acc: 0.427 - ETA: 0s - loss: 7.8564 - acc: 0.430 - ETA: 0s - loss: 7.9108 - acc: 0.429 - ETA: 0s - loss: 7.9457 - acc: 0.427 - ETA: 0s - loss: 7.9331 - acc: 0.428 - ETA: 0s - loss: 7.9477 - acc: 0.427 - ETA: 0s - loss: 7.9184 - acc: 0.428 - ETA: 0s - loss: 7.9185 - acc: 0.428 - ETA: 0s - loss: 7.9283 - acc: 0.427 - ETA: 0s - loss: 7.9115 - acc: 0.428 - ETA: 0s - loss: 7.8797 - acc: 0.430 - ETA: 0s - loss: 7.8815 - acc: 0.430 - ETA: 0s - loss: 7.8805 - acc: 0.431 - ETA: 0s - loss: 7.8692 - acc: 0.433 - ETA: 0s - loss: 7.8693 - acc: 0.432 - ETA: 0s - loss: 7.8833 - acc: 0.430 - ETA: 0s - loss: 7.8793 - acc: 0.431 - ETA: 0s - loss: 7.8830 - acc: 0.431 - ETA: 0s - loss: 7.8896 - acc: 0.430 - ETA: 0s - loss: 7.8771 - acc: 0.4326Epoch 00003: val\_loss improved from 8.28147 to 8.00214, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 316us/step - loss: 7.8730 - acc: 0.4331 - val\_loss: 8.0021 - val\_acc: 0.4108
Epoch 4/20
6660/6680 [============================>.] - ETA: 2s - loss: 10.2346 - acc: 0.30 - ETA: 1s - loss: 7.9086 - acc: 0.4350 - ETA: 1s - loss: 7.6358 - acc: 0.452 - ETA: 1s - loss: 7.7019 - acc: 0.456 - ETA: 1s - loss: 7.7241 - acc: 0.461 - ETA: 1s - loss: 7.6322 - acc: 0.466 - ETA: 1s - loss: 7.9440 - acc: 0.446 - ETA: 1s - loss: 7.9563 - acc: 0.446 - ETA: 1s - loss: 7.8990 - acc: 0.451 - ETA: 1s - loss: 7.8082 - acc: 0.454 - ETA: 1s - loss: 7.7788 - acc: 0.457 - ETA: 1s - loss: 7.7310 - acc: 0.459 - ETA: 1s - loss: 7.6337 - acc: 0.465 - ETA: 1s - loss: 7.7070 - acc: 0.461 - ETA: 1s - loss: 7.7278 - acc: 0.459 - ETA: 1s - loss: 7.7361 - acc: 0.460 - ETA: 1s - loss: 7.7184 - acc: 0.460 - ETA: 1s - loss: 7.6844 - acc: 0.462 - ETA: 0s - loss: 7.6776 - acc: 0.462 - ETA: 0s - loss: 7.6832 - acc: 0.463 - ETA: 0s - loss: 7.6852 - acc: 0.463 - ETA: 0s - loss: 7.6396 - acc: 0.466 - ETA: 0s - loss: 7.6266 - acc: 0.466 - ETA: 0s - loss: 7.5796 - acc: 0.467 - ETA: 0s - loss: 7.5733 - acc: 0.466 - ETA: 0s - loss: 7.5843 - acc: 0.465 - ETA: 0s - loss: 7.5564 - acc: 0.467 - ETA: 0s - loss: 7.5361 - acc: 0.469 - ETA: 0s - loss: 7.5424 - acc: 0.468 - ETA: 0s - loss: 7.5531 - acc: 0.467 - ETA: 0s - loss: 7.5607 - acc: 0.467 - ETA: 0s - loss: 7.5484 - acc: 0.468 - ETA: 0s - loss: 7.5317 - acc: 0.469 - ETA: 0s - loss: 7.5310 - acc: 0.468 - ETA: 0s - loss: 7.5488 - acc: 0.467 - ETA: 0s - loss: 7.5278 - acc: 0.469 - ETA: 0s - loss: 7.5214 - acc: 0.468 - ETA: 0s - loss: 7.5269 - acc: 0.4682Epoch 00004: val\_loss improved from 8.00214 to 7.79768, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 316us/step - loss: 7.5272 - acc: 0.4683 - val\_loss: 7.7977 - val\_acc: 0.4168
Epoch 5/20
6480/6680 [============================>.] - ETA: 2s - loss: 9.7882 - acc: 0.350 - ETA: 1s - loss: 7.5947 - acc: 0.500 - ETA: 1s - loss: 7.6016 - acc: 0.500 - ETA: 1s - loss: 7.5585 - acc: 0.503 - ETA: 1s - loss: 7.0826 - acc: 0.530 - ETA: 1s - loss: 7.0490 - acc: 0.526 - ETA: 1s - loss: 7.1493 - acc: 0.518 - ETA: 1s - loss: 7.1997 - acc: 0.516 - ETA: 1s - loss: 7.2640 - acc: 0.511 - ETA: 1s - loss: 7.1383 - acc: 0.518 - ETA: 1s - loss: 7.1667 - acc: 0.515 - ETA: 1s - loss: 7.1644 - acc: 0.515 - ETA: 1s - loss: 7.1267 - acc: 0.517 - ETA: 1s - loss: 7.1769 - acc: 0.514 - ETA: 1s - loss: 7.1809 - acc: 0.512 - ETA: 1s - loss: 7.2043 - acc: 0.509 - ETA: 1s - loss: 7.2040 - acc: 0.510 - ETA: 1s - loss: 7.1652 - acc: 0.513 - ETA: 0s - loss: 7.2177 - acc: 0.509 - ETA: 0s - loss: 7.2821 - acc: 0.505 - ETA: 0s - loss: 7.2795 - acc: 0.504 - ETA: 0s - loss: 7.2961 - acc: 0.503 - ETA: 0s - loss: 7.2887 - acc: 0.505 - ETA: 0s - loss: 7.3258 - acc: 0.502 - ETA: 0s - loss: 7.3141 - acc: 0.502 - ETA: 0s - loss: 7.3111 - acc: 0.502 - ETA: 0s - loss: 7.3342 - acc: 0.500 - ETA: 0s - loss: 7.3386 - acc: 0.501 - ETA: 0s - loss: 7.3652 - acc: 0.499 - ETA: 0s - loss: 7.3942 - acc: 0.498 - ETA: 0s - loss: 7.3880 - acc: 0.499 - ETA: 0s - loss: 7.4140 - acc: 0.497 - ETA: 0s - loss: 7.3920 - acc: 0.498 - ETA: 0s - loss: 7.4076 - acc: 0.498 - ETA: 0s - loss: 7.4086 - acc: 0.497 - ETA: 0s - loss: 7.3726 - acc: 0.4994Epoch 00005: val\_loss improved from 7.79768 to 7.79266, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 300us/step - loss: 7.3612 - acc: 0.5000 - val\_loss: 7.7927 - val\_acc: 0.4323
Epoch 6/20
6520/6680 [============================>.] - ETA: 2s - loss: 6.5483 - acc: 0.550 - ETA: 1s - loss: 7.1495 - acc: 0.522 - ETA: 1s - loss: 7.3192 - acc: 0.510 - ETA: 1s - loss: 7.3826 - acc: 0.512 - ETA: 1s - loss: 7.3478 - acc: 0.514 - ETA: 1s - loss: 7.5558 - acc: 0.495 - ETA: 1s - loss: 7.7571 - acc: 0.484 - ETA: 1s - loss: 7.6131 - acc: 0.496 - ETA: 1s - loss: 7.6419 - acc: 0.495 - ETA: 1s - loss: 7.4612 - acc: 0.503 - ETA: 1s - loss: 7.4234 - acc: 0.507 - ETA: 1s - loss: 7.3916 - acc: 0.509 - ETA: 1s - loss: 7.3931 - acc: 0.509 - ETA: 1s - loss: 7.3879 - acc: 0.510 - ETA: 1s - loss: 7.3667 - acc: 0.511 - ETA: 1s - loss: 7.3131 - acc: 0.515 - ETA: 1s - loss: 7.3157 - acc: 0.514 - ETA: 0s - loss: 7.3149 - acc: 0.513 - ETA: 0s - loss: 7.3831 - acc: 0.508 - ETA: 0s - loss: 7.3211 - acc: 0.512 - ETA: 0s - loss: 7.3508 - acc: 0.510 - ETA: 0s - loss: 7.3239 - acc: 0.512 - ETA: 0s - loss: 7.3156 - acc: 0.512 - ETA: 0s - loss: 7.3296 - acc: 0.512 - ETA: 0s - loss: 7.2987 - acc: 0.513 - ETA: 0s - loss: 7.3114 - acc: 0.512 - ETA: 0s - loss: 7.2851 - acc: 0.514 - ETA: 0s - loss: 7.2553 - acc: 0.516 - ETA: 0s - loss: 7.2811 - acc: 0.515 - ETA: 0s - loss: 7.2792 - acc: 0.514 - ETA: 0s - loss: 7.2533 - acc: 0.516 - ETA: 0s - loss: 7.2621 - acc: 0.516 - ETA: 0s - loss: 7.2398 - acc: 0.517 - ETA: 0s - loss: 7.2416 - acc: 0.516 - ETA: 0s - loss: 7.2491 - acc: 0.516 - ETA: 0s - loss: 7.2586 - acc: 0.5155Epoch 00006: val\_loss improved from 7.79266 to 7.73810, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 298us/step - loss: 7.2496 - acc: 0.5162 - val\_loss: 7.7381 - val\_acc: 0.4275
Epoch 7/20
6620/6680 [============================>.] - ETA: 2s - loss: 7.2573 - acc: 0.550 - ETA: 1s - loss: 6.6907 - acc: 0.568 - ETA: 1s - loss: 7.0998 - acc: 0.537 - ETA: 1s - loss: 7.2102 - acc: 0.523 - ETA: 1s - loss: 7.2194 - acc: 0.524 - ETA: 1s - loss: 7.2877 - acc: 0.521 - ETA: 1s - loss: 7.3626 - acc: 0.518 - ETA: 1s - loss: 7.4819 - acc: 0.508 - ETA: 1s - loss: 7.5322 - acc: 0.504 - ETA: 1s - loss: 7.6618 - acc: 0.496 - ETA: 1s - loss: 7.6458 - acc: 0.496 - ETA: 1s - loss: 7.6319 - acc: 0.498 - ETA: 1s - loss: 7.6024 - acc: 0.499 - ETA: 1s - loss: 7.5882 - acc: 0.498 - ETA: 1s - loss: 7.5985 - acc: 0.497 - ETA: 1s - loss: 7.4817 - acc: 0.505 - ETA: 0s - loss: 7.4315 - acc: 0.507 - ETA: 0s - loss: 7.3672 - acc: 0.511 - ETA: 0s - loss: 7.3014 - acc: 0.514 - ETA: 0s - loss: 7.2863 - acc: 0.515 - ETA: 0s - loss: 7.2492 - acc: 0.516 - ETA: 0s - loss: 7.2163 - acc: 0.519 - ETA: 0s - loss: 7.2009 - acc: 0.521 - ETA: 0s - loss: 7.1815 - acc: 0.522 - ETA: 0s - loss: 7.1366 - acc: 0.524 - ETA: 0s - loss: 7.0840 - acc: 0.527 - ETA: 0s - loss: 7.1048 - acc: 0.526 - ETA: 0s - loss: 7.1078 - acc: 0.526 - ETA: 0s - loss: 7.1027 - acc: 0.526 - ETA: 0s - loss: 7.1169 - acc: 0.525 - ETA: 0s - loss: 7.1130 - acc: 0.526 - ETA: 0s - loss: 7.0741 - acc: 0.528 - ETA: 0s - loss: 7.0412 - acc: 0.530 - ETA: 0s - loss: 7.0448 - acc: 0.529 - ETA: 0s - loss: 7.0310 - acc: 0.530 - ETA: 0s - loss: 7.0399 - acc: 0.5302Epoch 00007: val\_loss improved from 7.73810 to 7.59115, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 295us/step - loss: 7.0552 - acc: 0.5292 - val\_loss: 7.5911 - val\_acc: 0.4599
Epoch 8/20
6620/6680 [============================>.] - ETA: 2s - loss: 7.2533 - acc: 0.550 - ETA: 1s - loss: 7.0537 - acc: 0.559 - ETA: 1s - loss: 7.0284 - acc: 0.557 - ETA: 1s - loss: 6.6576 - acc: 0.574 - ETA: 1s - loss: 6.5921 - acc: 0.572 - ETA: 1s - loss: 6.7232 - acc: 0.566 - ETA: 1s - loss: 7.0438 - acc: 0.547 - ETA: 1s - loss: 7.0635 - acc: 0.545 - ETA: 1s - loss: 7.0274 - acc: 0.546 - ETA: 1s - loss: 7.0866 - acc: 0.539 - ETA: 1s - loss: 7.0828 - acc: 0.540 - ETA: 1s - loss: 7.0668 - acc: 0.541 - ETA: 1s - loss: 7.0906 - acc: 0.537 - ETA: 1s - loss: 7.1210 - acc: 0.536 - ETA: 1s - loss: 7.1878 - acc: 0.532 - ETA: 1s - loss: 7.1134 - acc: 0.537 - ETA: 1s - loss: 7.0889 - acc: 0.539 - ETA: 0s - loss: 7.0823 - acc: 0.539 - ETA: 0s - loss: 7.1147 - acc: 0.537 - ETA: 0s - loss: 7.0742 - acc: 0.540 - ETA: 0s - loss: 7.0206 - acc: 0.542 - ETA: 0s - loss: 6.9860 - acc: 0.544 - ETA: 0s - loss: 6.9302 - acc: 0.545 - ETA: 0s - loss: 6.9199 - acc: 0.545 - ETA: 0s - loss: 6.9177 - acc: 0.545 - ETA: 0s - loss: 6.9317 - acc: 0.544 - ETA: 0s - loss: 6.9152 - acc: 0.545 - ETA: 0s - loss: 6.9077 - acc: 0.545 - ETA: 0s - loss: 6.9178 - acc: 0.545 - ETA: 0s - loss: 6.9222 - acc: 0.545 - ETA: 0s - loss: 6.9237 - acc: 0.545 - ETA: 0s - loss: 6.9141 - acc: 0.545 - ETA: 0s - loss: 6.9138 - acc: 0.546 - ETA: 0s - loss: 6.9393 - acc: 0.544 - ETA: 0s - loss: 6.9048 - acc: 0.546 - ETA: 0s - loss: 6.8760 - acc: 0.547 - ETA: 0s - loss: 6.8978 - acc: 0.5461Epoch 00008: val\_loss improved from 7.59115 to 7.45011, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 308us/step - loss: 6.8870 - acc: 0.5466 - val\_loss: 7.4501 - val\_acc: 0.4527
Epoch 9/20
6600/6680 [============================>.] - ETA: 2s - loss: 9.3857 - acc: 0.400 - ETA: 1s - loss: 6.7968 - acc: 0.555 - ETA: 1s - loss: 6.9671 - acc: 0.547 - ETA: 1s - loss: 6.9407 - acc: 0.550 - ETA: 1s - loss: 6.9065 - acc: 0.552 - ETA: 1s - loss: 6.7869 - acc: 0.558 - ETA: 1s - loss: 6.8928 - acc: 0.551 - ETA: 1s - loss: 6.7772 - acc: 0.560 - ETA: 1s - loss: 6.8045 - acc: 0.561 - ETA: 1s - loss: 6.7120 - acc: 0.566 - ETA: 1s - loss: 6.7031 - acc: 0.567 - ETA: 1s - loss: 6.6763 - acc: 0.569 - ETA: 1s - loss: 6.6808 - acc: 0.569 - ETA: 1s - loss: 6.6297 - acc: 0.571 - ETA: 1s - loss: 6.6708 - acc: 0.568 - ETA: 1s - loss: 6.6911 - acc: 0.566 - ETA: 1s - loss: 6.7284 - acc: 0.564 - ETA: 1s - loss: 6.6802 - acc: 0.567 - ETA: 1s - loss: 6.6729 - acc: 0.567 - ETA: 0s - loss: 6.7079 - acc: 0.565 - ETA: 0s - loss: 6.7388 - acc: 0.562 - ETA: 0s - loss: 6.7783 - acc: 0.560 - ETA: 0s - loss: 6.7780 - acc: 0.560 - ETA: 0s - loss: 6.7691 - acc: 0.559 - ETA: 0s - loss: 6.7769 - acc: 0.559 - ETA: 0s - loss: 6.7352 - acc: 0.561 - ETA: 0s - loss: 6.7427 - acc: 0.560 - ETA: 0s - loss: 6.7310 - acc: 0.560 - ETA: 0s - loss: 6.7115 - acc: 0.561 - ETA: 0s - loss: 6.7382 - acc: 0.559 - ETA: 0s - loss: 6.7653 - acc: 0.556 - ETA: 0s - loss: 6.7714 - acc: 0.556 - ETA: 0s - loss: 6.7703 - acc: 0.557 - ETA: 0s - loss: 6.7598 - acc: 0.558 - ETA: 0s - loss: 6.7477 - acc: 0.558 - ETA: 0s - loss: 6.7361 - acc: 0.558 - ETA: 0s - loss: 6.7393 - acc: 0.5586Epoch 00009: val\_loss improved from 7.45011 to 7.32922, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 304us/step - loss: 6.7434 - acc: 0.5585 - val\_loss: 7.3292 - val\_acc: 0.4719
Epoch 10/20
6500/6680 [============================>.] - ETA: 2s - loss: 7.3807 - acc: 0.500 - ETA: 1s - loss: 6.2826 - acc: 0.587 - ETA: 1s - loss: 6.2573 - acc: 0.593 - ETA: 1s - loss: 6.4851 - acc: 0.583 - ETA: 1s - loss: 6.6233 - acc: 0.576 - ETA: 1s - loss: 6.5290 - acc: 0.584 - ETA: 1s - loss: 6.6298 - acc: 0.577 - ETA: 1s - loss: 6.4134 - acc: 0.590 - ETA: 1s - loss: 6.5616 - acc: 0.581 - ETA: 1s - loss: 6.6025 - acc: 0.578 - ETA: 1s - loss: 6.5642 - acc: 0.581 - ETA: 1s - loss: 6.5674 - acc: 0.580 - ETA: 1s - loss: 6.5725 - acc: 0.579 - ETA: 1s - loss: 6.6267 - acc: 0.576 - ETA: 1s - loss: 6.6035 - acc: 0.578 - ETA: 1s - loss: 6.6007 - acc: 0.578 - ETA: 1s - loss: 6.6247 - acc: 0.576 - ETA: 0s - loss: 6.6651 - acc: 0.573 - ETA: 0s - loss: 6.6410 - acc: 0.575 - ETA: 0s - loss: 6.6611 - acc: 0.573 - ETA: 0s - loss: 6.6792 - acc: 0.572 - ETA: 0s - loss: 6.6522 - acc: 0.573 - ETA: 0s - loss: 6.6747 - acc: 0.572 - ETA: 0s - loss: 6.6906 - acc: 0.570 - ETA: 0s - loss: 6.6863 - acc: 0.570 - ETA: 0s - loss: 6.6888 - acc: 0.570 - ETA: 0s - loss: 6.6604 - acc: 0.571 - ETA: 0s - loss: 6.6612 - acc: 0.571 - ETA: 0s - loss: 6.6659 - acc: 0.570 - ETA: 0s - loss: 6.6626 - acc: 0.570 - ETA: 0s - loss: 6.6638 - acc: 0.570 - ETA: 0s - loss: 6.6717 - acc: 0.570 - ETA: 0s - loss: 6.6689 - acc: 0.570 - ETA: 0s - loss: 6.6888 - acc: 0.569 - ETA: 0s - loss: 6.6839 - acc: 0.568 - ETA: 0s - loss: 6.6880 - acc: 0.5683Epoch 00010: val\_loss did not improve
6680/6680 [==============================] - 2s 298us/step - loss: 6.6810 - acc: 0.5687 - val\_loss: 7.4410 - val\_acc: 0.4527
Epoch 11/20
6580/6680 [============================>.] - ETA: 2s - loss: 5.6580 - acc: 0.650 - ETA: 1s - loss: 6.9960 - acc: 0.559 - ETA: 1s - loss: 6.8864 - acc: 0.567 - ETA: 1s - loss: 6.8438 - acc: 0.567 - ETA: 1s - loss: 6.7000 - acc: 0.575 - ETA: 1s - loss: 6.7338 - acc: 0.572 - ETA: 1s - loss: 6.6794 - acc: 0.575 - ETA: 1s - loss: 6.6091 - acc: 0.579 - ETA: 1s - loss: 6.6333 - acc: 0.578 - ETA: 1s - loss: 6.6369 - acc: 0.578 - ETA: 1s - loss: 6.6547 - acc: 0.575 - ETA: 1s - loss: 6.6417 - acc: 0.575 - ETA: 1s - loss: 6.6538 - acc: 0.575 - ETA: 1s - loss: 6.6054 - acc: 0.576 - ETA: 1s - loss: 6.5694 - acc: 0.579 - ETA: 1s - loss: 6.5073 - acc: 0.582 - ETA: 1s - loss: 6.4622 - acc: 0.585 - ETA: 0s - loss: 6.5487 - acc: 0.580 - ETA: 0s - loss: 6.5556 - acc: 0.578 - ETA: 0s - loss: 6.5691 - acc: 0.578 - ETA: 0s - loss: 6.5630 - acc: 0.578 - ETA: 0s - loss: 6.6060 - acc: 0.576 - ETA: 0s - loss: 6.6125 - acc: 0.575 - ETA: 0s - loss: 6.5787 - acc: 0.578 - ETA: 0s - loss: 6.6056 - acc: 0.576 - ETA: 0s - loss: 6.5935 - acc: 0.577 - ETA: 0s - loss: 6.5864 - acc: 0.577 - ETA: 0s - loss: 6.5758 - acc: 0.577 - ETA: 0s - loss: 6.6133 - acc: 0.574 - ETA: 0s - loss: 6.5800 - acc: 0.576 - ETA: 0s - loss: 6.5661 - acc: 0.577 - ETA: 0s - loss: 6.5635 - acc: 0.577 - ETA: 0s - loss: 6.5776 - acc: 0.576 - ETA: 0s - loss: 6.5888 - acc: 0.576 - ETA: 0s - loss: 6.6007 - acc: 0.575 - ETA: 0s - loss: 6.6128 - acc: 0.5743Epoch 00011: val\_loss did not improve
6680/6680 [==============================] - 2s 295us/step - loss: 6.6181 - acc: 0.5740 - val\_loss: 7.3495 - val\_acc: 0.4659
Epoch 12/20
6520/6680 [============================>.] - ETA: 3s - loss: 8.8658 - acc: 0.450 - ETA: 1s - loss: 6.9092 - acc: 0.559 - ETA: 1s - loss: 6.6405 - acc: 0.569 - ETA: 1s - loss: 6.7285 - acc: 0.566 - ETA: 1s - loss: 6.6096 - acc: 0.575 - ETA: 1s - loss: 6.6804 - acc: 0.571 - ETA: 1s - loss: 6.6217 - acc: 0.575 - ETA: 1s - loss: 6.5624 - acc: 0.581 - ETA: 1s - loss: 6.4695 - acc: 0.585 - ETA: 1s - loss: 6.4626 - acc: 0.586 - ETA: 1s - loss: 6.4387 - acc: 0.587 - ETA: 1s - loss: 6.4536 - acc: 0.585 - ETA: 1s - loss: 6.4239 - acc: 0.586 - ETA: 1s - loss: 6.4149 - acc: 0.587 - ETA: 1s - loss: 6.3919 - acc: 0.588 - ETA: 1s - loss: 6.3749 - acc: 0.589 - ETA: 1s - loss: 6.3807 - acc: 0.588 - ETA: 0s - loss: 6.4021 - acc: 0.587 - ETA: 0s - loss: 6.4320 - acc: 0.584 - ETA: 0s - loss: 6.4156 - acc: 0.585 - ETA: 0s - loss: 6.4510 - acc: 0.582 - ETA: 0s - loss: 6.4419 - acc: 0.582 - ETA: 0s - loss: 6.4235 - acc: 0.583 - ETA: 0s - loss: 6.4342 - acc: 0.582 - ETA: 0s - loss: 6.4278 - acc: 0.582 - ETA: 0s - loss: 6.4256 - acc: 0.582 - ETA: 0s - loss: 6.4085 - acc: 0.582 - ETA: 0s - loss: 6.4065 - acc: 0.582 - ETA: 0s - loss: 6.4289 - acc: 0.581 - ETA: 0s - loss: 6.4510 - acc: 0.580 - ETA: 0s - loss: 6.4659 - acc: 0.580 - ETA: 0s - loss: 6.4390 - acc: 0.581 - ETA: 0s - loss: 6.4460 - acc: 0.580 - ETA: 0s - loss: 6.4447 - acc: 0.580 - ETA: 0s - loss: 6.4858 - acc: 0.578 - ETA: 0s - loss: 6.4700 - acc: 0.5791Epoch 00012: val\_loss improved from 7.32922 to 7.25560, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 299us/step - loss: 6.4922 - acc: 0.5774 - val\_loss: 7.2556 - val\_acc: 0.4575
Epoch 13/20
6540/6680 [============================>.] - ETA: 2s - loss: 5.8571 - acc: 0.600 - ETA: 1s - loss: 6.7200 - acc: 0.545 - ETA: 1s - loss: 6.2693 - acc: 0.583 - ETA: 1s - loss: 6.4135 - acc: 0.577 - ETA: 1s - loss: 6.4209 - acc: 0.578 - ETA: 1s - loss: 6.6058 - acc: 0.567 - ETA: 1s - loss: 6.4900 - acc: 0.573 - ETA: 1s - loss: 6.4536 - acc: 0.576 - ETA: 1s - loss: 6.5184 - acc: 0.571 - ETA: 1s - loss: 6.6039 - acc: 0.567 - ETA: 1s - loss: 6.5301 - acc: 0.572 - ETA: 1s - loss: 6.6055 - acc: 0.568 - ETA: 1s - loss: 6.5741 - acc: 0.569 - ETA: 1s - loss: 6.5280 - acc: 0.571 - ETA: 1s - loss: 6.5060 - acc: 0.574 - ETA: 0s - loss: 6.5176 - acc: 0.573 - ETA: 0s - loss: 6.4850 - acc: 0.575 - ETA: 0s - loss: 6.4794 - acc: 0.576 - ETA: 0s - loss: 6.4757 - acc: 0.577 - ETA: 0s - loss: 6.4729 - acc: 0.576 - ETA: 0s - loss: 6.4546 - acc: 0.577 - ETA: 0s - loss: 6.4952 - acc: 0.575 - ETA: 0s - loss: 6.4789 - acc: 0.576 - ETA: 0s - loss: 6.4432 - acc: 0.578 - ETA: 0s - loss: 6.4734 - acc: 0.577 - ETA: 0s - loss: 6.4821 - acc: 0.576 - ETA: 0s - loss: 6.4575 - acc: 0.577 - ETA: 0s - loss: 6.4257 - acc: 0.579 - ETA: 0s - loss: 6.4216 - acc: 0.580 - ETA: 0s - loss: 6.3855 - acc: 0.582 - ETA: 0s - loss: 6.3745 - acc: 0.583 - ETA: 0s - loss: 6.3774 - acc: 0.582 - ETA: 0s - loss: 6.3752 - acc: 0.582 - ETA: 0s - loss: 6.3615 - acc: 0.5827Epoch 00013: val\_loss improved from 7.25560 to 7.08228, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 285us/step - loss: 6.3709 - acc: 0.5822 - val\_loss: 7.0823 - val\_acc: 0.4719
Epoch 14/20
6520/6680 [============================>.] - ETA: 2s - loss: 3.2256 - acc: 0.800 - ETA: 1s - loss: 6.0825 - acc: 0.604 - ETA: 1s - loss: 6.1787 - acc: 0.590 - ETA: 1s - loss: 6.3123 - acc: 0.583 - ETA: 1s - loss: 6.2054 - acc: 0.591 - ETA: 1s - loss: 6.1863 - acc: 0.595 - ETA: 1s - loss: 6.1457 - acc: 0.598 - ETA: 1s - loss: 6.1384 - acc: 0.600 - ETA: 1s - loss: 6.0898 - acc: 0.604 - ETA: 1s - loss: 6.1241 - acc: 0.601 - ETA: 1s - loss: 6.0533 - acc: 0.607 - ETA: 1s - loss: 6.0854 - acc: 0.603 - ETA: 1s - loss: 6.1212 - acc: 0.602 - ETA: 1s - loss: 6.1489 - acc: 0.600 - ETA: 1s - loss: 6.1499 - acc: 0.601 - ETA: 1s - loss: 6.2701 - acc: 0.593 - ETA: 1s - loss: 6.2779 - acc: 0.593 - ETA: 0s - loss: 6.2748 - acc: 0.593 - ETA: 0s - loss: 6.2328 - acc: 0.596 - ETA: 0s - loss: 6.2258 - acc: 0.597 - ETA: 0s - loss: 6.2808 - acc: 0.594 - ETA: 0s - loss: 6.3099 - acc: 0.592 - ETA: 0s - loss: 6.3077 - acc: 0.592 - ETA: 0s - loss: 6.2945 - acc: 0.593 - ETA: 0s - loss: 6.2874 - acc: 0.594 - ETA: 0s - loss: 6.2941 - acc: 0.593 - ETA: 0s - loss: 6.2819 - acc: 0.594 - ETA: 0s - loss: 6.2854 - acc: 0.594 - ETA: 0s - loss: 6.2782 - acc: 0.594 - ETA: 0s - loss: 6.2640 - acc: 0.595 - ETA: 0s - loss: 6.2590 - acc: 0.595 - ETA: 0s - loss: 6.2497 - acc: 0.596 - ETA: 0s - loss: 6.2453 - acc: 0.596 - ETA: 0s - loss: 6.2343 - acc: 0.597 - ETA: 0s - loss: 6.2433 - acc: 0.597 - ETA: 0s - loss: 6.2281 - acc: 0.5980Epoch 00014: val\_loss improved from 7.08228 to 7.04985, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 300us/step - loss: 6.2225 - acc: 0.5982 - val\_loss: 7.0499 - val\_acc: 0.4743
Epoch 15/20
6620/6680 [============================>.] - ETA: 2s - loss: 7.2532 - acc: 0.550 - ETA: 1s - loss: 6.8988 - acc: 0.563 - ETA: 1s - loss: 6.7006 - acc: 0.572 - ETA: 1s - loss: 6.5652 - acc: 0.583 - ETA: 1s - loss: 6.5315 - acc: 0.583 - ETA: 1s - loss: 6.2717 - acc: 0.600 - ETA: 1s - loss: 6.2329 - acc: 0.602 - ETA: 1s - loss: 6.3242 - acc: 0.597 - ETA: 1s - loss: 6.2494 - acc: 0.602 - ETA: 1s - loss: 6.2404 - acc: 0.601 - ETA: 1s - loss: 6.2718 - acc: 0.601 - ETA: 1s - loss: 6.3061 - acc: 0.597 - ETA: 1s - loss: 6.2895 - acc: 0.598 - ETA: 1s - loss: 6.3243 - acc: 0.595 - ETA: 1s - loss: 6.3116 - acc: 0.596 - ETA: 1s - loss: 6.3313 - acc: 0.595 - ETA: 1s - loss: 6.3195 - acc: 0.596 - ETA: 1s - loss: 6.3332 - acc: 0.595 - ETA: 0s - loss: 6.2699 - acc: 0.598 - ETA: 0s - loss: 6.2352 - acc: 0.600 - ETA: 0s - loss: 6.2185 - acc: 0.602 - ETA: 0s - loss: 6.1738 - acc: 0.604 - ETA: 0s - loss: 6.1625 - acc: 0.605 - ETA: 0s - loss: 6.1593 - acc: 0.605 - ETA: 0s - loss: 6.2028 - acc: 0.602 - ETA: 0s - loss: 6.2189 - acc: 0.601 - ETA: 0s - loss: 6.1786 - acc: 0.603 - ETA: 0s - loss: 6.1834 - acc: 0.602 - ETA: 0s - loss: 6.1715 - acc: 0.602 - ETA: 0s - loss: 6.1716 - acc: 0.603 - ETA: 0s - loss: 6.1632 - acc: 0.603 - ETA: 0s - loss: 6.1752 - acc: 0.602 - ETA: 0s - loss: 6.1548 - acc: 0.602 - ETA: 0s - loss: 6.1488 - acc: 0.603 - ETA: 0s - loss: 6.1297 - acc: 0.604 - ETA: 0s - loss: 6.1401 - acc: 0.603 - ETA: 0s - loss: 6.1515 - acc: 0.6035Epoch 00015: val\_loss improved from 7.04985 to 6.92389, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 303us/step - loss: 6.1447 - acc: 0.6039 - val\_loss: 6.9239 - val\_acc: 0.4802
Epoch 16/20
6600/6680 [============================>.] - ETA: 2s - loss: 4.3418 - acc: 0.700 - ETA: 1s - loss: 5.1757 - acc: 0.672 - ETA: 1s - loss: 5.7529 - acc: 0.632 - ETA: 1s - loss: 5.6891 - acc: 0.636 - ETA: 1s - loss: 5.8641 - acc: 0.621 - ETA: 1s - loss: 6.0676 - acc: 0.610 - ETA: 1s - loss: 5.9576 - acc: 0.617 - ETA: 1s - loss: 5.9938 - acc: 0.615 - ETA: 1s - loss: 6.0058 - acc: 0.616 - ETA: 1s - loss: 5.9746 - acc: 0.617 - ETA: 1s - loss: 5.9251 - acc: 0.620 - ETA: 1s - loss: 5.9411 - acc: 0.619 - ETA: 1s - loss: 5.9869 - acc: 0.616 - ETA: 1s - loss: 6.0644 - acc: 0.611 - ETA: 1s - loss: 6.0488 - acc: 0.613 - ETA: 1s - loss: 6.0106 - acc: 0.615 - ETA: 1s - loss: 6.0252 - acc: 0.614 - ETA: 0s - loss: 5.9725 - acc: 0.617 - ETA: 0s - loss: 5.9739 - acc: 0.618 - ETA: 0s - loss: 5.9502 - acc: 0.619 - ETA: 0s - loss: 5.9172 - acc: 0.621 - ETA: 0s - loss: 5.8952 - acc: 0.622 - ETA: 0s - loss: 5.9377 - acc: 0.620 - ETA: 0s - loss: 5.9140 - acc: 0.622 - ETA: 0s - loss: 5.9695 - acc: 0.619 - ETA: 0s - loss: 5.9720 - acc: 0.619 - ETA: 0s - loss: 5.9428 - acc: 0.621 - ETA: 0s - loss: 5.9470 - acc: 0.620 - ETA: 0s - loss: 5.9749 - acc: 0.618 - ETA: 0s - loss: 5.9771 - acc: 0.618 - ETA: 0s - loss: 6.0126 - acc: 0.615 - ETA: 0s - loss: 6.0109 - acc: 0.615 - ETA: 0s - loss: 6.0066 - acc: 0.616 - ETA: 0s - loss: 6.0204 - acc: 0.615 - ETA: 0s - loss: 6.0249 - acc: 0.614 - ETA: 0s - loss: 6.0412 - acc: 0.613 - ETA: 0s - loss: 6.0507 - acc: 0.6132Epoch 00016: val\_loss improved from 6.92389 to 6.85881, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 307us/step - loss: 6.0532 - acc: 0.6132 - val\_loss: 6.8588 - val\_acc: 0.4910
Epoch 17/20
6580/6680 [============================>.] - ETA: 2s - loss: 6.4835 - acc: 0.600 - ETA: 1s - loss: 6.7981 - acc: 0.570 - ETA: 1s - loss: 6.2107 - acc: 0.610 - ETA: 1s - loss: 6.2019 - acc: 0.612 - ETA: 1s - loss: 6.0025 - acc: 0.624 - ETA: 1s - loss: 5.9064 - acc: 0.628 - ETA: 1s - loss: 5.9311 - acc: 0.626 - ETA: 1s - loss: 5.9560 - acc: 0.625 - ETA: 1s - loss: 5.9165 - acc: 0.626 - ETA: 1s - loss: 5.9727 - acc: 0.621 - ETA: 1s - loss: 6.0618 - acc: 0.615 - ETA: 1s - loss: 5.9719 - acc: 0.620 - ETA: 1s - loss: 6.0017 - acc: 0.619 - ETA: 1s - loss: 6.0057 - acc: 0.617 - ETA: 1s - loss: 5.9757 - acc: 0.617 - ETA: 1s - loss: 6.0210 - acc: 0.614 - ETA: 1s - loss: 6.0261 - acc: 0.613 - ETA: 1s - loss: 6.0477 - acc: 0.613 - ETA: 1s - loss: 6.0416 - acc: 0.613 - ETA: 1s - loss: 6.0364 - acc: 0.613 - ETA: 0s - loss: 5.9680 - acc: 0.617 - ETA: 0s - loss: 5.9868 - acc: 0.616 - ETA: 0s - loss: 5.9379 - acc: 0.620 - ETA: 0s - loss: 5.9189 - acc: 0.620 - ETA: 0s - loss: 5.9326 - acc: 0.620 - ETA: 0s - loss: 5.9029 - acc: 0.621 - ETA: 0s - loss: 5.9283 - acc: 0.619 - ETA: 0s - loss: 5.9356 - acc: 0.619 - ETA: 0s - loss: 5.9229 - acc: 0.619 - ETA: 0s - loss: 5.9134 - acc: 0.620 - ETA: 0s - loss: 5.9343 - acc: 0.619 - ETA: 0s - loss: 5.9171 - acc: 0.620 - ETA: 0s - loss: 5.9204 - acc: 0.619 - ETA: 0s - loss: 5.9322 - acc: 0.619 - ETA: 0s - loss: 5.9528 - acc: 0.617 - ETA: 0s - loss: 5.9430 - acc: 0.618 - ETA: 0s - loss: 5.9742 - acc: 0.6164Epoch 00017: val\_loss improved from 6.85881 to 6.80238, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 306us/step - loss: 5.9881 - acc: 0.6156 - val\_loss: 6.8024 - val\_acc: 0.4970
Epoch 18/20
6660/6680 [============================>.] - ETA: 3s - loss: 3.2238 - acc: 0.800 - ETA: 1s - loss: 4.7245 - acc: 0.700 - ETA: 1s - loss: 5.4794 - acc: 0.654 - ETA: 1s - loss: 5.7987 - acc: 0.632 - ETA: 1s - loss: 6.0036 - acc: 0.619 - ETA: 1s - loss: 5.7649 - acc: 0.634 - ETA: 1s - loss: 5.7517 - acc: 0.631 - ETA: 1s - loss: 5.6774 - acc: 0.635 - ETA: 1s - loss: 5.8063 - acc: 0.628 - ETA: 1s - loss: 5.8255 - acc: 0.627 - ETA: 1s - loss: 5.7449 - acc: 0.631 - ETA: 1s - loss: 5.8369 - acc: 0.625 - ETA: 1s - loss: 5.8021 - acc: 0.629 - ETA: 1s - loss: 5.7955 - acc: 0.629 - ETA: 1s - loss: 5.7683 - acc: 0.632 - ETA: 0s - loss: 5.8217 - acc: 0.629 - ETA: 0s - loss: 5.8235 - acc: 0.629 - ETA: 0s - loss: 5.8344 - acc: 0.628 - ETA: 0s - loss: 5.7920 - acc: 0.631 - ETA: 0s - loss: 5.8022 - acc: 0.630 - ETA: 0s - loss: 5.7499 - acc: 0.634 - ETA: 0s - loss: 5.7218 - acc: 0.636 - ETA: 0s - loss: 5.7522 - acc: 0.634 - ETA: 0s - loss: 5.7784 - acc: 0.633 - ETA: 0s - loss: 5.8133 - acc: 0.631 - ETA: 0s - loss: 5.8646 - acc: 0.628 - ETA: 0s - loss: 5.8706 - acc: 0.627 - ETA: 0s - loss: 5.8929 - acc: 0.626 - ETA: 0s - loss: 5.8578 - acc: 0.628 - ETA: 0s - loss: 5.8708 - acc: 0.627 - ETA: 0s - loss: 5.8504 - acc: 0.628 - ETA: 0s - loss: 5.8949 - acc: 0.625 - ETA: 0s - loss: 5.9207 - acc: 0.624 - ETA: 0s - loss: 5.9319 - acc: 0.623 - ETA: 0s - loss: 5.9284 - acc: 0.6231Epoch 00018: val\_loss improved from 6.80238 to 6.79682, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 288us/step - loss: 5.9203 - acc: 0.6237 - val\_loss: 6.7968 - val\_acc: 0.4946
Epoch 19/20
6500/6680 [============================>.] - ETA: 3s - loss: 4.0300 - acc: 0.750 - ETA: 1s - loss: 4.9401 - acc: 0.686 - ETA: 1s - loss: 5.0967 - acc: 0.676 - ETA: 1s - loss: 5.2302 - acc: 0.669 - ETA: 1s - loss: 5.3757 - acc: 0.658 - ETA: 1s - loss: 5.4949 - acc: 0.651 - ETA: 1s - loss: 5.4790 - acc: 0.650 - ETA: 1s - loss: 5.5936 - acc: 0.644 - ETA: 1s - loss: 5.7176 - acc: 0.634 - ETA: 1s - loss: 5.7435 - acc: 0.634 - ETA: 1s - loss: 5.6959 - acc: 0.636 - ETA: 1s - loss: 5.6711 - acc: 0.638 - ETA: 1s - loss: 5.7667 - acc: 0.632 - ETA: 1s - loss: 5.7904 - acc: 0.629 - ETA: 1s - loss: 5.7438 - acc: 0.632 - ETA: 1s - loss: 5.7455 - acc: 0.631 - ETA: 1s - loss: 5.7569 - acc: 0.631 - ETA: 1s - loss: 5.7850 - acc: 0.629 - ETA: 0s - loss: 5.7827 - acc: 0.629 - ETA: 0s - loss: 5.8175 - acc: 0.627 - ETA: 0s - loss: 5.7673 - acc: 0.629 - ETA: 0s - loss: 5.7795 - acc: 0.628 - ETA: 0s - loss: 5.7756 - acc: 0.628 - ETA: 0s - loss: 5.7967 - acc: 0.627 - ETA: 0s - loss: 5.8045 - acc: 0.626 - ETA: 0s - loss: 5.8181 - acc: 0.625 - ETA: 0s - loss: 5.8260 - acc: 0.624 - ETA: 0s - loss: 5.8102 - acc: 0.625 - ETA: 0s - loss: 5.8514 - acc: 0.623 - ETA: 0s - loss: 5.8241 - acc: 0.625 - ETA: 0s - loss: 5.8160 - acc: 0.625 - ETA: 0s - loss: 5.8108 - acc: 0.625 - ETA: 0s - loss: 5.8194 - acc: 0.625 - ETA: 0s - loss: 5.8293 - acc: 0.624 - ETA: 0s - loss: 5.8478 - acc: 0.623 - ETA: 0s - loss: 5.8720 - acc: 0.6223Epoch 00019: val\_loss did not improve
6680/6680 [==============================] - 2s 298us/step - loss: 5.8714 - acc: 0.6225 - val\_loss: 6.8311 - val\_acc: 0.4874
Epoch 20/20
6540/6680 [============================>.] - ETA: 2s - loss: 6.4474 - acc: 0.600 - ETA: 1s - loss: 4.9996 - acc: 0.681 - ETA: 1s - loss: 5.5719 - acc: 0.650 - ETA: 1s - loss: 5.8934 - acc: 0.630 - ETA: 1s - loss: 5.8268 - acc: 0.634 - ETA: 1s - loss: 5.7279 - acc: 0.640 - ETA: 1s - loss: 5.5426 - acc: 0.651 - ETA: 1s - loss: 5.5032 - acc: 0.651 - ETA: 1s - loss: 5.6418 - acc: 0.643 - ETA: 1s - loss: 5.7060 - acc: 0.639 - ETA: 1s - loss: 5.7633 - acc: 0.634 - ETA: 1s - loss: 5.8009 - acc: 0.633 - ETA: 1s - loss: 5.8123 - acc: 0.632 - ETA: 1s - loss: 5.8053 - acc: 0.631 - ETA: 1s - loss: 5.7771 - acc: 0.633 - ETA: 1s - loss: 5.7845 - acc: 0.633 - ETA: 1s - loss: 5.7675 - acc: 0.634 - ETA: 1s - loss: 5.7949 - acc: 0.633 - ETA: 0s - loss: 5.7742 - acc: 0.634 - ETA: 0s - loss: 5.7619 - acc: 0.634 - ETA: 0s - loss: 5.7780 - acc: 0.633 - ETA: 0s - loss: 5.7690 - acc: 0.634 - ETA: 0s - loss: 5.8219 - acc: 0.631 - ETA: 0s - loss: 5.8271 - acc: 0.631 - ETA: 0s - loss: 5.8256 - acc: 0.631 - ETA: 0s - loss: 5.7913 - acc: 0.633 - ETA: 0s - loss: 5.8160 - acc: 0.631 - ETA: 0s - loss: 5.8173 - acc: 0.631 - ETA: 0s - loss: 5.8357 - acc: 0.629 - ETA: 0s - loss: 5.8227 - acc: 0.630 - ETA: 0s - loss: 5.7953 - acc: 0.632 - ETA: 0s - loss: 5.8036 - acc: 0.631 - ETA: 0s - loss: 5.8018 - acc: 0.631 - ETA: 0s - loss: 5.7927 - acc: 0.632 - ETA: 0s - loss: 5.7927 - acc: 0.632 - ETA: 0s - loss: 5.8005 - acc: 0.6317Epoch 00020: val\_loss improved from 6.79682 to 6.68496, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s 301us/step - loss: 5.8067 - acc: 0.6311 - val\_loss: 6.6850 - val\_acc: 0.5114

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}21}]:} <keras.callbacks.History at 0x242e386bda0>
\end{Verbatim}
            
    \subsubsection{Load the Model with the Best Validation
Loss}\label{load-the-model-with-the-best-validation-loss}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.VGG16.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Test the Model}\label{test-the-model}

Now, we can use the CNN to test how well it identifies breed within our
test dataset of dog images. We print the test accuracy below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{c+c1}{\PYZsh{} get index of predicted dog breed for each image in test set}
         \PY{n}{VGG16\PYZus{}predictions} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{feature}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{test\PYZus{}VGG16}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} report test accuracy}
         \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{VGG16\PYZus{}predictions}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{VGG16\PYZus{}predictions}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test accuracy: 50.1196\%

    \end{Verbatim}

    \subsubsection{Predict Dog Breed with the
Model}\label{predict-dog-breed-with-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{k+kn}{from} \PY{n+nn}{extract\PYZus{}bottleneck\PYZus{}features} \PY{k}{import} \PY{o}{*}
         
         \PY{k}{def} \PY{n+nf}{VGG16\PYZus{}predict\PYZus{}breed}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} extract bottleneck features}
             \PY{n}{bottleneck\PYZus{}feature} \PY{o}{=} \PY{n}{extract\PYZus{}VGG16}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} obtain predicted vector}
             \PY{n}{predicted\PYZus{}vector} \PY{o}{=} \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{bottleneck\PYZus{}feature}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} return dog breed that is predicted by the model}
             \PY{k}{return} \PY{n}{dog\PYZus{}names}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{predicted\PYZus{}vector}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 5: Create a CNN to Classify Dog Breeds (using Transfer
Learning)

You will now use transfer learning to create a CNN that can identify dog
breed from images. Your CNN must attain at least 60\% accuracy on the
test set.

In Step 4, we used transfer learning to create a CNN using VGG-16
bottleneck features. In this section, you must use the bottleneck
features from a different pre-trained model. To make things easier for
you, we have pre-computed the features for all of the networks that are
currently available in Keras: -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz}{VGG-19}
bottleneck features -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz}{ResNet-50}
bottleneck features -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz}{Inception}
bottleneck features -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz}{Xception}
bottleneck features

The files are encoded as such:

\begin{verbatim}
Dog{network}Data.npz
\end{verbatim}

where \texttt{\{network\}}, in the above filename, can be one of
\texttt{VGG19}, \texttt{Resnet50}, \texttt{InceptionV3}, or
\texttt{Xception}. Pick one of the above architectures, download the
corresponding bottleneck features, and store the downloaded file in the
\texttt{bottleneck\_features/} folder in the repository.

\subsubsection{(IMPLEMENTATION) Obtain Bottleneck
Features}\label{implementation-obtain-bottleneck-features}

In the code block below, extract the bottleneck features corresponding
to the train, test, and validation sets by running the following:

\begin{verbatim}
bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')
train_{network} = bottleneck_features['train']
valid_{network} = bottleneck_features['valid']
test_{network} = bottleneck_features['test']
\end{verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Obtain bottleneck features from another pre\PYZhy{}trained CNN.}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications} \PY{k}{import} \PY{n}{Xception}
         
         \PY{n}{bottleneck\PYZus{}features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bottleneck\PYZus{}features/DogXceptionData.npz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{train\PYZus{}Xception} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{valid\PYZus{}Xception} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{test\PYZus{}Xception} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Model
Architecture}\label{implementation-model-architecture}

Create a CNN to classify dog breed. At the end of your code cell block,
summarize the layers of your model by executing the line:

\begin{verbatim}
    <your model's name>.summary()
\end{verbatim}

\textbf{Question 5:} Outline the steps you took to get to your final CNN
architecture and your reasoning at each step. Describe why you think the
architecture is suitable for the current problem.

\textbf{Answer:} By using a pretrained model with more images, the
accuracy can finally get much higher. Choose Xception is maybe the
slowest network, but should provide highest accuracy.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Define your architecture.}
         \PY{n}{Xception\PYZus{}model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{Xception\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{train\PYZus{}Xception}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{Xception\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{Xception\PYZus{}model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
global\_average\_pooling2d\_2 ( (None, 2048)              0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_4 (Dense)              (None, 133)               272517    
=================================================================
Total params: 272,517
Trainable params: 272,517
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \subsubsection{(IMPLEMENTATION) Compile the
Model}\label{implementation-compile-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Compile the model.}
         \PY{n}{Xception\PYZus{}model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rmsprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Train the
Model}\label{implementation-train-the-model}

Train your model in the code cell below. Use model checkpointing to save
the model that attains the best validation loss.

You are welcome to
\href{https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html}{augment
the training data}, but this is not a requirement.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Train the model.}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{ModelCheckpoint}  
         
         \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.Xception.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                        \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{Xception\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}Xception}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} 
                   \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{valid\PYZus{}Xception}\PY{p}{,} \PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{,}
                   \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/20
6660/6680 [============================>.] - ETA: 2:13 - loss: 5.1381 - acc: 0.0000e+0 - ETA: 37s - loss: 4.8754 - acc: 0.1125    - ETA: 21s - loss: 4.8287 - acc: 0.10 - ETA: 15s - loss: 4.5370 - acc: 0.17 - ETA: 12s - loss: 4.2921 - acc: 0.21 - ETA: 11s - loss: 4.1033 - acc: 0.24 - ETA: 10s - loss: 3.9061 - acc: 0.27 - ETA: 9s - loss: 3.7719 - acc: 0.3000 - ETA: 8s - loss: 3.6923 - acc: 0.315 - ETA: 8s - loss: 3.5622 - acc: 0.330 - ETA: 8s - loss: 3.4474 - acc: 0.354 - ETA: 7s - loss: 3.3397 - acc: 0.373 - ETA: 7s - loss: 3.2507 - acc: 0.392 - ETA: 7s - loss: 3.1106 - acc: 0.413 - ETA: 6s - loss: 2.9723 - acc: 0.441 - ETA: 6s - loss: 2.8673 - acc: 0.457 - ETA: 6s - loss: 2.7637 - acc: 0.469 - ETA: 5s - loss: 2.6704 - acc: 0.484 - ETA: 5s - loss: 2.6103 - acc: 0.491 - ETA: 5s - loss: 2.5237 - acc: 0.502 - ETA: 5s - loss: 2.4492 - acc: 0.513 - ETA: 5s - loss: 2.3858 - acc: 0.520 - ETA: 5s - loss: 2.3198 - acc: 0.530 - ETA: 5s - loss: 2.2575 - acc: 0.539 - ETA: 4s - loss: 2.2100 - acc: 0.547 - ETA: 4s - loss: 2.1562 - acc: 0.556 - ETA: 4s - loss: 2.1147 - acc: 0.562 - ETA: 4s - loss: 2.0640 - acc: 0.571 - ETA: 4s - loss: 2.0330 - acc: 0.576 - ETA: 4s - loss: 1.9905 - acc: 0.583 - ETA: 4s - loss: 1.9494 - acc: 0.589 - ETA: 4s - loss: 1.9097 - acc: 0.595 - ETA: 4s - loss: 1.8745 - acc: 0.602 - ETA: 3s - loss: 1.8388 - acc: 0.608 - ETA: 3s - loss: 1.8057 - acc: 0.614 - ETA: 3s - loss: 1.7785 - acc: 0.618 - ETA: 3s - loss: 1.7486 - acc: 0.624 - ETA: 3s - loss: 1.7162 - acc: 0.629 - ETA: 3s - loss: 1.6849 - acc: 0.633 - ETA: 3s - loss: 1.6622 - acc: 0.636 - ETA: 3s - loss: 1.6410 - acc: 0.638 - ETA: 3s - loss: 1.6148 - acc: 0.643 - ETA: 3s - loss: 1.5893 - acc: 0.648 - ETA: 3s - loss: 1.5670 - acc: 0.652 - ETA: 2s - loss: 1.5453 - acc: 0.655 - ETA: 2s - loss: 1.5235 - acc: 0.658 - ETA: 2s - loss: 1.5139 - acc: 0.660 - ETA: 2s - loss: 1.4930 - acc: 0.663 - ETA: 2s - loss: 1.4760 - acc: 0.665 - ETA: 2s - loss: 1.4579 - acc: 0.667 - ETA: 2s - loss: 1.4387 - acc: 0.671 - ETA: 2s - loss: 1.4220 - acc: 0.673 - ETA: 2s - loss: 1.4091 - acc: 0.675 - ETA: 2s - loss: 1.3915 - acc: 0.677 - ETA: 2s - loss: 1.3767 - acc: 0.680 - ETA: 2s - loss: 1.3576 - acc: 0.684 - ETA: 2s - loss: 1.3476 - acc: 0.685 - ETA: 2s - loss: 1.3381 - acc: 0.686 - ETA: 1s - loss: 1.3260 - acc: 0.689 - ETA: 1s - loss: 1.3084 - acc: 0.693 - ETA: 1s - loss: 1.2939 - acc: 0.695 - ETA: 1s - loss: 1.2820 - acc: 0.697 - ETA: 1s - loss: 1.2722 - acc: 0.698 - ETA: 1s - loss: 1.2649 - acc: 0.699 - ETA: 1s - loss: 1.2591 - acc: 0.699 - ETA: 1s - loss: 1.2488 - acc: 0.701 - ETA: 1s - loss: 1.2359 - acc: 0.704 - ETA: 1s - loss: 1.2274 - acc: 0.704 - ETA: 1s - loss: 1.2166 - acc: 0.707 - ETA: 1s - loss: 1.2087 - acc: 0.708 - ETA: 1s - loss: 1.1993 - acc: 0.711 - ETA: 1s - loss: 1.1926 - acc: 0.711 - ETA: 1s - loss: 1.1814 - acc: 0.714 - ETA: 1s - loss: 1.1728 - acc: 0.715 - ETA: 0s - loss: 1.1628 - acc: 0.718 - ETA: 0s - loss: 1.1535 - acc: 0.719 - ETA: 0s - loss: 1.1448 - acc: 0.721 - ETA: 0s - loss: 1.1398 - acc: 0.722 - ETA: 0s - loss: 1.1301 - acc: 0.723 - ETA: 0s - loss: 1.1234 - acc: 0.725 - ETA: 0s - loss: 1.1177 - acc: 0.726 - ETA: 0s - loss: 1.1119 - acc: 0.727 - ETA: 0s - loss: 1.1046 - acc: 0.728 - ETA: 0s - loss: 1.0956 - acc: 0.729 - ETA: 0s - loss: 1.0881 - acc: 0.731 - ETA: 0s - loss: 1.0814 - acc: 0.732 - ETA: 0s - loss: 1.0773 - acc: 0.732 - ETA: 0s - loss: 1.0707 - acc: 0.734 - ETA: 0s - loss: 1.0628 - acc: 0.735 - ETA: 0s - loss: 1.0555 - acc: 0.7362Epoch 00001: val\_loss improved from inf to 0.52757, saving model to saved\_models/weights.best.Xception.hdf5
6680/6680 [==============================] - 6s 909us/step - loss: 1.0538 - acc: 0.7365 - val\_loss: 0.5276 - val\_acc: 0.8335
Epoch 2/20
6600/6680 [============================>.] - ETA: 4s - loss: 0.2194 - acc: 0.900 - ETA: 4s - loss: 0.3783 - acc: 0.870 - ETA: 4s - loss: 0.3795 - acc: 0.861 - ETA: 4s - loss: 0.3404 - acc: 0.876 - ETA: 4s - loss: 0.3395 - acc: 0.876 - ETA: 4s - loss: 0.3681 - acc: 0.869 - ETA: 4s - loss: 0.3655 - acc: 0.872 - ETA: 4s - loss: 0.3678 - acc: 0.881 - ETA: 4s - loss: 0.3528 - acc: 0.885 - ETA: 4s - loss: 0.3711 - acc: 0.879 - ETA: 4s - loss: 0.3797 - acc: 0.873 - ETA: 4s - loss: 0.3928 - acc: 0.869 - ETA: 4s - loss: 0.4042 - acc: 0.871 - ETA: 4s - loss: 0.4092 - acc: 0.872 - ETA: 4s - loss: 0.4146 - acc: 0.867 - ETA: 4s - loss: 0.4059 - acc: 0.871 - ETA: 4s - loss: 0.4056 - acc: 0.873 - ETA: 4s - loss: 0.4145 - acc: 0.871 - ETA: 4s - loss: 0.4206 - acc: 0.867 - ETA: 4s - loss: 0.4210 - acc: 0.868 - ETA: 4s - loss: 0.4180 - acc: 0.868 - ETA: 4s - loss: 0.4193 - acc: 0.866 - ETA: 3s - loss: 0.4246 - acc: 0.865 - ETA: 3s - loss: 0.4292 - acc: 0.864 - ETA: 3s - loss: 0.4275 - acc: 0.865 - ETA: 3s - loss: 0.4210 - acc: 0.867 - ETA: 3s - loss: 0.4227 - acc: 0.866 - ETA: 3s - loss: 0.4198 - acc: 0.867 - ETA: 3s - loss: 0.4164 - acc: 0.868 - ETA: 3s - loss: 0.4169 - acc: 0.868 - ETA: 3s - loss: 0.4186 - acc: 0.867 - ETA: 3s - loss: 0.4164 - acc: 0.869 - ETA: 3s - loss: 0.4191 - acc: 0.867 - ETA: 3s - loss: 0.4175 - acc: 0.867 - ETA: 3s - loss: 0.4154 - acc: 0.868 - ETA: 3s - loss: 0.4142 - acc: 0.868 - ETA: 3s - loss: 0.4082 - acc: 0.870 - ETA: 3s - loss: 0.4076 - acc: 0.871 - ETA: 3s - loss: 0.4087 - acc: 0.871 - ETA: 3s - loss: 0.4090 - acc: 0.870 - ETA: 3s - loss: 0.4088 - acc: 0.870 - ETA: 3s - loss: 0.4079 - acc: 0.871 - ETA: 3s - loss: 0.4053 - acc: 0.871 - ETA: 2s - loss: 0.4036 - acc: 0.871 - ETA: 2s - loss: 0.4049 - acc: 0.870 - ETA: 2s - loss: 0.4056 - acc: 0.870 - ETA: 2s - loss: 0.4095 - acc: 0.869 - ETA: 2s - loss: 0.4119 - acc: 0.867 - ETA: 2s - loss: 0.4133 - acc: 0.866 - ETA: 2s - loss: 0.4141 - acc: 0.866 - ETA: 2s - loss: 0.4141 - acc: 0.866 - ETA: 2s - loss: 0.4142 - acc: 0.866 - ETA: 2s - loss: 0.4125 - acc: 0.866 - ETA: 2s - loss: 0.4148 - acc: 0.865 - ETA: 2s - loss: 0.4177 - acc: 0.864 - ETA: 2s - loss: 0.4180 - acc: 0.864 - ETA: 2s - loss: 0.4183 - acc: 0.864 - ETA: 2s - loss: 0.4163 - acc: 0.864 - ETA: 1s - loss: 0.4173 - acc: 0.863 - ETA: 1s - loss: 0.4167 - acc: 0.864 - ETA: 1s - loss: 0.4138 - acc: 0.864 - ETA: 1s - loss: 0.4147 - acc: 0.864 - ETA: 1s - loss: 0.4139 - acc: 0.864 - ETA: 1s - loss: 0.4121 - acc: 0.865 - ETA: 1s - loss: 0.4123 - acc: 0.864 - ETA: 1s - loss: 0.4124 - acc: 0.864 - ETA: 1s - loss: 0.4098 - acc: 0.864 - ETA: 1s - loss: 0.4082 - acc: 0.865 - ETA: 1s - loss: 0.4083 - acc: 0.865 - ETA: 1s - loss: 0.4076 - acc: 0.865 - ETA: 1s - loss: 0.4078 - acc: 0.865 - ETA: 1s - loss: 0.4083 - acc: 0.865 - ETA: 1s - loss: 0.4080 - acc: 0.866 - ETA: 1s - loss: 0.4072 - acc: 0.866 - ETA: 1s - loss: 0.4066 - acc: 0.867 - ETA: 0s - loss: 0.4051 - acc: 0.867 - ETA: 0s - loss: 0.4063 - acc: 0.867 - ETA: 0s - loss: 0.4050 - acc: 0.868 - ETA: 0s - loss: 0.4022 - acc: 0.868 - ETA: 0s - loss: 0.4013 - acc: 0.869 - ETA: 0s - loss: 0.4013 - acc: 0.869 - ETA: 0s - loss: 0.4010 - acc: 0.869 - ETA: 0s - loss: 0.4015 - acc: 0.869 - ETA: 0s - loss: 0.4001 - acc: 0.870 - ETA: 0s - loss: 0.3996 - acc: 0.870 - ETA: 0s - loss: 0.3980 - acc: 0.871 - ETA: 0s - loss: 0.3969 - acc: 0.871 - ETA: 0s - loss: 0.3983 - acc: 0.871 - ETA: 0s - loss: 0.3985 - acc: 0.871 - ETA: 0s - loss: 0.3964 - acc: 0.871 - ETA: 0s - loss: 0.3971 - acc: 0.8709Epoch 00002: val\_loss improved from 0.52757 to 0.52135, saving model to saved\_models/weights.best.Xception.hdf5
6680/6680 [==============================] - 6s 838us/step - loss: 0.3987 - acc: 0.8707 - val\_loss: 0.5213 - val\_acc: 0.8347
Epoch 3/20
6640/6680 [============================>.] - ETA: 6s - loss: 0.5150 - acc: 0.800 - ETA: 4s - loss: 0.5048 - acc: 0.860 - ETA: 4s - loss: 0.3472 - acc: 0.905 - ETA: 4s - loss: 0.3373 - acc: 0.903 - ETA: 4s - loss: 0.3211 - acc: 0.900 - ETA: 4s - loss: 0.3201 - acc: 0.897 - ETA: 4s - loss: 0.3101 - acc: 0.895 - ETA: 4s - loss: 0.2986 - acc: 0.901 - ETA: 4s - loss: 0.2828 - acc: 0.906 - ETA: 4s - loss: 0.2766 - acc: 0.907 - ETA: 4s - loss: 0.2746 - acc: 0.907 - ETA: 4s - loss: 0.2849 - acc: 0.905 - ETA: 4s - loss: 0.2940 - acc: 0.905 - ETA: 4s - loss: 0.2893 - acc: 0.903 - ETA: 4s - loss: 0.2980 - acc: 0.904 - ETA: 4s - loss: 0.2923 - acc: 0.906 - ETA: 4s - loss: 0.2975 - acc: 0.904 - ETA: 4s - loss: 0.2984 - acc: 0.900 - ETA: 4s - loss: 0.2956 - acc: 0.902 - ETA: 4s - loss: 0.3038 - acc: 0.901 - ETA: 3s - loss: 0.3127 - acc: 0.899 - ETA: 3s - loss: 0.3148 - acc: 0.898 - ETA: 3s - loss: 0.3113 - acc: 0.900 - ETA: 3s - loss: 0.3103 - acc: 0.901 - ETA: 3s - loss: 0.3059 - acc: 0.903 - ETA: 3s - loss: 0.3070 - acc: 0.901 - ETA: 3s - loss: 0.3145 - acc: 0.901 - ETA: 3s - loss: 0.3139 - acc: 0.901 - ETA: 3s - loss: 0.3121 - acc: 0.901 - ETA: 3s - loss: 0.3097 - acc: 0.902 - ETA: 3s - loss: 0.3093 - acc: 0.902 - ETA: 3s - loss: 0.3085 - acc: 0.901 - ETA: 3s - loss: 0.3045 - acc: 0.903 - ETA: 3s - loss: 0.3049 - acc: 0.903 - ETA: 3s - loss: 0.3061 - acc: 0.902 - ETA: 3s - loss: 0.3057 - acc: 0.902 - ETA: 2s - loss: 0.3033 - acc: 0.902 - ETA: 2s - loss: 0.3044 - acc: 0.902 - ETA: 2s - loss: 0.3024 - acc: 0.903 - ETA: 2s - loss: 0.3017 - acc: 0.903 - ETA: 2s - loss: 0.3021 - acc: 0.903 - ETA: 2s - loss: 0.3003 - acc: 0.904 - ETA: 2s - loss: 0.3031 - acc: 0.905 - ETA: 2s - loss: 0.3020 - acc: 0.905 - ETA: 2s - loss: 0.3007 - acc: 0.905 - ETA: 2s - loss: 0.2987 - acc: 0.905 - ETA: 2s - loss: 0.3022 - acc: 0.905 - ETA: 2s - loss: 0.3006 - acc: 0.905 - ETA: 2s - loss: 0.3007 - acc: 0.904 - ETA: 2s - loss: 0.3009 - acc: 0.905 - ETA: 2s - loss: 0.3023 - acc: 0.904 - ETA: 2s - loss: 0.3019 - acc: 0.905 - ETA: 2s - loss: 0.3026 - acc: 0.904 - ETA: 1s - loss: 0.3043 - acc: 0.903 - ETA: 1s - loss: 0.3068 - acc: 0.902 - ETA: 1s - loss: 0.3084 - acc: 0.901 - ETA: 1s - loss: 0.3109 - acc: 0.900 - ETA: 1s - loss: 0.3142 - acc: 0.900 - ETA: 1s - loss: 0.3132 - acc: 0.900 - ETA: 1s - loss: 0.3141 - acc: 0.900 - ETA: 1s - loss: 0.3149 - acc: 0.899 - ETA: 1s - loss: 0.3166 - acc: 0.899 - ETA: 1s - loss: 0.3179 - acc: 0.899 - ETA: 1s - loss: 0.3190 - acc: 0.898 - ETA: 1s - loss: 0.3184 - acc: 0.898 - ETA: 1s - loss: 0.3188 - acc: 0.898 - ETA: 1s - loss: 0.3198 - acc: 0.898 - ETA: 1s - loss: 0.3197 - acc: 0.898 - ETA: 1s - loss: 0.3179 - acc: 0.899 - ETA: 1s - loss: 0.3163 - acc: 0.899 - ETA: 0s - loss: 0.3182 - acc: 0.898 - ETA: 0s - loss: 0.3164 - acc: 0.899 - ETA: 0s - loss: 0.3140 - acc: 0.900 - ETA: 0s - loss: 0.3147 - acc: 0.900 - ETA: 0s - loss: 0.3161 - acc: 0.899 - ETA: 0s - loss: 0.3162 - acc: 0.900 - ETA: 0s - loss: 0.3185 - acc: 0.899 - ETA: 0s - loss: 0.3181 - acc: 0.899 - ETA: 0s - loss: 0.3215 - acc: 0.898 - ETA: 0s - loss: 0.3198 - acc: 0.899 - ETA: 0s - loss: 0.3183 - acc: 0.899 - ETA: 0s - loss: 0.3185 - acc: 0.898 - ETA: 0s - loss: 0.3195 - acc: 0.899 - ETA: 0s - loss: 0.3203 - acc: 0.898 - ETA: 0s - loss: 0.3186 - acc: 0.898 - ETA: 0s - loss: 0.3179 - acc: 0.899 - ETA: 0s - loss: 0.3182 - acc: 0.8991Epoch 00003: val\_loss improved from 0.52135 to 0.50882, saving model to saved\_models/weights.best.Xception.hdf5
6680/6680 [==============================] - 5s 797us/step - loss: 0.3210 - acc: 0.8985 - val\_loss: 0.5088 - val\_acc: 0.8299
Epoch 4/20
6640/6680 [============================>.] - ETA: 5s - loss: 0.2540 - acc: 0.900 - ETA: 4s - loss: 0.2664 - acc: 0.880 - ETA: 4s - loss: 0.2370 - acc: 0.888 - ETA: 4s - loss: 0.2030 - acc: 0.907 - ETA: 4s - loss: 0.2417 - acc: 0.905 - ETA: 4s - loss: 0.2681 - acc: 0.904 - ETA: 4s - loss: 0.2586 - acc: 0.904 - ETA: 4s - loss: 0.2448 - acc: 0.910 - ETA: 4s - loss: 0.2487 - acc: 0.912 - ETA: 4s - loss: 0.2582 - acc: 0.912 - ETA: 4s - loss: 0.2540 - acc: 0.915 - ETA: 4s - loss: 0.2510 - acc: 0.916 - ETA: 4s - loss: 0.2455 - acc: 0.920 - ETA: 4s - loss: 0.2412 - acc: 0.921 - ETA: 4s - loss: 0.2346 - acc: 0.923 - ETA: 3s - loss: 0.2321 - acc: 0.924 - ETA: 3s - loss: 0.2351 - acc: 0.924 - ETA: 3s - loss: 0.2287 - acc: 0.926 - ETA: 3s - loss: 0.2277 - acc: 0.927 - ETA: 3s - loss: 0.2254 - acc: 0.926 - ETA: 3s - loss: 0.2233 - acc: 0.927 - ETA: 3s - loss: 0.2273 - acc: 0.925 - ETA: 3s - loss: 0.2332 - acc: 0.924 - ETA: 3s - loss: 0.2327 - acc: 0.925 - ETA: 3s - loss: 0.2422 - acc: 0.923 - ETA: 3s - loss: 0.2396 - acc: 0.923 - ETA: 3s - loss: 0.2353 - acc: 0.924 - ETA: 3s - loss: 0.2361 - acc: 0.925 - ETA: 3s - loss: 0.2403 - acc: 0.923 - ETA: 3s - loss: 0.2400 - acc: 0.923 - ETA: 3s - loss: 0.2406 - acc: 0.921 - ETA: 3s - loss: 0.2392 - acc: 0.922 - ETA: 2s - loss: 0.2382 - acc: 0.922 - ETA: 2s - loss: 0.2424 - acc: 0.923 - ETA: 2s - loss: 0.2480 - acc: 0.921 - ETA: 2s - loss: 0.2497 - acc: 0.921 - ETA: 2s - loss: 0.2464 - acc: 0.922 - ETA: 2s - loss: 0.2450 - acc: 0.923 - ETA: 2s - loss: 0.2451 - acc: 0.922 - ETA: 2s - loss: 0.2446 - acc: 0.923 - ETA: 2s - loss: 0.2449 - acc: 0.921 - ETA: 2s - loss: 0.2433 - acc: 0.922 - ETA: 2s - loss: 0.2458 - acc: 0.921 - ETA: 2s - loss: 0.2459 - acc: 0.921 - ETA: 2s - loss: 0.2514 - acc: 0.920 - ETA: 2s - loss: 0.2520 - acc: 0.920 - ETA: 2s - loss: 0.2569 - acc: 0.919 - ETA: 2s - loss: 0.2578 - acc: 0.918 - ETA: 2s - loss: 0.2555 - acc: 0.919 - ETA: 2s - loss: 0.2546 - acc: 0.920 - ETA: 2s - loss: 0.2545 - acc: 0.920 - ETA: 1s - loss: 0.2576 - acc: 0.919 - ETA: 1s - loss: 0.2576 - acc: 0.919 - ETA: 1s - loss: 0.2563 - acc: 0.919 - ETA: 1s - loss: 0.2591 - acc: 0.918 - ETA: 1s - loss: 0.2569 - acc: 0.919 - ETA: 1s - loss: 0.2575 - acc: 0.918 - ETA: 1s - loss: 0.2574 - acc: 0.918 - ETA: 1s - loss: 0.2582 - acc: 0.917 - ETA: 1s - loss: 0.2587 - acc: 0.917 - ETA: 1s - loss: 0.2567 - acc: 0.918 - ETA: 1s - loss: 0.2571 - acc: 0.918 - ETA: 1s - loss: 0.2623 - acc: 0.916 - ETA: 1s - loss: 0.2651 - acc: 0.915 - ETA: 1s - loss: 0.2654 - acc: 0.916 - ETA: 1s - loss: 0.2647 - acc: 0.917 - ETA: 1s - loss: 0.2647 - acc: 0.917 - ETA: 1s - loss: 0.2636 - acc: 0.917 - ETA: 0s - loss: 0.2649 - acc: 0.917 - ETA: 0s - loss: 0.2645 - acc: 0.918 - ETA: 0s - loss: 0.2662 - acc: 0.917 - ETA: 0s - loss: 0.2642 - acc: 0.918 - ETA: 0s - loss: 0.2640 - acc: 0.918 - ETA: 0s - loss: 0.2639 - acc: 0.917 - ETA: 0s - loss: 0.2645 - acc: 0.917 - ETA: 0s - loss: 0.2663 - acc: 0.917 - ETA: 0s - loss: 0.2656 - acc: 0.917 - ETA: 0s - loss: 0.2670 - acc: 0.917 - ETA: 0s - loss: 0.2670 - acc: 0.917 - ETA: 0s - loss: 0.2664 - acc: 0.917 - ETA: 0s - loss: 0.2660 - acc: 0.917 - ETA: 0s - loss: 0.2684 - acc: 0.917 - ETA: 0s - loss: 0.2717 - acc: 0.916 - ETA: 0s - loss: 0.2714 - acc: 0.916 - ETA: 0s - loss: 0.2729 - acc: 0.9163Epoch 00004: val\_loss improved from 0.50882 to 0.49970, saving model to saved\_models/weights.best.Xception.hdf5
6680/6680 [==============================] - 5s 776us/step - loss: 0.2722 - acc: 0.9163 - val\_loss: 0.4997 - val\_acc: 0.8611
Epoch 5/20
6660/6680 [============================>.] - ETA: 5s - loss: 0.1764 - acc: 0.950 - ETA: 4s - loss: 0.1521 - acc: 0.940 - ETA: 4s - loss: 0.1681 - acc: 0.922 - ETA: 4s - loss: 0.2025 - acc: 0.907 - ETA: 4s - loss: 0.2241 - acc: 0.908 - ETA: 4s - loss: 0.2139 - acc: 0.911 - ETA: 4s - loss: 0.2149 - acc: 0.910 - ETA: 4s - loss: 0.2143 - acc: 0.910 - ETA: 4s - loss: 0.2120 - acc: 0.913 - ETA: 4s - loss: 0.2100 - acc: 0.916 - ETA: 4s - loss: 0.2109 - acc: 0.917 - ETA: 4s - loss: 0.2060 - acc: 0.921 - ETA: 4s - loss: 0.2201 - acc: 0.918 - ETA: 4s - loss: 0.2170 - acc: 0.918 - ETA: 3s - loss: 0.2172 - acc: 0.921 - ETA: 3s - loss: 0.2250 - acc: 0.921 - ETA: 3s - loss: 0.2293 - acc: 0.922 - ETA: 3s - loss: 0.2310 - acc: 0.923 - ETA: 3s - loss: 0.2348 - acc: 0.922 - ETA: 3s - loss: 0.2378 - acc: 0.922 - ETA: 3s - loss: 0.2382 - acc: 0.922 - ETA: 3s - loss: 0.2378 - acc: 0.923 - ETA: 3s - loss: 0.2329 - acc: 0.924 - ETA: 3s - loss: 0.2267 - acc: 0.926 - ETA: 3s - loss: 0.2260 - acc: 0.927 - ETA: 3s - loss: 0.2213 - acc: 0.929 - ETA: 3s - loss: 0.2240 - acc: 0.928 - ETA: 3s - loss: 0.2317 - acc: 0.927 - ETA: 3s - loss: 0.2318 - acc: 0.927 - ETA: 3s - loss: 0.2328 - acc: 0.926 - ETA: 3s - loss: 0.2290 - acc: 0.927 - ETA: 2s - loss: 0.2293 - acc: 0.927 - ETA: 2s - loss: 0.2308 - acc: 0.927 - ETA: 2s - loss: 0.2318 - acc: 0.926 - ETA: 2s - loss: 0.2282 - acc: 0.928 - ETA: 2s - loss: 0.2274 - acc: 0.928 - ETA: 2s - loss: 0.2259 - acc: 0.928 - ETA: 2s - loss: 0.2265 - acc: 0.928 - ETA: 2s - loss: 0.2360 - acc: 0.926 - ETA: 2s - loss: 0.2341 - acc: 0.926 - ETA: 2s - loss: 0.2348 - acc: 0.926 - ETA: 2s - loss: 0.2345 - acc: 0.926 - ETA: 2s - loss: 0.2312 - acc: 0.927 - ETA: 2s - loss: 0.2339 - acc: 0.927 - ETA: 2s - loss: 0.2345 - acc: 0.926 - ETA: 2s - loss: 0.2342 - acc: 0.927 - ETA: 2s - loss: 0.2345 - acc: 0.927 - ETA: 2s - loss: 0.2345 - acc: 0.926 - ETA: 2s - loss: 0.2336 - acc: 0.927 - ETA: 2s - loss: 0.2316 - acc: 0.928 - ETA: 1s - loss: 0.2319 - acc: 0.928 - ETA: 1s - loss: 0.2320 - acc: 0.928 - ETA: 1s - loss: 0.2317 - acc: 0.927 - ETA: 1s - loss: 0.2306 - acc: 0.927 - ETA: 1s - loss: 0.2312 - acc: 0.926 - ETA: 1s - loss: 0.2321 - acc: 0.926 - ETA: 1s - loss: 0.2335 - acc: 0.926 - ETA: 1s - loss: 0.2342 - acc: 0.926 - ETA: 1s - loss: 0.2367 - acc: 0.925 - ETA: 1s - loss: 0.2381 - acc: 0.925 - ETA: 1s - loss: 0.2374 - acc: 0.925 - ETA: 1s - loss: 0.2366 - acc: 0.926 - ETA: 1s - loss: 0.2361 - acc: 0.926 - ETA: 1s - loss: 0.2349 - acc: 0.926 - ETA: 1s - loss: 0.2349 - acc: 0.926 - ETA: 1s - loss: 0.2382 - acc: 0.925 - ETA: 1s - loss: 0.2381 - acc: 0.925 - ETA: 1s - loss: 0.2363 - acc: 0.925 - ETA: 0s - loss: 0.2380 - acc: 0.925 - ETA: 0s - loss: 0.2387 - acc: 0.924 - ETA: 0s - loss: 0.2408 - acc: 0.924 - ETA: 0s - loss: 0.2418 - acc: 0.923 - ETA: 0s - loss: 0.2404 - acc: 0.923 - ETA: 0s - loss: 0.2390 - acc: 0.924 - ETA: 0s - loss: 0.2391 - acc: 0.924 - ETA: 0s - loss: 0.2404 - acc: 0.923 - ETA: 0s - loss: 0.2448 - acc: 0.921 - ETA: 0s - loss: 0.2431 - acc: 0.922 - ETA: 0s - loss: 0.2417 - acc: 0.923 - ETA: 0s - loss: 0.2432 - acc: 0.922 - ETA: 0s - loss: 0.2446 - acc: 0.921 - ETA: 0s - loss: 0.2435 - acc: 0.922 - ETA: 0s - loss: 0.2422 - acc: 0.922 - ETA: 0s - loss: 0.2429 - acc: 0.922 - ETA: 0s - loss: 0.2433 - acc: 0.922 - ETA: 0s - loss: 0.2450 - acc: 0.922 - ETA: 0s - loss: 0.2434 - acc: 0.9231Epoch 00005: val\_loss did not improve
6680/6680 [==============================] - 5s 783us/step - loss: 0.2439 - acc: 0.9229 - val\_loss: 0.5133 - val\_acc: 0.8503
Epoch 6/20
6660/6680 [============================>.] - ETA: 4s - loss: 0.0829 - acc: 0.950 - ETA: 4s - loss: 0.2244 - acc: 0.950 - ETA: 4s - loss: 0.2255 - acc: 0.944 - ETA: 4s - loss: 0.1887 - acc: 0.950 - ETA: 5s - loss: 0.1661 - acc: 0.956 - ETA: 5s - loss: 0.1744 - acc: 0.952 - ETA: 5s - loss: 0.1705 - acc: 0.954 - ETA: 5s - loss: 0.1710 - acc: 0.952 - ETA: 5s - loss: 0.1657 - acc: 0.953 - ETA: 5s - loss: 0.1713 - acc: 0.951 - ETA: 4s - loss: 0.1747 - acc: 0.950 - ETA: 4s - loss: 0.1794 - acc: 0.950 - ETA: 4s - loss: 0.1790 - acc: 0.948 - ETA: 4s - loss: 0.1720 - acc: 0.950 - ETA: 4s - loss: 0.1861 - acc: 0.944 - ETA: 4s - loss: 0.1819 - acc: 0.946 - ETA: 4s - loss: 0.1953 - acc: 0.943 - ETA: 4s - loss: 0.1912 - acc: 0.944 - ETA: 4s - loss: 0.1864 - acc: 0.944 - ETA: 4s - loss: 0.1885 - acc: 0.941 - ETA: 4s - loss: 0.1868 - acc: 0.942 - ETA: 4s - loss: 0.1855 - acc: 0.942 - ETA: 4s - loss: 0.1898 - acc: 0.941 - ETA: 4s - loss: 0.1901 - acc: 0.942 - ETA: 3s - loss: 0.1853 - acc: 0.942 - ETA: 3s - loss: 0.1814 - acc: 0.944 - ETA: 3s - loss: 0.1787 - acc: 0.945 - ETA: 3s - loss: 0.1790 - acc: 0.945 - ETA: 3s - loss: 0.1779 - acc: 0.945 - ETA: 3s - loss: 0.1755 - acc: 0.946 - ETA: 3s - loss: 0.1758 - acc: 0.945 - ETA: 3s - loss: 0.1727 - acc: 0.946 - ETA: 3s - loss: 0.1739 - acc: 0.946 - ETA: 3s - loss: 0.1776 - acc: 0.945 - ETA: 3s - loss: 0.1811 - acc: 0.945 - ETA: 3s - loss: 0.1807 - acc: 0.945 - ETA: 3s - loss: 0.1769 - acc: 0.946 - ETA: 3s - loss: 0.1770 - acc: 0.945 - ETA: 2s - loss: 0.1822 - acc: 0.943 - ETA: 2s - loss: 0.1869 - acc: 0.942 - ETA: 2s - loss: 0.1887 - acc: 0.941 - ETA: 2s - loss: 0.1867 - acc: 0.942 - ETA: 2s - loss: 0.1924 - acc: 0.941 - ETA: 2s - loss: 0.1921 - acc: 0.942 - ETA: 2s - loss: 0.1901 - acc: 0.943 - ETA: 2s - loss: 0.1941 - acc: 0.942 - ETA: 2s - loss: 0.1931 - acc: 0.942 - ETA: 2s - loss: 0.1918 - acc: 0.942 - ETA: 2s - loss: 0.1905 - acc: 0.942 - ETA: 2s - loss: 0.1902 - acc: 0.942 - ETA: 2s - loss: 0.1913 - acc: 0.941 - ETA: 2s - loss: 0.1926 - acc: 0.941 - ETA: 2s - loss: 0.1960 - acc: 0.939 - ETA: 2s - loss: 0.1967 - acc: 0.939 - ETA: 2s - loss: 0.2006 - acc: 0.938 - ETA: 1s - loss: 0.2052 - acc: 0.938 - ETA: 1s - loss: 0.2043 - acc: 0.938 - ETA: 1s - loss: 0.2043 - acc: 0.937 - ETA: 1s - loss: 0.2052 - acc: 0.936 - ETA: 1s - loss: 0.2062 - acc: 0.937 - ETA: 1s - loss: 0.2098 - acc: 0.936 - ETA: 1s - loss: 0.2128 - acc: 0.936 - ETA: 1s - loss: 0.2119 - acc: 0.936 - ETA: 1s - loss: 0.2112 - acc: 0.936 - ETA: 1s - loss: 0.2097 - acc: 0.936 - ETA: 1s - loss: 0.2084 - acc: 0.936 - ETA: 1s - loss: 0.2089 - acc: 0.937 - ETA: 1s - loss: 0.2103 - acc: 0.936 - ETA: 1s - loss: 0.2108 - acc: 0.936 - ETA: 1s - loss: 0.2106 - acc: 0.935 - ETA: 1s - loss: 0.2117 - acc: 0.935 - ETA: 0s - loss: 0.2102 - acc: 0.935 - ETA: 0s - loss: 0.2104 - acc: 0.935 - ETA: 0s - loss: 0.2100 - acc: 0.935 - ETA: 0s - loss: 0.2114 - acc: 0.935 - ETA: 0s - loss: 0.2102 - acc: 0.935 - ETA: 0s - loss: 0.2108 - acc: 0.935 - ETA: 0s - loss: 0.2123 - acc: 0.935 - ETA: 0s - loss: 0.2127 - acc: 0.935 - ETA: 0s - loss: 0.2132 - acc: 0.935 - ETA: 0s - loss: 0.2139 - acc: 0.934 - ETA: 0s - loss: 0.2135 - acc: 0.935 - ETA: 0s - loss: 0.2115 - acc: 0.935 - ETA: 0s - loss: 0.2127 - acc: 0.935 - ETA: 0s - loss: 0.2120 - acc: 0.935 - ETA: 0s - loss: 0.2138 - acc: 0.935 - ETA: 0s - loss: 0.2138 - acc: 0.935 - ETA: 0s - loss: 0.2173 - acc: 0.9351Epoch 00006: val\_loss did not improve
6680/6680 [==============================] - 5s 795us/step - loss: 0.2173 - acc: 0.9352 - val\_loss: 0.5525 - val\_acc: 0.8647
Epoch 7/20
6660/6680 [============================>.] - ETA: 4s - loss: 0.0970 - acc: 0.950 - ETA: 4s - loss: 0.1757 - acc: 0.920 - ETA: 4s - loss: 0.1474 - acc: 0.944 - ETA: 4s - loss: 0.1780 - acc: 0.950 - ETA: 4s - loss: 0.1936 - acc: 0.944 - ETA: 4s - loss: 0.1747 - acc: 0.950 - ETA: 4s - loss: 0.1698 - acc: 0.950 - ETA: 4s - loss: 0.1577 - acc: 0.951 - ETA: 4s - loss: 0.1732 - acc: 0.948 - ETA: 4s - loss: 0.1715 - acc: 0.948 - ETA: 4s - loss: 0.1637 - acc: 0.950 - ETA: 4s - loss: 0.1655 - acc: 0.950 - ETA: 4s - loss: 0.1671 - acc: 0.950 - ETA: 4s - loss: 0.1595 - acc: 0.951 - ETA: 3s - loss: 0.1658 - acc: 0.950 - ETA: 3s - loss: 0.1764 - acc: 0.950 - ETA: 3s - loss: 0.1833 - acc: 0.949 - ETA: 3s - loss: 0.1789 - acc: 0.950 - ETA: 3s - loss: 0.1785 - acc: 0.950 - ETA: 3s - loss: 0.1865 - acc: 0.948 - ETA: 3s - loss: 0.1910 - acc: 0.946 - ETA: 3s - loss: 0.1893 - acc: 0.945 - ETA: 3s - loss: 0.1864 - acc: 0.947 - ETA: 3s - loss: 0.1826 - acc: 0.947 - ETA: 3s - loss: 0.1886 - acc: 0.946 - ETA: 3s - loss: 0.1864 - acc: 0.947 - ETA: 3s - loss: 0.1879 - acc: 0.948 - ETA: 3s - loss: 0.1846 - acc: 0.948 - ETA: 3s - loss: 0.1846 - acc: 0.948 - ETA: 3s - loss: 0.1850 - acc: 0.947 - ETA: 3s - loss: 0.1870 - acc: 0.947 - ETA: 2s - loss: 0.1837 - acc: 0.948 - ETA: 2s - loss: 0.1842 - acc: 0.948 - ETA: 2s - loss: 0.1849 - acc: 0.946 - ETA: 2s - loss: 0.1825 - acc: 0.946 - ETA: 2s - loss: 0.1809 - acc: 0.946 - ETA: 2s - loss: 0.1804 - acc: 0.947 - ETA: 2s - loss: 0.1824 - acc: 0.946 - ETA: 2s - loss: 0.1821 - acc: 0.946 - ETA: 2s - loss: 0.1826 - acc: 0.945 - ETA: 2s - loss: 0.1853 - acc: 0.944 - ETA: 2s - loss: 0.1831 - acc: 0.945 - ETA: 2s - loss: 0.1812 - acc: 0.946 - ETA: 2s - loss: 0.1830 - acc: 0.945 - ETA: 2s - loss: 0.1825 - acc: 0.945 - ETA: 2s - loss: 0.1823 - acc: 0.946 - ETA: 2s - loss: 0.1798 - acc: 0.946 - ETA: 2s - loss: 0.1815 - acc: 0.946 - ETA: 2s - loss: 0.1799 - acc: 0.946 - ETA: 1s - loss: 0.1788 - acc: 0.946 - ETA: 1s - loss: 0.1807 - acc: 0.946 - ETA: 1s - loss: 0.1811 - acc: 0.946 - ETA: 1s - loss: 0.1840 - acc: 0.945 - ETA: 1s - loss: 0.1850 - acc: 0.945 - ETA: 1s - loss: 0.1827 - acc: 0.945 - ETA: 1s - loss: 0.1814 - acc: 0.946 - ETA: 1s - loss: 0.1819 - acc: 0.945 - ETA: 1s - loss: 0.1805 - acc: 0.946 - ETA: 1s - loss: 0.1826 - acc: 0.945 - ETA: 1s - loss: 0.1825 - acc: 0.945 - ETA: 1s - loss: 0.1830 - acc: 0.945 - ETA: 1s - loss: 0.1819 - acc: 0.945 - ETA: 1s - loss: 0.1816 - acc: 0.945 - ETA: 1s - loss: 0.1826 - acc: 0.945 - ETA: 1s - loss: 0.1833 - acc: 0.944 - ETA: 1s - loss: 0.1866 - acc: 0.943 - ETA: 1s - loss: 0.1874 - acc: 0.944 - ETA: 1s - loss: 0.1875 - acc: 0.943 - ETA: 0s - loss: 0.1887 - acc: 0.943 - ETA: 0s - loss: 0.1889 - acc: 0.943 - ETA: 0s - loss: 0.1875 - acc: 0.943 - ETA: 0s - loss: 0.1865 - acc: 0.943 - ETA: 0s - loss: 0.1856 - acc: 0.944 - ETA: 0s - loss: 0.1850 - acc: 0.943 - ETA: 0s - loss: 0.1853 - acc: 0.944 - ETA: 0s - loss: 0.1841 - acc: 0.944 - ETA: 0s - loss: 0.1855 - acc: 0.944 - ETA: 0s - loss: 0.1856 - acc: 0.943 - ETA: 0s - loss: 0.1851 - acc: 0.943 - ETA: 0s - loss: 0.1854 - acc: 0.943 - ETA: 0s - loss: 0.1879 - acc: 0.943 - ETA: 0s - loss: 0.1925 - acc: 0.943 - ETA: 0s - loss: 0.1929 - acc: 0.942 - ETA: 0s - loss: 0.1934 - acc: 0.942 - ETA: 0s - loss: 0.1960 - acc: 0.941 - ETA: 0s - loss: 0.1964 - acc: 0.9413Epoch 00007: val\_loss did not improve
6680/6680 [==============================] - 5s 771us/step - loss: 0.1959 - acc: 0.9413 - val\_loss: 0.5337 - val\_acc: 0.8611
Epoch 8/20
6660/6680 [============================>.] - ETA: 5s - loss: 0.0473 - acc: 1.000 - ETA: 4s - loss: 0.2389 - acc: 0.950 - ETA: 4s - loss: 0.1863 - acc: 0.955 - ETA: 4s - loss: 0.1505 - acc: 0.953 - ETA: 4s - loss: 0.1798 - acc: 0.950 - ETA: 4s - loss: 0.1633 - acc: 0.950 - ETA: 4s - loss: 0.1558 - acc: 0.952 - ETA: 4s - loss: 0.1396 - acc: 0.958 - ETA: 4s - loss: 0.1595 - acc: 0.951 - ETA: 4s - loss: 0.1628 - acc: 0.950 - ETA: 4s - loss: 0.1632 - acc: 0.950 - ETA: 4s - loss: 0.1632 - acc: 0.950 - ETA: 4s - loss: 0.1611 - acc: 0.950 - ETA: 4s - loss: 0.1546 - acc: 0.950 - ETA: 4s - loss: 0.1561 - acc: 0.947 - ETA: 4s - loss: 0.1580 - acc: 0.945 - ETA: 3s - loss: 0.1582 - acc: 0.946 - ETA: 3s - loss: 0.1612 - acc: 0.945 - ETA: 3s - loss: 0.1569 - acc: 0.946 - ETA: 3s - loss: 0.1596 - acc: 0.946 - ETA: 3s - loss: 0.1641 - acc: 0.946 - ETA: 3s - loss: 0.1604 - acc: 0.947 - ETA: 3s - loss: 0.1580 - acc: 0.947 - ETA: 3s - loss: 0.1635 - acc: 0.946 - ETA: 3s - loss: 0.1615 - acc: 0.947 - ETA: 3s - loss: 0.1596 - acc: 0.948 - ETA: 3s - loss: 0.1581 - acc: 0.950 - ETA: 3s - loss: 0.1585 - acc: 0.950 - ETA: 3s - loss: 0.1677 - acc: 0.949 - ETA: 3s - loss: 0.1675 - acc: 0.949 - ETA: 3s - loss: 0.1669 - acc: 0.948 - ETA: 3s - loss: 0.1680 - acc: 0.947 - ETA: 2s - loss: 0.1685 - acc: 0.948 - ETA: 2s - loss: 0.1687 - acc: 0.947 - ETA: 2s - loss: 0.1688 - acc: 0.947 - ETA: 2s - loss: 0.1720 - acc: 0.946 - ETA: 2s - loss: 0.1724 - acc: 0.946 - ETA: 2s - loss: 0.1757 - acc: 0.945 - ETA: 2s - loss: 0.1775 - acc: 0.944 - ETA: 2s - loss: 0.1811 - acc: 0.943 - ETA: 2s - loss: 0.1843 - acc: 0.943 - ETA: 2s - loss: 0.1820 - acc: 0.943 - ETA: 2s - loss: 0.1843 - acc: 0.942 - ETA: 2s - loss: 0.1826 - acc: 0.943 - ETA: 2s - loss: 0.1856 - acc: 0.942 - ETA: 2s - loss: 0.1843 - acc: 0.943 - ETA: 2s - loss: 0.1837 - acc: 0.943 - ETA: 2s - loss: 0.1833 - acc: 0.942 - ETA: 2s - loss: 0.1855 - acc: 0.942 - ETA: 1s - loss: 0.1837 - acc: 0.942 - ETA: 1s - loss: 0.1835 - acc: 0.942 - ETA: 1s - loss: 0.1831 - acc: 0.942 - ETA: 1s - loss: 0.1830 - acc: 0.942 - ETA: 1s - loss: 0.1818 - acc: 0.943 - ETA: 1s - loss: 0.1827 - acc: 0.942 - ETA: 1s - loss: 0.1841 - acc: 0.942 - ETA: 1s - loss: 0.1844 - acc: 0.942 - ETA: 1s - loss: 0.1822 - acc: 0.943 - ETA: 1s - loss: 0.1810 - acc: 0.943 - ETA: 1s - loss: 0.1808 - acc: 0.943 - ETA: 1s - loss: 0.1805 - acc: 0.943 - ETA: 1s - loss: 0.1826 - acc: 0.943 - ETA: 1s - loss: 0.1819 - acc: 0.943 - ETA: 1s - loss: 0.1818 - acc: 0.942 - ETA: 1s - loss: 0.1824 - acc: 0.943 - ETA: 1s - loss: 0.1809 - acc: 0.943 - ETA: 0s - loss: 0.1819 - acc: 0.943 - ETA: 0s - loss: 0.1819 - acc: 0.943 - ETA: 0s - loss: 0.1817 - acc: 0.943 - ETA: 0s - loss: 0.1814 - acc: 0.943 - ETA: 0s - loss: 0.1801 - acc: 0.944 - ETA: 0s - loss: 0.1793 - acc: 0.944 - ETA: 0s - loss: 0.1789 - acc: 0.944 - ETA: 0s - loss: 0.1775 - acc: 0.944 - ETA: 0s - loss: 0.1764 - acc: 0.944 - ETA: 0s - loss: 0.1762 - acc: 0.945 - ETA: 0s - loss: 0.1763 - acc: 0.944 - ETA: 0s - loss: 0.1791 - acc: 0.943 - ETA: 0s - loss: 0.1784 - acc: 0.943 - ETA: 0s - loss: 0.1781 - acc: 0.943 - ETA: 0s - loss: 0.1778 - acc: 0.943 - ETA: 0s - loss: 0.1776 - acc: 0.943 - ETA: 0s - loss: 0.1763 - acc: 0.943 - ETA: 0s - loss: 0.1770 - acc: 0.9437Epoch 00008: val\_loss did not improve
6680/6680 [==============================] - 5s 763us/step - loss: 0.1770 - acc: 0.9437 - val\_loss: 0.5453 - val\_acc: 0.8623
Epoch 9/20
6640/6680 [============================>.] - ETA: 5s - loss: 0.2641 - acc: 0.950 - ETA: 5s - loss: 0.2590 - acc: 0.937 - ETA: 5s - loss: 0.1892 - acc: 0.962 - ETA: 5s - loss: 0.1689 - acc: 0.959 - ETA: 5s - loss: 0.1586 - acc: 0.963 - ETA: 5s - loss: 0.1657 - acc: 0.961 - ETA: 5s - loss: 0.1702 - acc: 0.961 - ETA: 4s - loss: 0.1655 - acc: 0.961 - ETA: 4s - loss: 0.1501 - acc: 0.965 - ETA: 4s - loss: 0.1405 - acc: 0.967 - ETA: 4s - loss: 0.1360 - acc: 0.967 - ETA: 4s - loss: 0.1402 - acc: 0.965 - ETA: 4s - loss: 0.1471 - acc: 0.963 - ETA: 4s - loss: 0.1470 - acc: 0.963 - ETA: 4s - loss: 0.1393 - acc: 0.965 - ETA: 4s - loss: 0.1356 - acc: 0.965 - ETA: 4s - loss: 0.1407 - acc: 0.963 - ETA: 4s - loss: 0.1467 - acc: 0.960 - ETA: 4s - loss: 0.1575 - acc: 0.957 - ETA: 4s - loss: 0.1641 - acc: 0.955 - ETA: 4s - loss: 0.1664 - acc: 0.954 - ETA: 4s - loss: 0.1747 - acc: 0.954 - ETA: 3s - loss: 0.1692 - acc: 0.956 - ETA: 3s - loss: 0.1701 - acc: 0.954 - ETA: 3s - loss: 0.1671 - acc: 0.955 - ETA: 3s - loss: 0.1663 - acc: 0.954 - ETA: 3s - loss: 0.1666 - acc: 0.954 - ETA: 3s - loss: 0.1624 - acc: 0.955 - ETA: 3s - loss: 0.1607 - acc: 0.956 - ETA: 3s - loss: 0.1600 - acc: 0.955 - ETA: 3s - loss: 0.1558 - acc: 0.957 - ETA: 3s - loss: 0.1527 - acc: 0.958 - ETA: 3s - loss: 0.1532 - acc: 0.958 - ETA: 3s - loss: 0.1527 - acc: 0.957 - ETA: 3s - loss: 0.1527 - acc: 0.957 - ETA: 3s - loss: 0.1556 - acc: 0.956 - ETA: 3s - loss: 0.1569 - acc: 0.956 - ETA: 2s - loss: 0.1558 - acc: 0.956 - ETA: 2s - loss: 0.1573 - acc: 0.955 - ETA: 2s - loss: 0.1573 - acc: 0.956 - ETA: 2s - loss: 0.1561 - acc: 0.956 - ETA: 2s - loss: 0.1560 - acc: 0.956 - ETA: 2s - loss: 0.1554 - acc: 0.956 - ETA: 2s - loss: 0.1590 - acc: 0.956 - ETA: 2s - loss: 0.1577 - acc: 0.956 - ETA: 2s - loss: 0.1575 - acc: 0.956 - ETA: 2s - loss: 0.1595 - acc: 0.956 - ETA: 2s - loss: 0.1598 - acc: 0.956 - ETA: 2s - loss: 0.1602 - acc: 0.956 - ETA: 2s - loss: 0.1612 - acc: 0.955 - ETA: 2s - loss: 0.1607 - acc: 0.955 - ETA: 2s - loss: 0.1666 - acc: 0.953 - ETA: 2s - loss: 0.1659 - acc: 0.953 - ETA: 1s - loss: 0.1652 - acc: 0.952 - ETA: 1s - loss: 0.1644 - acc: 0.952 - ETA: 1s - loss: 0.1656 - acc: 0.952 - ETA: 1s - loss: 0.1665 - acc: 0.952 - ETA: 1s - loss: 0.1674 - acc: 0.952 - ETA: 1s - loss: 0.1663 - acc: 0.952 - ETA: 1s - loss: 0.1650 - acc: 0.952 - ETA: 1s - loss: 0.1645 - acc: 0.952 - ETA: 1s - loss: 0.1623 - acc: 0.953 - ETA: 1s - loss: 0.1619 - acc: 0.953 - ETA: 1s - loss: 0.1630 - acc: 0.952 - ETA: 1s - loss: 0.1620 - acc: 0.952 - ETA: 1s - loss: 0.1615 - acc: 0.952 - ETA: 1s - loss: 0.1604 - acc: 0.953 - ETA: 1s - loss: 0.1606 - acc: 0.952 - ETA: 1s - loss: 0.1590 - acc: 0.952 - ETA: 1s - loss: 0.1604 - acc: 0.952 - ETA: 1s - loss: 0.1594 - acc: 0.952 - ETA: 0s - loss: 0.1591 - acc: 0.952 - ETA: 0s - loss: 0.1602 - acc: 0.952 - ETA: 0s - loss: 0.1603 - acc: 0.951 - ETA: 0s - loss: 0.1604 - acc: 0.951 - ETA: 0s - loss: 0.1625 - acc: 0.951 - ETA: 0s - loss: 0.1614 - acc: 0.951 - ETA: 0s - loss: 0.1609 - acc: 0.951 - ETA: 0s - loss: 0.1605 - acc: 0.951 - ETA: 0s - loss: 0.1602 - acc: 0.951 - ETA: 0s - loss: 0.1597 - acc: 0.951 - ETA: 0s - loss: 0.1599 - acc: 0.951 - ETA: 0s - loss: 0.1598 - acc: 0.951 - ETA: 0s - loss: 0.1599 - acc: 0.951 - ETA: 0s - loss: 0.1592 - acc: 0.951 - ETA: 0s - loss: 0.1580 - acc: 0.951 - ETA: 0s - loss: 0.1575 - acc: 0.951 - ETA: 0s - loss: 0.1577 - acc: 0.951 - ETA: 0s - loss: 0.1570 - acc: 0.9521Epoch 00009: val\_loss did not improve
6680/6680 [==============================] - 5s 806us/step - loss: 0.1580 - acc: 0.9519 - val\_loss: 0.5691 - val\_acc: 0.8575
Epoch 10/20
6600/6680 [============================>.] - ETA: 5s - loss: 0.1398 - acc: 0.950 - ETA: 4s - loss: 0.0953 - acc: 0.970 - ETA: 4s - loss: 0.0934 - acc: 0.966 - ETA: 4s - loss: 0.0992 - acc: 0.961 - ETA: 4s - loss: 0.1094 - acc: 0.952 - ETA: 4s - loss: 0.1231 - acc: 0.952 - ETA: 4s - loss: 0.1158 - acc: 0.956 - ETA: 4s - loss: 0.1255 - acc: 0.953 - ETA: 4s - loss: 0.1226 - acc: 0.957 - ETA: 4s - loss: 0.1208 - acc: 0.960 - ETA: 4s - loss: 0.1249 - acc: 0.962 - ETA: 4s - loss: 0.1284 - acc: 0.962 - ETA: 4s - loss: 0.1343 - acc: 0.961 - ETA: 4s - loss: 0.1439 - acc: 0.958 - ETA: 4s - loss: 0.1485 - acc: 0.953 - ETA: 4s - loss: 0.1464 - acc: 0.954 - ETA: 3s - loss: 0.1571 - acc: 0.953 - ETA: 3s - loss: 0.1575 - acc: 0.953 - ETA: 3s - loss: 0.1577 - acc: 0.952 - ETA: 3s - loss: 0.1560 - acc: 0.952 - ETA: 3s - loss: 0.1519 - acc: 0.953 - ETA: 3s - loss: 0.1499 - acc: 0.954 - ETA: 3s - loss: 0.1478 - acc: 0.954 - ETA: 3s - loss: 0.1564 - acc: 0.953 - ETA: 3s - loss: 0.1586 - acc: 0.953 - ETA: 3s - loss: 0.1616 - acc: 0.951 - ETA: 3s - loss: 0.1594 - acc: 0.951 - ETA: 3s - loss: 0.1626 - acc: 0.950 - ETA: 3s - loss: 0.1607 - acc: 0.951 - ETA: 3s - loss: 0.1588 - acc: 0.951 - ETA: 3s - loss: 0.1579 - acc: 0.951 - ETA: 3s - loss: 0.1600 - acc: 0.951 - ETA: 3s - loss: 0.1599 - acc: 0.950 - ETA: 3s - loss: 0.1585 - acc: 0.951 - ETA: 2s - loss: 0.1564 - acc: 0.951 - ETA: 2s - loss: 0.1573 - acc: 0.951 - ETA: 2s - loss: 0.1614 - acc: 0.951 - ETA: 2s - loss: 0.1609 - acc: 0.951 - ETA: 2s - loss: 0.1586 - acc: 0.952 - ETA: 2s - loss: 0.1584 - acc: 0.951 - ETA: 2s - loss: 0.1564 - acc: 0.952 - ETA: 2s - loss: 0.1549 - acc: 0.952 - ETA: 2s - loss: 0.1532 - acc: 0.953 - ETA: 2s - loss: 0.1539 - acc: 0.954 - ETA: 2s - loss: 0.1577 - acc: 0.953 - ETA: 2s - loss: 0.1581 - acc: 0.954 - ETA: 2s - loss: 0.1570 - acc: 0.954 - ETA: 2s - loss: 0.1548 - acc: 0.955 - ETA: 2s - loss: 0.1525 - acc: 0.956 - ETA: 2s - loss: 0.1528 - acc: 0.955 - ETA: 2s - loss: 0.1511 - acc: 0.955 - ETA: 1s - loss: 0.1490 - acc: 0.956 - ETA: 1s - loss: 0.1493 - acc: 0.956 - ETA: 1s - loss: 0.1504 - acc: 0.955 - ETA: 1s - loss: 0.1484 - acc: 0.956 - ETA: 1s - loss: 0.1491 - acc: 0.956 - ETA: 1s - loss: 0.1477 - acc: 0.957 - ETA: 1s - loss: 0.1462 - acc: 0.957 - ETA: 1s - loss: 0.1458 - acc: 0.957 - ETA: 1s - loss: 0.1457 - acc: 0.956 - ETA: 1s - loss: 0.1461 - acc: 0.956 - ETA: 1s - loss: 0.1454 - acc: 0.956 - ETA: 1s - loss: 0.1470 - acc: 0.956 - ETA: 1s - loss: 0.1471 - acc: 0.956 - ETA: 1s - loss: 0.1469 - acc: 0.955 - ETA: 1s - loss: 0.1453 - acc: 0.956 - ETA: 1s - loss: 0.1469 - acc: 0.956 - ETA: 0s - loss: 0.1461 - acc: 0.956 - ETA: 0s - loss: 0.1469 - acc: 0.955 - ETA: 0s - loss: 0.1486 - acc: 0.955 - ETA: 0s - loss: 0.1511 - acc: 0.955 - ETA: 0s - loss: 0.1508 - acc: 0.955 - ETA: 0s - loss: 0.1505 - acc: 0.955 - ETA: 0s - loss: 0.1499 - acc: 0.955 - ETA: 0s - loss: 0.1501 - acc: 0.955 - ETA: 0s - loss: 0.1513 - acc: 0.955 - ETA: 0s - loss: 0.1520 - acc: 0.955 - ETA: 0s - loss: 0.1510 - acc: 0.955 - ETA: 0s - loss: 0.1500 - acc: 0.955 - ETA: 0s - loss: 0.1517 - acc: 0.955 - ETA: 0s - loss: 0.1513 - acc: 0.955 - ETA: 0s - loss: 0.1515 - acc: 0.955 - ETA: 0s - loss: 0.1508 - acc: 0.955 - ETA: 0s - loss: 0.1504 - acc: 0.9556Epoch 00010: val\_loss did not improve
6680/6680 [==============================] - 5s 773us/step - loss: 0.1495 - acc: 0.9558 - val\_loss: 0.5412 - val\_acc: 0.8539
Epoch 11/20
6620/6680 [============================>.] - ETA: 4s - loss: 0.1689 - acc: 0.950 - ETA: 4s - loss: 0.1223 - acc: 0.960 - ETA: 4s - loss: 0.1005 - acc: 0.972 - ETA: 4s - loss: 0.1202 - acc: 0.965 - ETA: 4s - loss: 0.1027 - acc: 0.970 - ETA: 4s - loss: 0.1017 - acc: 0.966 - ETA: 4s - loss: 0.0888 - acc: 0.972 - ETA: 4s - loss: 0.0934 - acc: 0.972 - ETA: 4s - loss: 0.1009 - acc: 0.971 - ETA: 4s - loss: 0.1085 - acc: 0.967 - ETA: 4s - loss: 0.1201 - acc: 0.963 - ETA: 4s - loss: 0.1161 - acc: 0.964 - ETA: 4s - loss: 0.1128 - acc: 0.964 - ETA: 3s - loss: 0.1179 - acc: 0.962 - ETA: 3s - loss: 0.1138 - acc: 0.963 - ETA: 3s - loss: 0.1103 - acc: 0.963 - ETA: 3s - loss: 0.1099 - acc: 0.962 - ETA: 3s - loss: 0.1156 - acc: 0.962 - ETA: 3s - loss: 0.1128 - acc: 0.962 - ETA: 3s - loss: 0.1089 - acc: 0.963 - ETA: 3s - loss: 0.1107 - acc: 0.962 - ETA: 3s - loss: 0.1081 - acc: 0.962 - ETA: 3s - loss: 0.1092 - acc: 0.961 - ETA: 3s - loss: 0.1079 - acc: 0.961 - ETA: 3s - loss: 0.1119 - acc: 0.962 - ETA: 3s - loss: 0.1109 - acc: 0.962 - ETA: 3s - loss: 0.1075 - acc: 0.963 - ETA: 3s - loss: 0.1141 - acc: 0.962 - ETA: 3s - loss: 0.1131 - acc: 0.962 - ETA: 3s - loss: 0.1155 - acc: 0.960 - ETA: 3s - loss: 0.1171 - acc: 0.960 - ETA: 3s - loss: 0.1195 - acc: 0.959 - ETA: 2s - loss: 0.1207 - acc: 0.959 - ETA: 2s - loss: 0.1198 - acc: 0.959 - ETA: 2s - loss: 0.1224 - acc: 0.958 - ETA: 2s - loss: 0.1302 - acc: 0.957 - ETA: 2s - loss: 0.1295 - acc: 0.957 - ETA: 2s - loss: 0.1272 - acc: 0.958 - ETA: 2s - loss: 0.1265 - acc: 0.957 - ETA: 2s - loss: 0.1258 - acc: 0.958 - ETA: 2s - loss: 0.1284 - acc: 0.957 - ETA: 2s - loss: 0.1300 - acc: 0.957 - ETA: 2s - loss: 0.1283 - acc: 0.957 - ETA: 2s - loss: 0.1309 - acc: 0.957 - ETA: 2s - loss: 0.1300 - acc: 0.958 - ETA: 2s - loss: 0.1295 - acc: 0.959 - ETA: 2s - loss: 0.1281 - acc: 0.959 - ETA: 2s - loss: 0.1287 - acc: 0.959 - ETA: 2s - loss: 0.1278 - acc: 0.959 - ETA: 2s - loss: 0.1324 - acc: 0.958 - ETA: 2s - loss: 0.1308 - acc: 0.959 - ETA: 1s - loss: 0.1294 - acc: 0.960 - ETA: 1s - loss: 0.1305 - acc: 0.960 - ETA: 1s - loss: 0.1345 - acc: 0.960 - ETA: 1s - loss: 0.1334 - acc: 0.960 - ETA: 1s - loss: 0.1365 - acc: 0.959 - ETA: 1s - loss: 0.1388 - acc: 0.958 - ETA: 1s - loss: 0.1385 - acc: 0.959 - ETA: 1s - loss: 0.1375 - acc: 0.959 - ETA: 1s - loss: 0.1370 - acc: 0.959 - ETA: 1s - loss: 0.1360 - acc: 0.959 - ETA: 1s - loss: 0.1376 - acc: 0.959 - ETA: 1s - loss: 0.1374 - acc: 0.958 - ETA: 1s - loss: 0.1392 - acc: 0.958 - ETA: 1s - loss: 0.1385 - acc: 0.958 - ETA: 1s - loss: 0.1378 - acc: 0.958 - ETA: 1s - loss: 0.1379 - acc: 0.958 - ETA: 1s - loss: 0.1371 - acc: 0.958 - ETA: 0s - loss: 0.1360 - acc: 0.958 - ETA: 0s - loss: 0.1354 - acc: 0.958 - ETA: 0s - loss: 0.1360 - acc: 0.958 - ETA: 0s - loss: 0.1406 - acc: 0.957 - ETA: 0s - loss: 0.1418 - acc: 0.957 - ETA: 0s - loss: 0.1416 - acc: 0.957 - ETA: 0s - loss: 0.1410 - acc: 0.957 - ETA: 0s - loss: 0.1406 - acc: 0.957 - ETA: 0s - loss: 0.1397 - acc: 0.957 - ETA: 0s - loss: 0.1391 - acc: 0.957 - ETA: 0s - loss: 0.1385 - acc: 0.957 - ETA: 0s - loss: 0.1395 - acc: 0.957 - ETA: 0s - loss: 0.1389 - acc: 0.957 - ETA: 0s - loss: 0.1380 - acc: 0.957 - ETA: 0s - loss: 0.1385 - acc: 0.957 - ETA: 0s - loss: 0.1379 - acc: 0.957 - ETA: 0s - loss: 0.1382 - acc: 0.957 - ETA: 0s - loss: 0.1383 - acc: 0.9577Epoch 00011: val\_loss did not improve
6680/6680 [==============================] - 5s 780us/step - loss: 0.1391 - acc: 0.9576 - val\_loss: 0.5823 - val\_acc: 0.8539
Epoch 12/20
6620/6680 [============================>.] - ETA: 4s - loss: 0.0042 - acc: 1.000 - ETA: 4s - loss: 0.1799 - acc: 0.960 - ETA: 4s - loss: 0.1369 - acc: 0.961 - ETA: 5s - loss: 0.1452 - acc: 0.954 - ETA: 5s - loss: 0.1147 - acc: 0.965 - ETA: 5s - loss: 0.1148 - acc: 0.963 - ETA: 5s - loss: 0.1012 - acc: 0.968 - ETA: 5s - loss: 0.0907 - acc: 0.972 - ETA: 5s - loss: 0.0832 - acc: 0.974 - ETA: 4s - loss: 0.0896 - acc: 0.974 - ETA: 4s - loss: 0.0908 - acc: 0.972 - ETA: 4s - loss: 0.0878 - acc: 0.973 - ETA: 4s - loss: 0.0880 - acc: 0.971 - ETA: 4s - loss: 0.0926 - acc: 0.970 - ETA: 4s - loss: 0.0997 - acc: 0.968 - ETA: 4s - loss: 0.1086 - acc: 0.967 - ETA: 4s - loss: 0.1054 - acc: 0.967 - ETA: 4s - loss: 0.1036 - acc: 0.968 - ETA: 4s - loss: 0.1039 - acc: 0.968 - ETA: 4s - loss: 0.1080 - acc: 0.966 - ETA: 4s - loss: 0.1130 - acc: 0.967 - ETA: 4s - loss: 0.1168 - acc: 0.965 - ETA: 4s - loss: 0.1134 - acc: 0.966 - ETA: 4s - loss: 0.1159 - acc: 0.965 - ETA: 4s - loss: 0.1124 - acc: 0.966 - ETA: 3s - loss: 0.1165 - acc: 0.965 - ETA: 3s - loss: 0.1132 - acc: 0.966 - ETA: 3s - loss: 0.1098 - acc: 0.967 - ETA: 3s - loss: 0.1072 - acc: 0.968 - ETA: 3s - loss: 0.1106 - acc: 0.967 - ETA: 3s - loss: 0.1117 - acc: 0.967 - ETA: 3s - loss: 0.1086 - acc: 0.968 - ETA: 3s - loss: 0.1105 - acc: 0.967 - ETA: 3s - loss: 0.1115 - acc: 0.967 - ETA: 3s - loss: 0.1119 - acc: 0.966 - ETA: 3s - loss: 0.1097 - acc: 0.967 - ETA: 3s - loss: 0.1132 - acc: 0.965 - ETA: 3s - loss: 0.1161 - acc: 0.965 - ETA: 3s - loss: 0.1144 - acc: 0.965 - ETA: 3s - loss: 0.1175 - acc: 0.964 - ETA: 2s - loss: 0.1185 - acc: 0.964 - ETA: 2s - loss: 0.1189 - acc: 0.964 - ETA: 2s - loss: 0.1231 - acc: 0.964 - ETA: 2s - loss: 0.1226 - acc: 0.964 - ETA: 2s - loss: 0.1230 - acc: 0.964 - ETA: 2s - loss: 0.1256 - acc: 0.964 - ETA: 2s - loss: 0.1258 - acc: 0.964 - ETA: 2s - loss: 0.1259 - acc: 0.964 - ETA: 2s - loss: 0.1274 - acc: 0.963 - ETA: 2s - loss: 0.1266 - acc: 0.963 - ETA: 2s - loss: 0.1282 - acc: 0.962 - ETA: 2s - loss: 0.1269 - acc: 0.963 - ETA: 2s - loss: 0.1282 - acc: 0.962 - ETA: 2s - loss: 0.1271 - acc: 0.963 - ETA: 2s - loss: 0.1254 - acc: 0.963 - ETA: 1s - loss: 0.1242 - acc: 0.963 - ETA: 1s - loss: 0.1240 - acc: 0.963 - ETA: 1s - loss: 0.1220 - acc: 0.964 - ETA: 1s - loss: 0.1224 - acc: 0.964 - ETA: 1s - loss: 0.1221 - acc: 0.963 - ETA: 1s - loss: 0.1212 - acc: 0.964 - ETA: 1s - loss: 0.1213 - acc: 0.963 - ETA: 1s - loss: 0.1214 - acc: 0.963 - ETA: 1s - loss: 0.1217 - acc: 0.963 - ETA: 1s - loss: 0.1205 - acc: 0.964 - ETA: 1s - loss: 0.1208 - acc: 0.963 - ETA: 1s - loss: 0.1196 - acc: 0.964 - ETA: 1s - loss: 0.1193 - acc: 0.964 - ETA: 1s - loss: 0.1187 - acc: 0.964 - ETA: 1s - loss: 0.1202 - acc: 0.963 - ETA: 1s - loss: 0.1220 - acc: 0.963 - ETA: 0s - loss: 0.1213 - acc: 0.963 - ETA: 0s - loss: 0.1206 - acc: 0.963 - ETA: 0s - loss: 0.1216 - acc: 0.963 - ETA: 0s - loss: 0.1209 - acc: 0.964 - ETA: 0s - loss: 0.1212 - acc: 0.963 - ETA: 0s - loss: 0.1213 - acc: 0.963 - ETA: 0s - loss: 0.1224 - acc: 0.963 - ETA: 0s - loss: 0.1213 - acc: 0.963 - ETA: 0s - loss: 0.1236 - acc: 0.963 - ETA: 0s - loss: 0.1248 - acc: 0.962 - ETA: 0s - loss: 0.1241 - acc: 0.962 - ETA: 0s - loss: 0.1227 - acc: 0.963 - ETA: 0s - loss: 0.1228 - acc: 0.962 - ETA: 0s - loss: 0.1219 - acc: 0.963 - ETA: 0s - loss: 0.1248 - acc: 0.962 - ETA: 0s - loss: 0.1248 - acc: 0.9627Epoch 00012: val\_loss did not improve
6680/6680 [==============================] - 5s 796us/step - loss: 0.1251 - acc: 0.9626 - val\_loss: 0.6377 - val\_acc: 0.8443
Epoch 13/20
6620/6680 [============================>.] - ETA: 4s - loss: 0.0098 - acc: 1.000 - ETA: 4s - loss: 0.0732 - acc: 0.970 - ETA: 4s - loss: 0.1135 - acc: 0.966 - ETA: 4s - loss: 0.1012 - acc: 0.969 - ETA: 4s - loss: 0.0870 - acc: 0.973 - ETA: 4s - loss: 0.0807 - acc: 0.976 - ETA: 4s - loss: 0.0991 - acc: 0.972 - ETA: 4s - loss: 0.0936 - acc: 0.970 - ETA: 4s - loss: 0.0911 - acc: 0.972 - ETA: 4s - loss: 0.0961 - acc: 0.971 - ETA: 4s - loss: 0.0911 - acc: 0.973 - ETA: 3s - loss: 0.0937 - acc: 0.972 - ETA: 3s - loss: 0.0921 - acc: 0.973 - ETA: 3s - loss: 0.0964 - acc: 0.970 - ETA: 3s - loss: 0.0943 - acc: 0.970 - ETA: 3s - loss: 0.0908 - acc: 0.972 - ETA: 3s - loss: 0.0905 - acc: 0.972 - ETA: 3s - loss: 0.0911 - acc: 0.971 - ETA: 3s - loss: 0.0998 - acc: 0.969 - ETA: 3s - loss: 0.0968 - acc: 0.970 - ETA: 3s - loss: 0.1024 - acc: 0.969 - ETA: 3s - loss: 0.1040 - acc: 0.969 - ETA: 3s - loss: 0.1092 - acc: 0.966 - ETA: 3s - loss: 0.1111 - acc: 0.966 - ETA: 3s - loss: 0.1094 - acc: 0.967 - ETA: 3s - loss: 0.1103 - acc: 0.965 - ETA: 3s - loss: 0.1133 - acc: 0.965 - ETA: 3s - loss: 0.1103 - acc: 0.967 - ETA: 3s - loss: 0.1106 - acc: 0.966 - ETA: 3s - loss: 0.1130 - acc: 0.965 - ETA: 3s - loss: 0.1121 - acc: 0.966 - ETA: 2s - loss: 0.1090 - acc: 0.967 - ETA: 2s - loss: 0.1098 - acc: 0.967 - ETA: 2s - loss: 0.1093 - acc: 0.967 - ETA: 2s - loss: 0.1068 - acc: 0.968 - ETA: 2s - loss: 0.1069 - acc: 0.967 - ETA: 2s - loss: 0.1086 - acc: 0.967 - ETA: 2s - loss: 0.1077 - acc: 0.967 - ETA: 2s - loss: 0.1082 - acc: 0.966 - ETA: 2s - loss: 0.1066 - acc: 0.967 - ETA: 2s - loss: 0.1070 - acc: 0.967 - ETA: 2s - loss: 0.1077 - acc: 0.967 - ETA: 2s - loss: 0.1086 - acc: 0.967 - ETA: 2s - loss: 0.1086 - acc: 0.967 - ETA: 2s - loss: 0.1080 - acc: 0.967 - ETA: 2s - loss: 0.1076 - acc: 0.968 - ETA: 2s - loss: 0.1083 - acc: 0.968 - ETA: 2s - loss: 0.1079 - acc: 0.967 - ETA: 2s - loss: 0.1080 - acc: 0.967 - ETA: 2s - loss: 0.1071 - acc: 0.967 - ETA: 2s - loss: 0.1070 - acc: 0.967 - ETA: 1s - loss: 0.1072 - acc: 0.967 - ETA: 1s - loss: 0.1066 - acc: 0.966 - ETA: 1s - loss: 0.1049 - acc: 0.967 - ETA: 1s - loss: 0.1042 - acc: 0.967 - ETA: 1s - loss: 0.1076 - acc: 0.966 - ETA: 1s - loss: 0.1069 - acc: 0.966 - ETA: 1s - loss: 0.1053 - acc: 0.967 - ETA: 1s - loss: 0.1057 - acc: 0.967 - ETA: 1s - loss: 0.1054 - acc: 0.967 - ETA: 1s - loss: 0.1055 - acc: 0.967 - ETA: 1s - loss: 0.1056 - acc: 0.967 - ETA: 1s - loss: 0.1061 - acc: 0.967 - ETA: 1s - loss: 0.1065 - acc: 0.967 - ETA: 1s - loss: 0.1060 - acc: 0.966 - ETA: 1s - loss: 0.1055 - acc: 0.967 - ETA: 1s - loss: 0.1072 - acc: 0.966 - ETA: 1s - loss: 0.1062 - acc: 0.966 - ETA: 1s - loss: 0.1083 - acc: 0.966 - ETA: 0s - loss: 0.1114 - acc: 0.965 - ETA: 0s - loss: 0.1130 - acc: 0.965 - ETA: 0s - loss: 0.1157 - acc: 0.965 - ETA: 0s - loss: 0.1149 - acc: 0.965 - ETA: 0s - loss: 0.1141 - acc: 0.965 - ETA: 0s - loss: 0.1148 - acc: 0.965 - ETA: 0s - loss: 0.1148 - acc: 0.965 - ETA: 0s - loss: 0.1152 - acc: 0.965 - ETA: 0s - loss: 0.1165 - acc: 0.965 - ETA: 0s - loss: 0.1159 - acc: 0.965 - ETA: 0s - loss: 0.1175 - acc: 0.964 - ETA: 0s - loss: 0.1167 - acc: 0.964 - ETA: 0s - loss: 0.1167 - acc: 0.965 - ETA: 0s - loss: 0.1158 - acc: 0.965 - ETA: 0s - loss: 0.1161 - acc: 0.964 - ETA: 0s - loss: 0.1159 - acc: 0.965 - ETA: 0s - loss: 0.1164 - acc: 0.964 - ETA: 0s - loss: 0.1158 - acc: 0.965 - ETA: 0s - loss: 0.1148 - acc: 0.9653Epoch 00013: val\_loss did not improve
6680/6680 [==============================] - 5s 784us/step - loss: 0.1151 - acc: 0.9654 - val\_loss: 0.6266 - val\_acc: 0.8527
Epoch 14/20
6640/6680 [============================>.] - ETA: 5s - loss: 0.0316 - acc: 1.000 - ETA: 4s - loss: 0.1022 - acc: 0.990 - ETA: 4s - loss: 0.1116 - acc: 0.977 - ETA: 4s - loss: 0.1081 - acc: 0.969 - ETA: 4s - loss: 0.0999 - acc: 0.973 - ETA: 4s - loss: 0.0876 - acc: 0.978 - ETA: 4s - loss: 0.0762 - acc: 0.980 - ETA: 4s - loss: 0.0828 - acc: 0.981 - ETA: 4s - loss: 0.0813 - acc: 0.980 - ETA: 4s - loss: 0.0816 - acc: 0.978 - ETA: 4s - loss: 0.0783 - acc: 0.979 - ETA: 4s - loss: 0.0809 - acc: 0.977 - ETA: 4s - loss: 0.0838 - acc: 0.975 - ETA: 4s - loss: 0.0957 - acc: 0.971 - ETA: 3s - loss: 0.0945 - acc: 0.971 - ETA: 3s - loss: 0.0982 - acc: 0.969 - ETA: 3s - loss: 0.0968 - acc: 0.969 - ETA: 3s - loss: 0.0969 - acc: 0.968 - ETA: 3s - loss: 0.0963 - acc: 0.968 - ETA: 3s - loss: 0.0949 - acc: 0.968 - ETA: 3s - loss: 0.0940 - acc: 0.968 - ETA: 3s - loss: 0.0937 - acc: 0.968 - ETA: 3s - loss: 0.0975 - acc: 0.969 - ETA: 3s - loss: 0.0954 - acc: 0.969 - ETA: 3s - loss: 0.0925 - acc: 0.970 - ETA: 3s - loss: 0.0943 - acc: 0.971 - ETA: 3s - loss: 0.0940 - acc: 0.971 - ETA: 3s - loss: 0.0913 - acc: 0.971 - ETA: 3s - loss: 0.0900 - acc: 0.971 - ETA: 3s - loss: 0.0894 - acc: 0.971 - ETA: 3s - loss: 0.0930 - acc: 0.970 - ETA: 3s - loss: 0.0907 - acc: 0.971 - ETA: 2s - loss: 0.0929 - acc: 0.970 - ETA: 2s - loss: 0.0920 - acc: 0.970 - ETA: 2s - loss: 0.0912 - acc: 0.970 - ETA: 2s - loss: 0.0947 - acc: 0.970 - ETA: 2s - loss: 0.0927 - acc: 0.971 - ETA: 2s - loss: 0.0929 - acc: 0.971 - ETA: 2s - loss: 0.0971 - acc: 0.970 - ETA: 2s - loss: 0.0968 - acc: 0.970 - ETA: 2s - loss: 0.0966 - acc: 0.970 - ETA: 2s - loss: 0.0961 - acc: 0.970 - ETA: 2s - loss: 0.0967 - acc: 0.971 - ETA: 2s - loss: 0.0997 - acc: 0.970 - ETA: 2s - loss: 0.0976 - acc: 0.971 - ETA: 2s - loss: 0.0989 - acc: 0.971 - ETA: 2s - loss: 0.0991 - acc: 0.971 - ETA: 2s - loss: 0.0994 - acc: 0.971 - ETA: 2s - loss: 0.0980 - acc: 0.971 - ETA: 2s - loss: 0.0996 - acc: 0.970 - ETA: 1s - loss: 0.1005 - acc: 0.970 - ETA: 1s - loss: 0.0997 - acc: 0.970 - ETA: 1s - loss: 0.0991 - acc: 0.970 - ETA: 1s - loss: 0.0991 - acc: 0.969 - ETA: 1s - loss: 0.1008 - acc: 0.969 - ETA: 1s - loss: 0.1010 - acc: 0.969 - ETA: 1s - loss: 0.1011 - acc: 0.969 - ETA: 1s - loss: 0.1035 - acc: 0.968 - ETA: 1s - loss: 0.1044 - acc: 0.968 - ETA: 1s - loss: 0.1062 - acc: 0.968 - ETA: 1s - loss: 0.1062 - acc: 0.968 - ETA: 1s - loss: 0.1073 - acc: 0.968 - ETA: 1s - loss: 0.1074 - acc: 0.968 - ETA: 1s - loss: 0.1075 - acc: 0.968 - ETA: 1s - loss: 0.1071 - acc: 0.968 - ETA: 1s - loss: 0.1072 - acc: 0.968 - ETA: 1s - loss: 0.1066 - acc: 0.968 - ETA: 1s - loss: 0.1057 - acc: 0.968 - ETA: 0s - loss: 0.1059 - acc: 0.968 - ETA: 0s - loss: 0.1060 - acc: 0.968 - ETA: 0s - loss: 0.1055 - acc: 0.968 - ETA: 0s - loss: 0.1055 - acc: 0.968 - ETA: 0s - loss: 0.1045 - acc: 0.968 - ETA: 0s - loss: 0.1049 - acc: 0.967 - ETA: 0s - loss: 0.1042 - acc: 0.968 - ETA: 0s - loss: 0.1043 - acc: 0.967 - ETA: 0s - loss: 0.1042 - acc: 0.967 - ETA: 0s - loss: 0.1055 - acc: 0.967 - ETA: 0s - loss: 0.1048 - acc: 0.967 - ETA: 0s - loss: 0.1047 - acc: 0.967 - ETA: 0s - loss: 0.1057 - acc: 0.967 - ETA: 0s - loss: 0.1066 - acc: 0.966 - ETA: 0s - loss: 0.1086 - acc: 0.966 - ETA: 0s - loss: 0.1086 - acc: 0.966 - ETA: 0s - loss: 0.1088 - acc: 0.9660Epoch 00014: val\_loss did not improve
6680/6680 [==============================] - 5s 776us/step - loss: 0.1086 - acc: 0.9660 - val\_loss: 0.6454 - val\_acc: 0.8551
Epoch 15/20
6620/6680 [============================>.] - ETA: 5s - loss: 0.0116 - acc: 1.000 - ETA: 4s - loss: 0.0626 - acc: 0.990 - ETA: 4s - loss: 0.0588 - acc: 0.983 - ETA: 4s - loss: 0.0464 - acc: 0.988 - ETA: 4s - loss: 0.0601 - acc: 0.979 - ETA: 4s - loss: 0.0581 - acc: 0.980 - ETA: 4s - loss: 0.0646 - acc: 0.980 - ETA: 4s - loss: 0.0598 - acc: 0.983 - ETA: 4s - loss: 0.0535 - acc: 0.985 - ETA: 4s - loss: 0.0515 - acc: 0.985 - ETA: 4s - loss: 0.0686 - acc: 0.980 - ETA: 4s - loss: 0.0657 - acc: 0.982 - ETA: 4s - loss: 0.0686 - acc: 0.981 - ETA: 4s - loss: 0.0636 - acc: 0.983 - ETA: 4s - loss: 0.0664 - acc: 0.981 - ETA: 4s - loss: 0.0648 - acc: 0.981 - ETA: 4s - loss: 0.0647 - acc: 0.982 - ETA: 4s - loss: 0.0627 - acc: 0.982 - ETA: 4s - loss: 0.0651 - acc: 0.981 - ETA: 4s - loss: 0.0692 - acc: 0.982 - ETA: 3s - loss: 0.0734 - acc: 0.980 - ETA: 3s - loss: 0.0710 - acc: 0.981 - ETA: 3s - loss: 0.0769 - acc: 0.980 - ETA: 3s - loss: 0.0771 - acc: 0.980 - ETA: 3s - loss: 0.0752 - acc: 0.981 - ETA: 3s - loss: 0.0739 - acc: 0.981 - ETA: 3s - loss: 0.0872 - acc: 0.978 - ETA: 3s - loss: 0.0889 - acc: 0.978 - ETA: 3s - loss: 0.0884 - acc: 0.977 - ETA: 3s - loss: 0.0947 - acc: 0.977 - ETA: 3s - loss: 0.0949 - acc: 0.977 - ETA: 3s - loss: 0.0937 - acc: 0.976 - ETA: 3s - loss: 0.0937 - acc: 0.976 - ETA: 3s - loss: 0.0924 - acc: 0.976 - ETA: 3s - loss: 0.0932 - acc: 0.977 - ETA: 3s - loss: 0.0920 - acc: 0.977 - ETA: 3s - loss: 0.0906 - acc: 0.977 - ETA: 3s - loss: 0.0915 - acc: 0.977 - ETA: 3s - loss: 0.0937 - acc: 0.976 - ETA: 3s - loss: 0.0940 - acc: 0.976 - ETA: 2s - loss: 0.0955 - acc: 0.975 - ETA: 2s - loss: 0.0942 - acc: 0.975 - ETA: 2s - loss: 0.0936 - acc: 0.975 - ETA: 2s - loss: 0.0923 - acc: 0.975 - ETA: 2s - loss: 0.0937 - acc: 0.974 - ETA: 2s - loss: 0.0935 - acc: 0.973 - ETA: 2s - loss: 0.0956 - acc: 0.973 - ETA: 2s - loss: 0.0950 - acc: 0.973 - ETA: 2s - loss: 0.0945 - acc: 0.973 - ETA: 2s - loss: 0.0960 - acc: 0.972 - ETA: 2s - loss: 0.0965 - acc: 0.972 - ETA: 2s - loss: 0.0955 - acc: 0.972 - ETA: 2s - loss: 0.0997 - acc: 0.971 - ETA: 2s - loss: 0.0998 - acc: 0.971 - ETA: 2s - loss: 0.0990 - acc: 0.971 - ETA: 2s - loss: 0.1008 - acc: 0.971 - ETA: 2s - loss: 0.0999 - acc: 0.971 - ETA: 2s - loss: 0.0987 - acc: 0.972 - ETA: 1s - loss: 0.0986 - acc: 0.971 - ETA: 1s - loss: 0.0981 - acc: 0.972 - ETA: 1s - loss: 0.0994 - acc: 0.971 - ETA: 1s - loss: 0.0993 - acc: 0.971 - ETA: 1s - loss: 0.0987 - acc: 0.971 - ETA: 1s - loss: 0.0985 - acc: 0.972 - ETA: 1s - loss: 0.0976 - acc: 0.972 - ETA: 1s - loss: 0.0971 - acc: 0.972 - ETA: 1s - loss: 0.0973 - acc: 0.972 - ETA: 1s - loss: 0.0966 - acc: 0.972 - ETA: 1s - loss: 0.0955 - acc: 0.972 - ETA: 1s - loss: 0.0947 - acc: 0.972 - ETA: 1s - loss: 0.0937 - acc: 0.973 - ETA: 1s - loss: 0.0941 - acc: 0.972 - ETA: 1s - loss: 0.0946 - acc: 0.972 - ETA: 1s - loss: 0.0942 - acc: 0.972 - ETA: 0s - loss: 0.0934 - acc: 0.972 - ETA: 0s - loss: 0.0958 - acc: 0.972 - ETA: 0s - loss: 0.0955 - acc: 0.972 - ETA: 0s - loss: 0.0962 - acc: 0.972 - ETA: 0s - loss: 0.0961 - acc: 0.972 - ETA: 0s - loss: 0.0972 - acc: 0.972 - ETA: 0s - loss: 0.0978 - acc: 0.971 - ETA: 0s - loss: 0.0980 - acc: 0.971 - ETA: 0s - loss: 0.1000 - acc: 0.971 - ETA: 0s - loss: 0.1015 - acc: 0.971 - ETA: 0s - loss: 0.1010 - acc: 0.971 - ETA: 0s - loss: 0.1001 - acc: 0.971 - ETA: 0s - loss: 0.1007 - acc: 0.971 - ETA: 0s - loss: 0.1002 - acc: 0.971 - ETA: 0s - loss: 0.1001 - acc: 0.971 - ETA: 0s - loss: 0.0995 - acc: 0.971 - ETA: 0s - loss: 0.1005 - acc: 0.9711Epoch 00015: val\_loss did not improve
6680/6680 [==============================] - 6s 824us/step - loss: 0.1004 - acc: 0.9713 - val\_loss: 0.6339 - val\_acc: 0.8491
Epoch 16/20
6620/6680 [============================>.] - ETA: 4s - loss: 0.0078 - acc: 1.000 - ETA: 4s - loss: 0.0827 - acc: 0.980 - ETA: 4s - loss: 0.0611 - acc: 0.988 - ETA: 4s - loss: 0.0688 - acc: 0.976 - ETA: 4s - loss: 0.0839 - acc: 0.979 - ETA: 4s - loss: 0.1092 - acc: 0.973 - ETA: 4s - loss: 0.1041 - acc: 0.972 - ETA: 4s - loss: 0.1142 - acc: 0.972 - ETA: 4s - loss: 0.1057 - acc: 0.973 - ETA: 4s - loss: 0.0990 - acc: 0.975 - ETA: 4s - loss: 0.0947 - acc: 0.975 - ETA: 4s - loss: 0.0921 - acc: 0.974 - ETA: 4s - loss: 0.0888 - acc: 0.974 - ETA: 4s - loss: 0.0916 - acc: 0.973 - ETA: 4s - loss: 0.1045 - acc: 0.971 - ETA: 4s - loss: 0.0995 - acc: 0.972 - ETA: 3s - loss: 0.0979 - acc: 0.973 - ETA: 3s - loss: 0.0927 - acc: 0.974 - ETA: 3s - loss: 0.0914 - acc: 0.974 - ETA: 3s - loss: 0.0922 - acc: 0.974 - ETA: 3s - loss: 0.0890 - acc: 0.974 - ETA: 3s - loss: 0.0902 - acc: 0.975 - ETA: 3s - loss: 0.0927 - acc: 0.975 - ETA: 3s - loss: 0.0897 - acc: 0.975 - ETA: 3s - loss: 0.0969 - acc: 0.974 - ETA: 3s - loss: 0.1043 - acc: 0.972 - ETA: 3s - loss: 0.1035 - acc: 0.972 - ETA: 3s - loss: 0.1056 - acc: 0.971 - ETA: 3s - loss: 0.1042 - acc: 0.971 - ETA: 3s - loss: 0.1018 - acc: 0.971 - ETA: 3s - loss: 0.1006 - acc: 0.971 - ETA: 3s - loss: 0.0990 - acc: 0.972 - ETA: 3s - loss: 0.0968 - acc: 0.972 - ETA: 2s - loss: 0.0971 - acc: 0.972 - ETA: 2s - loss: 0.0992 - acc: 0.972 - ETA: 2s - loss: 0.0977 - acc: 0.972 - ETA: 2s - loss: 0.0958 - acc: 0.972 - ETA: 2s - loss: 0.0959 - acc: 0.972 - ETA: 2s - loss: 0.0948 - acc: 0.972 - ETA: 2s - loss: 0.0960 - acc: 0.972 - ETA: 2s - loss: 0.0945 - acc: 0.973 - ETA: 2s - loss: 0.0933 - acc: 0.973 - ETA: 2s - loss: 0.0963 - acc: 0.972 - ETA: 2s - loss: 0.0950 - acc: 0.973 - ETA: 2s - loss: 0.0931 - acc: 0.973 - ETA: 2s - loss: 0.0933 - acc: 0.973 - ETA: 2s - loss: 0.0924 - acc: 0.974 - ETA: 2s - loss: 0.0936 - acc: 0.973 - ETA: 2s - loss: 0.0932 - acc: 0.973 - ETA: 2s - loss: 0.0921 - acc: 0.974 - ETA: 1s - loss: 0.0907 - acc: 0.974 - ETA: 1s - loss: 0.0892 - acc: 0.974 - ETA: 1s - loss: 0.0887 - acc: 0.975 - ETA: 1s - loss: 0.0919 - acc: 0.975 - ETA: 1s - loss: 0.0923 - acc: 0.974 - ETA: 1s - loss: 0.0913 - acc: 0.974 - ETA: 1s - loss: 0.0907 - acc: 0.974 - ETA: 1s - loss: 0.0894 - acc: 0.975 - ETA: 1s - loss: 0.0881 - acc: 0.975 - ETA: 1s - loss: 0.0877 - acc: 0.975 - ETA: 1s - loss: 0.0867 - acc: 0.975 - ETA: 1s - loss: 0.0870 - acc: 0.975 - ETA: 1s - loss: 0.0871 - acc: 0.975 - ETA: 1s - loss: 0.0863 - acc: 0.975 - ETA: 1s - loss: 0.0866 - acc: 0.975 - ETA: 1s - loss: 0.0876 - acc: 0.974 - ETA: 1s - loss: 0.0864 - acc: 0.975 - ETA: 0s - loss: 0.0857 - acc: 0.975 - ETA: 0s - loss: 0.0859 - acc: 0.974 - ETA: 0s - loss: 0.0858 - acc: 0.974 - ETA: 0s - loss: 0.0859 - acc: 0.974 - ETA: 0s - loss: 0.0892 - acc: 0.974 - ETA: 0s - loss: 0.0903 - acc: 0.974 - ETA: 0s - loss: 0.0902 - acc: 0.973 - ETA: 0s - loss: 0.0911 - acc: 0.973 - ETA: 0s - loss: 0.0918 - acc: 0.972 - ETA: 0s - loss: 0.0908 - acc: 0.973 - ETA: 0s - loss: 0.0928 - acc: 0.972 - ETA: 0s - loss: 0.0922 - acc: 0.972 - ETA: 0s - loss: 0.0921 - acc: 0.973 - ETA: 0s - loss: 0.0916 - acc: 0.973 - ETA: 0s - loss: 0.0910 - acc: 0.973 - ETA: 0s - loss: 0.0915 - acc: 0.9733Epoch 00016: val\_loss did not improve
6680/6680 [==============================] - 5s 769us/step - loss: 0.0909 - acc: 0.9734 - val\_loss: 0.6603 - val\_acc: 0.8539
Epoch 17/20
6660/6680 [============================>.] - ETA: 4s - loss: 0.2501 - acc: 0.950 - ETA: 4s - loss: 0.0782 - acc: 0.990 - ETA: 4s - loss: 0.0601 - acc: 0.988 - ETA: 4s - loss: 0.0434 - acc: 0.992 - ETA: 4s - loss: 0.0399 - acc: 0.991 - ETA: 4s - loss: 0.0370 - acc: 0.990 - ETA: 4s - loss: 0.0369 - acc: 0.988 - ETA: 4s - loss: 0.0367 - acc: 0.987 - ETA: 4s - loss: 0.0425 - acc: 0.984 - ETA: 4s - loss: 0.0502 - acc: 0.983 - ETA: 4s - loss: 0.0527 - acc: 0.982 - ETA: 4s - loss: 0.0536 - acc: 0.983 - ETA: 4s - loss: 0.0527 - acc: 0.982 - ETA: 4s - loss: 0.0517 - acc: 0.984 - ETA: 4s - loss: 0.0504 - acc: 0.984 - ETA: 3s - loss: 0.0513 - acc: 0.983 - ETA: 3s - loss: 0.0532 - acc: 0.983 - ETA: 3s - loss: 0.0598 - acc: 0.981 - ETA: 3s - loss: 0.0603 - acc: 0.981 - ETA: 3s - loss: 0.0596 - acc: 0.981 - ETA: 3s - loss: 0.0623 - acc: 0.980 - ETA: 3s - loss: 0.0609 - acc: 0.981 - ETA: 3s - loss: 0.0590 - acc: 0.982 - ETA: 3s - loss: 0.0601 - acc: 0.981 - ETA: 3s - loss: 0.0579 - acc: 0.982 - ETA: 3s - loss: 0.0599 - acc: 0.981 - ETA: 3s - loss: 0.0619 - acc: 0.980 - ETA: 3s - loss: 0.0612 - acc: 0.979 - ETA: 3s - loss: 0.0600 - acc: 0.980 - ETA: 3s - loss: 0.0588 - acc: 0.980 - ETA: 3s - loss: 0.0627 - acc: 0.979 - ETA: 3s - loss: 0.0629 - acc: 0.979 - ETA: 2s - loss: 0.0631 - acc: 0.979 - ETA: 2s - loss: 0.0695 - acc: 0.978 - ETA: 2s - loss: 0.0717 - acc: 0.977 - ETA: 2s - loss: 0.0714 - acc: 0.978 - ETA: 2s - loss: 0.0703 - acc: 0.978 - ETA: 2s - loss: 0.0732 - acc: 0.978 - ETA: 2s - loss: 0.0738 - acc: 0.978 - ETA: 2s - loss: 0.0722 - acc: 0.978 - ETA: 2s - loss: 0.0727 - acc: 0.978 - ETA: 2s - loss: 0.0750 - acc: 0.978 - ETA: 2s - loss: 0.0784 - acc: 0.978 - ETA: 2s - loss: 0.0778 - acc: 0.978 - ETA: 2s - loss: 0.0791 - acc: 0.978 - ETA: 2s - loss: 0.0789 - acc: 0.977 - ETA: 2s - loss: 0.0787 - acc: 0.977 - ETA: 2s - loss: 0.0778 - acc: 0.977 - ETA: 2s - loss: 0.0778 - acc: 0.977 - ETA: 2s - loss: 0.0817 - acc: 0.976 - ETA: 1s - loss: 0.0826 - acc: 0.977 - ETA: 1s - loss: 0.0818 - acc: 0.977 - ETA: 1s - loss: 0.0820 - acc: 0.977 - ETA: 1s - loss: 0.0817 - acc: 0.977 - ETA: 1s - loss: 0.0810 - acc: 0.977 - ETA: 1s - loss: 0.0801 - acc: 0.977 - ETA: 1s - loss: 0.0804 - acc: 0.977 - ETA: 1s - loss: 0.0803 - acc: 0.977 - ETA: 1s - loss: 0.0799 - acc: 0.977 - ETA: 1s - loss: 0.0796 - acc: 0.977 - ETA: 1s - loss: 0.0787 - acc: 0.977 - ETA: 1s - loss: 0.0783 - acc: 0.977 - ETA: 1s - loss: 0.0785 - acc: 0.977 - ETA: 1s - loss: 0.0795 - acc: 0.977 - ETA: 1s - loss: 0.0801 - acc: 0.977 - ETA: 1s - loss: 0.0798 - acc: 0.977 - ETA: 1s - loss: 0.0795 - acc: 0.977 - ETA: 1s - loss: 0.0793 - acc: 0.976 - ETA: 1s - loss: 0.0818 - acc: 0.976 - ETA: 0s - loss: 0.0810 - acc: 0.976 - ETA: 0s - loss: 0.0817 - acc: 0.976 - ETA: 0s - loss: 0.0833 - acc: 0.976 - ETA: 0s - loss: 0.0826 - acc: 0.976 - ETA: 0s - loss: 0.0838 - acc: 0.976 - ETA: 0s - loss: 0.0836 - acc: 0.976 - ETA: 0s - loss: 0.0830 - acc: 0.976 - ETA: 0s - loss: 0.0833 - acc: 0.976 - ETA: 0s - loss: 0.0827 - acc: 0.976 - ETA: 0s - loss: 0.0832 - acc: 0.976 - ETA: 0s - loss: 0.0829 - acc: 0.976 - ETA: 0s - loss: 0.0837 - acc: 0.976 - ETA: 0s - loss: 0.0849 - acc: 0.975 - ETA: 0s - loss: 0.0862 - acc: 0.975 - ETA: 0s - loss: 0.0859 - acc: 0.975 - ETA: 0s - loss: 0.0858 - acc: 0.975 - ETA: 0s - loss: 0.0866 - acc: 0.975 - ETA: 0s - loss: 0.0874 - acc: 0.9754Epoch 00017: val\_loss did not improve
6680/6680 [==============================] - 5s 789us/step - loss: 0.0873 - acc: 0.9754 - val\_loss: 0.6814 - val\_acc: 0.8563
Epoch 18/20
6600/6680 [============================>.] - ETA: 4s - loss: 0.0070 - acc: 1.000 - ETA: 4s - loss: 0.0660 - acc: 0.980 - ETA: 4s - loss: 0.0433 - acc: 0.988 - ETA: 4s - loss: 0.0586 - acc: 0.984 - ETA: 4s - loss: 0.0514 - acc: 0.987 - ETA: 4s - loss: 0.0460 - acc: 0.989 - ETA: 4s - loss: 0.0847 - acc: 0.984 - ETA: 4s - loss: 0.0957 - acc: 0.978 - ETA: 4s - loss: 0.0893 - acc: 0.980 - ETA: 4s - loss: 0.0845 - acc: 0.980 - ETA: 4s - loss: 0.0770 - acc: 0.982 - ETA: 4s - loss: 0.0749 - acc: 0.981 - ETA: 4s - loss: 0.0851 - acc: 0.980 - ETA: 4s - loss: 0.0813 - acc: 0.980 - ETA: 4s - loss: 0.0780 - acc: 0.981 - ETA: 4s - loss: 0.0800 - acc: 0.980 - ETA: 4s - loss: 0.0762 - acc: 0.981 - ETA: 4s - loss: 0.0752 - acc: 0.979 - ETA: 4s - loss: 0.0767 - acc: 0.979 - ETA: 4s - loss: 0.0764 - acc: 0.978 - ETA: 4s - loss: 0.0764 - acc: 0.979 - ETA: 4s - loss: 0.0746 - acc: 0.979 - ETA: 4s - loss: 0.0784 - acc: 0.979 - ETA: 3s - loss: 0.0778 - acc: 0.979 - ETA: 3s - loss: 0.0805 - acc: 0.978 - ETA: 3s - loss: 0.0846 - acc: 0.977 - ETA: 3s - loss: 0.0825 - acc: 0.978 - ETA: 3s - loss: 0.0836 - acc: 0.977 - ETA: 3s - loss: 0.0824 - acc: 0.977 - ETA: 3s - loss: 0.0803 - acc: 0.978 - ETA: 3s - loss: 0.0788 - acc: 0.978 - ETA: 3s - loss: 0.0765 - acc: 0.979 - ETA: 3s - loss: 0.0761 - acc: 0.979 - ETA: 3s - loss: 0.0745 - acc: 0.979 - ETA: 3s - loss: 0.0732 - acc: 0.980 - ETA: 3s - loss: 0.0722 - acc: 0.979 - ETA: 3s - loss: 0.0703 - acc: 0.980 - ETA: 3s - loss: 0.0697 - acc: 0.980 - ETA: 3s - loss: 0.0683 - acc: 0.980 - ETA: 3s - loss: 0.0684 - acc: 0.980 - ETA: 3s - loss: 0.0702 - acc: 0.979 - ETA: 2s - loss: 0.0695 - acc: 0.979 - ETA: 2s - loss: 0.0682 - acc: 0.980 - ETA: 2s - loss: 0.0671 - acc: 0.980 - ETA: 2s - loss: 0.0657 - acc: 0.981 - ETA: 2s - loss: 0.0650 - acc: 0.981 - ETA: 2s - loss: 0.0645 - acc: 0.981 - ETA: 2s - loss: 0.0652 - acc: 0.980 - ETA: 2s - loss: 0.0665 - acc: 0.980 - ETA: 2s - loss: 0.0662 - acc: 0.980 - ETA: 2s - loss: 0.0679 - acc: 0.979 - ETA: 2s - loss: 0.0707 - acc: 0.979 - ETA: 2s - loss: 0.0697 - acc: 0.979 - ETA: 2s - loss: 0.0687 - acc: 0.980 - ETA: 2s - loss: 0.0697 - acc: 0.979 - ETA: 2s - loss: 0.0692 - acc: 0.979 - ETA: 1s - loss: 0.0701 - acc: 0.979 - ETA: 1s - loss: 0.0694 - acc: 0.979 - ETA: 1s - loss: 0.0706 - acc: 0.978 - ETA: 1s - loss: 0.0705 - acc: 0.978 - ETA: 1s - loss: 0.0713 - acc: 0.978 - ETA: 1s - loss: 0.0735 - acc: 0.978 - ETA: 1s - loss: 0.0728 - acc: 0.978 - ETA: 1s - loss: 0.0730 - acc: 0.978 - ETA: 1s - loss: 0.0724 - acc: 0.978 - ETA: 1s - loss: 0.0748 - acc: 0.978 - ETA: 1s - loss: 0.0746 - acc: 0.978 - ETA: 1s - loss: 0.0740 - acc: 0.978 - ETA: 1s - loss: 0.0742 - acc: 0.978 - ETA: 1s - loss: 0.0750 - acc: 0.978 - ETA: 1s - loss: 0.0748 - acc: 0.978 - ETA: 1s - loss: 0.0753 - acc: 0.978 - ETA: 0s - loss: 0.0772 - acc: 0.977 - ETA: 0s - loss: 0.0772 - acc: 0.977 - ETA: 0s - loss: 0.0772 - acc: 0.977 - ETA: 0s - loss: 0.0772 - acc: 0.977 - ETA: 0s - loss: 0.0782 - acc: 0.977 - ETA: 0s - loss: 0.0777 - acc: 0.977 - ETA: 0s - loss: 0.0795 - acc: 0.977 - ETA: 0s - loss: 0.0789 - acc: 0.977 - ETA: 0s - loss: 0.0794 - acc: 0.977 - ETA: 0s - loss: 0.0799 - acc: 0.976 - ETA: 0s - loss: 0.0794 - acc: 0.976 - ETA: 0s - loss: 0.0795 - acc: 0.976 - ETA: 0s - loss: 0.0814 - acc: 0.976 - ETA: 0s - loss: 0.0820 - acc: 0.975 - ETA: 0s - loss: 0.0820 - acc: 0.975 - ETA: 0s - loss: 0.0850 - acc: 0.9748Epoch 00018: val\_loss did not improve
6680/6680 [==============================] - 5s 801us/step - loss: 0.0846 - acc: 0.9749 - val\_loss: 0.6693 - val\_acc: 0.8563
Epoch 19/20
6660/6680 [============================>.] - ETA: 5s - loss: 0.0798 - acc: 0.950 - ETA: 4s - loss: 0.0885 - acc: 0.960 - ETA: 4s - loss: 0.0814 - acc: 0.966 - ETA: 4s - loss: 0.0686 - acc: 0.973 - ETA: 4s - loss: 0.0679 - acc: 0.973 - ETA: 4s - loss: 0.0572 - acc: 0.978 - ETA: 4s - loss: 0.0555 - acc: 0.980 - ETA: 4s - loss: 0.0783 - acc: 0.977 - ETA: 4s - loss: 0.0752 - acc: 0.977 - ETA: 4s - loss: 0.0823 - acc: 0.978 - ETA: 4s - loss: 0.0814 - acc: 0.978 - ETA: 4s - loss: 0.0811 - acc: 0.978 - ETA: 4s - loss: 0.0885 - acc: 0.975 - ETA: 4s - loss: 0.0892 - acc: 0.974 - ETA: 4s - loss: 0.0836 - acc: 0.975 - ETA: 3s - loss: 0.0829 - acc: 0.975 - ETA: 3s - loss: 0.0814 - acc: 0.975 - ETA: 3s - loss: 0.0913 - acc: 0.974 - ETA: 3s - loss: 0.0929 - acc: 0.973 - ETA: 3s - loss: 0.0938 - acc: 0.973 - ETA: 3s - loss: 0.0904 - acc: 0.974 - ETA: 3s - loss: 0.0899 - acc: 0.974 - ETA: 3s - loss: 0.0893 - acc: 0.974 - ETA: 3s - loss: 0.0878 - acc: 0.974 - ETA: 3s - loss: 0.0904 - acc: 0.973 - ETA: 3s - loss: 0.0895 - acc: 0.974 - ETA: 3s - loss: 0.0923 - acc: 0.974 - ETA: 3s - loss: 0.0906 - acc: 0.974 - ETA: 3s - loss: 0.0904 - acc: 0.974 - ETA: 3s - loss: 0.0924 - acc: 0.975 - ETA: 3s - loss: 0.0925 - acc: 0.975 - ETA: 3s - loss: 0.0911 - acc: 0.974 - ETA: 3s - loss: 0.0894 - acc: 0.974 - ETA: 3s - loss: 0.0890 - acc: 0.974 - ETA: 3s - loss: 0.0865 - acc: 0.975 - ETA: 3s - loss: 0.0873 - acc: 0.974 - ETA: 3s - loss: 0.0873 - acc: 0.974 - ETA: 2s - loss: 0.0862 - acc: 0.975 - ETA: 2s - loss: 0.0891 - acc: 0.975 - ETA: 2s - loss: 0.0905 - acc: 0.974 - ETA: 2s - loss: 0.0902 - acc: 0.974 - ETA: 2s - loss: 0.0883 - acc: 0.975 - ETA: 2s - loss: 0.0863 - acc: 0.975 - ETA: 2s - loss: 0.0847 - acc: 0.976 - ETA: 2s - loss: 0.0834 - acc: 0.976 - ETA: 2s - loss: 0.0826 - acc: 0.976 - ETA: 2s - loss: 0.0814 - acc: 0.977 - ETA: 2s - loss: 0.0849 - acc: 0.976 - ETA: 2s - loss: 0.0838 - acc: 0.977 - ETA: 2s - loss: 0.0861 - acc: 0.976 - ETA: 2s - loss: 0.0874 - acc: 0.976 - ETA: 2s - loss: 0.0863 - acc: 0.977 - ETA: 2s - loss: 0.0852 - acc: 0.977 - ETA: 2s - loss: 0.0844 - acc: 0.977 - ETA: 2s - loss: 0.0837 - acc: 0.977 - ETA: 1s - loss: 0.0826 - acc: 0.978 - ETA: 1s - loss: 0.0824 - acc: 0.978 - ETA: 1s - loss: 0.0818 - acc: 0.978 - ETA: 1s - loss: 0.0810 - acc: 0.978 - ETA: 1s - loss: 0.0810 - acc: 0.978 - ETA: 1s - loss: 0.0811 - acc: 0.978 - ETA: 1s - loss: 0.0811 - acc: 0.978 - ETA: 1s - loss: 0.0805 - acc: 0.978 - ETA: 1s - loss: 0.0810 - acc: 0.978 - ETA: 1s - loss: 0.0806 - acc: 0.978 - ETA: 1s - loss: 0.0805 - acc: 0.978 - ETA: 1s - loss: 0.0802 - acc: 0.977 - ETA: 1s - loss: 0.0791 - acc: 0.978 - ETA: 1s - loss: 0.0784 - acc: 0.978 - ETA: 1s - loss: 0.0777 - acc: 0.978 - ETA: 1s - loss: 0.0785 - acc: 0.978 - ETA: 1s - loss: 0.0778 - acc: 0.978 - ETA: 1s - loss: 0.0769 - acc: 0.979 - ETA: 1s - loss: 0.0760 - acc: 0.979 - ETA: 0s - loss: 0.0750 - acc: 0.979 - ETA: 0s - loss: 0.0751 - acc: 0.979 - ETA: 0s - loss: 0.0757 - acc: 0.979 - ETA: 0s - loss: 0.0754 - acc: 0.979 - ETA: 0s - loss: 0.0758 - acc: 0.978 - ETA: 0s - loss: 0.0755 - acc: 0.978 - ETA: 0s - loss: 0.0762 - acc: 0.978 - ETA: 0s - loss: 0.0757 - acc: 0.978 - ETA: 0s - loss: 0.0756 - acc: 0.978 - ETA: 0s - loss: 0.0766 - acc: 0.978 - ETA: 0s - loss: 0.0769 - acc: 0.978 - ETA: 0s - loss: 0.0794 - acc: 0.977 - ETA: 0s - loss: 0.0796 - acc: 0.977 - ETA: 0s - loss: 0.0804 - acc: 0.976 - ETA: 0s - loss: 0.0806 - acc: 0.976 - ETA: 0s - loss: 0.0802 - acc: 0.976 - ETA: 0s - loss: 0.0804 - acc: 0.976 - ETA: 0s - loss: 0.0797 - acc: 0.9769Epoch 00019: val\_loss did not improve
6680/6680 [==============================] - 6s 834us/step - loss: 0.0795 - acc: 0.9769 - val\_loss: 0.6986 - val\_acc: 0.8575
Epoch 20/20
6660/6680 [============================>.] - ETA: 5s - loss: 0.0075 - acc: 1.000 - ETA: 4s - loss: 0.0815 - acc: 0.970 - ETA: 4s - loss: 0.0705 - acc: 0.972 - ETA: 4s - loss: 0.0504 - acc: 0.980 - ETA: 4s - loss: 0.0506 - acc: 0.981 - ETA: 4s - loss: 0.0746 - acc: 0.982 - ETA: 4s - loss: 0.0897 - acc: 0.977 - ETA: 4s - loss: 0.0873 - acc: 0.976 - ETA: 4s - loss: 0.0845 - acc: 0.976 - ETA: 4s - loss: 0.1165 - acc: 0.972 - ETA: 4s - loss: 0.1125 - acc: 0.972 - ETA: 4s - loss: 0.1094 - acc: 0.971 - ETA: 4s - loss: 0.1049 - acc: 0.970 - ETA: 4s - loss: 0.0999 - acc: 0.971 - ETA: 4s - loss: 0.0961 - acc: 0.972 - ETA: 4s - loss: 0.0906 - acc: 0.974 - ETA: 3s - loss: 0.0872 - acc: 0.974 - ETA: 3s - loss: 0.0865 - acc: 0.973 - ETA: 3s - loss: 0.0838 - acc: 0.973 - ETA: 3s - loss: 0.0798 - acc: 0.975 - ETA: 3s - loss: 0.0855 - acc: 0.975 - ETA: 3s - loss: 0.0839 - acc: 0.975 - ETA: 3s - loss: 0.0843 - acc: 0.975 - ETA: 3s - loss: 0.0825 - acc: 0.975 - ETA: 3s - loss: 0.0802 - acc: 0.975 - ETA: 3s - loss: 0.0774 - acc: 0.976 - ETA: 3s - loss: 0.0769 - acc: 0.976 - ETA: 3s - loss: 0.0758 - acc: 0.977 - ETA: 3s - loss: 0.0733 - acc: 0.977 - ETA: 3s - loss: 0.0730 - acc: 0.977 - ETA: 3s - loss: 0.0755 - acc: 0.976 - ETA: 3s - loss: 0.0758 - acc: 0.977 - ETA: 3s - loss: 0.0773 - acc: 0.977 - ETA: 2s - loss: 0.0771 - acc: 0.977 - ETA: 2s - loss: 0.0753 - acc: 0.977 - ETA: 2s - loss: 0.0803 - acc: 0.977 - ETA: 2s - loss: 0.0816 - acc: 0.976 - ETA: 2s - loss: 0.0806 - acc: 0.976 - ETA: 2s - loss: 0.0789 - acc: 0.977 - ETA: 2s - loss: 0.0775 - acc: 0.977 - ETA: 2s - loss: 0.0763 - acc: 0.978 - ETA: 2s - loss: 0.0754 - acc: 0.977 - ETA: 2s - loss: 0.0768 - acc: 0.977 - ETA: 2s - loss: 0.0774 - acc: 0.977 - ETA: 2s - loss: 0.0770 - acc: 0.977 - ETA: 2s - loss: 0.0758 - acc: 0.977 - ETA: 2s - loss: 0.0762 - acc: 0.977 - ETA: 2s - loss: 0.0773 - acc: 0.977 - ETA: 2s - loss: 0.0775 - acc: 0.977 - ETA: 2s - loss: 0.0780 - acc: 0.976 - ETA: 1s - loss: 0.0772 - acc: 0.976 - ETA: 1s - loss: 0.0772 - acc: 0.976 - ETA: 1s - loss: 0.0761 - acc: 0.977 - ETA: 1s - loss: 0.0773 - acc: 0.976 - ETA: 1s - loss: 0.0761 - acc: 0.977 - ETA: 1s - loss: 0.0754 - acc: 0.977 - ETA: 1s - loss: 0.0757 - acc: 0.977 - ETA: 1s - loss: 0.0774 - acc: 0.977 - ETA: 1s - loss: 0.0765 - acc: 0.977 - ETA: 1s - loss: 0.0759 - acc: 0.977 - ETA: 1s - loss: 0.0755 - acc: 0.977 - ETA: 1s - loss: 0.0744 - acc: 0.978 - ETA: 1s - loss: 0.0745 - acc: 0.977 - ETA: 1s - loss: 0.0748 - acc: 0.977 - ETA: 1s - loss: 0.0759 - acc: 0.976 - ETA: 1s - loss: 0.0764 - acc: 0.976 - ETA: 1s - loss: 0.0761 - acc: 0.976 - ETA: 0s - loss: 0.0779 - acc: 0.976 - ETA: 0s - loss: 0.0768 - acc: 0.976 - ETA: 0s - loss: 0.0758 - acc: 0.976 - ETA: 0s - loss: 0.0749 - acc: 0.977 - ETA: 0s - loss: 0.0754 - acc: 0.976 - ETA: 0s - loss: 0.0746 - acc: 0.977 - ETA: 0s - loss: 0.0747 - acc: 0.976 - ETA: 0s - loss: 0.0739 - acc: 0.977 - ETA: 0s - loss: 0.0748 - acc: 0.976 - ETA: 0s - loss: 0.0744 - acc: 0.976 - ETA: 0s - loss: 0.0746 - acc: 0.976 - ETA: 0s - loss: 0.0752 - acc: 0.976 - ETA: 0s - loss: 0.0744 - acc: 0.976 - ETA: 0s - loss: 0.0739 - acc: 0.976 - ETA: 0s - loss: 0.0732 - acc: 0.977 - ETA: 0s - loss: 0.0726 - acc: 0.977 - ETA: 0s - loss: 0.0724 - acc: 0.977 - ETA: 0s - loss: 0.0720 - acc: 0.9772Epoch 00020: val\_loss did not improve
6680/6680 [==============================] - 5s 770us/step - loss: 0.0719 - acc: 0.9772 - val\_loss: 0.6966 - val\_acc: 0.8491

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}28}]:} <keras.callbacks.History at 0x242e22a0a58>
\end{Verbatim}
            
    \subsubsection{(IMPLEMENTATION) Load the Model with the Best Validation
Loss}\label{implementation-load-the-model-with-the-best-validation-loss}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Load the model weights with the best validation loss.}
         \PY{n}{Xception\PYZus{}model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.Xception.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Test the
Model}\label{implementation-test-the-model}

Try out your model on the test dataset of dog images. Ensure that your
test accuracy is greater than 60\%.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Calculate classification accuracy on the test dataset.}
         \PY{n}{Xception\PYZus{}predictions} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{Xception\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{feature}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{test\PYZus{}Xception}\PY{p}{]}
         
         \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{Xception\PYZus{}predictions}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Xception\PYZus{}predictions}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test accuracy: 84.9282\%

    \end{Verbatim}

    \subsubsection{(IMPLEMENTATION) Predict Dog Breed with the
Model}\label{implementation-predict-dog-breed-with-the-model}

Write a function that takes an image path as input and returns the dog
breed (\texttt{Affenpinscher}, \texttt{Afghan\_hound}, etc) that is
predicted by your model.

Similar to the analogous function in Step 5, your function should have
three steps: 1. Extract the bottleneck features corresponding to the
chosen CNN model. 2. Supply the bottleneck features as input to the
model to return the predicted vector. Note that the argmax of this
prediction vector gives the index of the predicted dog breed. 3. Use the
\texttt{dog\_names} array defined in Step 0 of this notebook to return
the corresponding breed.

The functions to extract the bottleneck features can be found in
\texttt{extract\_bottleneck\_features.py}, and they have been imported
in an earlier code cell. To obtain the bottleneck features corresponding
to your chosen CNN architecture, you need to use the function

\begin{verbatim}
extract_{network}
\end{verbatim}

where \texttt{\{network\}}, in the above filename, should be one of
\texttt{VGG19}, \texttt{Resnet50}, \texttt{InceptionV3}, or
\texttt{Xception}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Write a function that takes a path to an image as input}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} and returns the dog breed that is predicted by the model.}
         
         \PY{k}{def} \PY{n+nf}{Xception\PYZus{}predict\PYZus{}breed}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{n}{bottleneck\PYZus{}feature} \PY{o}{=} \PY{n}{extract\PYZus{}Xception}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}
             \PY{n}{predicted\PYZus{}vector} \PY{o}{=} \PY{n}{Xception\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{bottleneck\PYZus{}feature}\PY{p}{)}
             \PY{n}{dog\PYZus{}breed} \PY{o}{=} \PY{n}{dog\PYZus{}names}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{predicted\PYZus{}vector}\PY{p}{)}\PY{p}{]}
             \PY{k}{return} \PY{n}{dog\PYZus{}breed}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 6: Write your Algorithm

Write an algorithm that accepts a file path to an image and first
determines whether the image contains a human, dog, or neither. Then, -
if a \textbf{dog} is detected in the image, return the predicted breed.
- if a \textbf{human} is detected in the image, return the resembling
dog breed. - if \textbf{neither} is detected in the image, provide
output that indicates an error.

You are welcome to write your own functions for detecting humans and
dogs in images, but feel free to use the \texttt{face\_detector} and
\texttt{dog\_detector} functions developed above. You are
\textbf{required} to use your CNN from Step 5 to predict dog breed.

Some sample output for our algorithm is provided below, but feel free to
design your own user experience!

\begin{figure}
\centering
\includegraphics{images/sample_human_output.png}
\caption{Sample Human Output}
\end{figure}

\subsubsection{(IMPLEMENTATION) Write your
Algorithm}\label{implementation-write-your-algorithm}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Write your algorithm.}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Feel free to use as many code cells as needed.}
         
         \PY{k}{def} \PY{n+nf}{check\PYZus{}for\PYZus{}dog\PYZus{}breed}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{n}{dog\PYZus{}detected} \PY{o}{=} \PY{k+kc}{False}
             \PY{n}{dog\PYZus{}breed} \PY{o}{=} \PY{n}{Xception\PYZus{}predict\PYZus{}breed}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
             
             \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
             \PY{n}{cv\PYZus{}rgb} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2RGB}\PY{p}{)}
             \PY{n}{imgplot} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{cv\PYZus{}rgb}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
             
             \PY{k}{if} \PY{n}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
                 \PY{n}{dog\PYZus{}detected} \PY{o}{=} \PY{k+kc}{True} 
             \PY{k}{elif} \PY{o+ow}{not} \PY{n}{face\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{This is neither a human nor a dog}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{k}{return}
             \PY{k}{if} \PY{n}{dog\PYZus{}detected}\PY{p}{:}
                 \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{This is a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{dog\PYZus{}breed}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dog}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{k}{return}
             \PY{k}{else}\PY{p}{:}
                 \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{This is a human that looks like a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{dog\PYZus{}breed}\PY{p}{)}
                 \PY{k}{return}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 7: Test Your Algorithm

In this section, you will take your new algorithm for a spin! What kind
of dog does the algorithm think that \textbf{you} look like? If you have
a dog, does it predict your dog's breed accurately? If you have a cat,
does it mistakenly think that your cat is a dog?

\subsubsection{(IMPLEMENTATION) Test Your Algorithm on Sample
Images!}\label{implementation-test-your-algorithm-on-sample-images}

Test your algorithm at least six images on your computer. Feel free to
use any images you like. Use at least two human and two dog images.

\textbf{Question 6:} Is the output better than you expected :) ? Or
worse :( ? Provide at least three possible points of improvement for
your algorithm.

\textbf{Answer:} With an accuracy of about 85\% the output is quite as
expected. As far as I could verify the dog breeds of the test images
manually they are correct. Humans got identified correctly, too -
however, the similarities with dog breeds are a bit funny. Finally, Yoda
got identified as neither human nor dog ;-)

Deep learning in general involves a lot of trial and error with hyper
parameters, so more trainings with various parameter settings would
certainly allow even better results. Another way for improvement is
image augmentation for invariant representions of the image to be
learned as well.

Worth a try would be batch normalization in the network as it would
normalize the weighted inputs in the network. Without batch
normalization only the inputs at the beginning of the network get
normalized.

Tuning the number of epochs could be optimized by the early stopping
callback that Keras provides. Early stopping would automatically end
training based on specified rules, such as the accuracy not improving
after a given number of epochs.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} TODO: Execute your algorithm from Step 6 on}
         \PY{c+c1}{\PYZsh{}\PYZsh{} at least 6 images on your computer.}
         \PY{c+c1}{\PYZsh{}\PYZsh{} Feel free to use as many code cells as needed.}
         
         \PY{k+kn}{import} \PY{n+nn}{os}
         \PY{n}{my\PYZus{}files} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{glob}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mytestimages/*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{file} \PY{o+ow}{in} \PY{n}{my\PYZus{}files}\PY{p}{:}
             \PY{n}{check\PYZus{}for\PYZus{}dog\PYZus{}breed}\PY{p}{(}\PY{n}{file}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is a Chihuahua dog

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is a human that looks like a Nova\_scotia\_duck\_tolling\_retriever

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is a Golden\_retriever dog

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_6.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is a human that looks like a Havanese

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_8.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is a Beauceron dog

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_10.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is a American\_staffordshire\_terrier dog

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_12.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is a Alaskan\_malamute dog

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_14.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is a human that looks like a Havanese

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_16.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is a Collie dog

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_18.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is neither a human nor a dog

    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
